{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **LMR-Text Local Alignment Search Class**\n",
    "\n",
    "Idea: The idea is to inspire from BLAST, Basic Local Alignment Search Tool for genomics data and develop light and simple alignment search tool for LMR text. We have to take raw prediction from the model and find a match within the initl tweet to identify the correct word the model is trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Literal\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Version 1: DynamicProgramming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicTextAligner:\n",
    "    def __init__(self, text, subtext):\n",
    "        self.text = text\n",
    "        self.subtext = subtext\n",
    "        self.subtext_chunks = subtext.split()\n",
    "        self.chunk_ids = list(range(len(self.subtext_chunks)))\n",
    "        self.text_words_offsets = self._get_text_word_offsets()\n",
    "    \n",
    "    def _get_text_word_offsets(self):\n",
    "        words = self.text.split()\n",
    "        word_offsets = []\n",
    "        current_position = 0\n",
    "        \n",
    "        for word in words:\n",
    "            start_offset = self.text.find(word, current_position)\n",
    "            end_offset = start_offset + len(word) - 1\n",
    "            word_offsets.append({\n",
    "                'word': word,\n",
    "                'start_offset': start_offset,\n",
    "                'end_offset': end_offset\n",
    "            })\n",
    "            current_position = end_offset + 1\n",
    "\n",
    "        return word_offsets\n",
    "    \n",
    "    def find_chunk_positions(self):\n",
    "        results = []\n",
    "        text_len = len(self.text)\n",
    "\n",
    "        current_pos = 0\n",
    "        for idx, chunk in enumerate(self.subtext_chunks):\n",
    "            chunk_len = len(chunk)\n",
    "            \n",
    "            # Search for the chunk starting from the current position\n",
    "            match = None\n",
    "            for i in range(current_pos, text_len - chunk_len + 1):\n",
    "                if self.text[i:i + chunk_len] == chunk:\n",
    "                    match = (i, i + chunk_len - 1)\n",
    "                    break\n",
    "            \n",
    "            if match:\n",
    "                start_offset, end_offset = match\n",
    "                results.append({\n",
    "                    'chunk_id': idx,\n",
    "                    'chunk': chunk,\n",
    "                    'start_offset': start_offset,\n",
    "                    'end_offset': end_offset\n",
    "                })\n",
    "                current_pos = end_offset + 1\n",
    "\n",
    "        return results\n",
    "    \n",
    "    def merge_consecutive_words(self, words):\n",
    "        merged_words = []\n",
    "        i = 0\n",
    "        while i < len(words):\n",
    "            current_word = words[i]\n",
    "            while i + 1 < len(words) and current_word['end_offset'] + 2 == words[i + 1]['start_offset']:\n",
    "                next_word = words[i + 1]\n",
    "                current_word['word'] += f\" {next_word['word']}\"\n",
    "                current_word['end_offset'] = next_word['end_offset']\n",
    "                i += 1 \n",
    "            merged_words.append(current_word)\n",
    "            i += 1\n",
    "        return merged_words\n",
    "    \n",
    "    def merge_consecutive_words(self, words):\n",
    "        merged_words = []\n",
    "        i = 0\n",
    "        while i < len(words):\n",
    "            current_word = words[i]\n",
    "            if i + 1 < len(words):\n",
    "                next_word = words[i + 1]\n",
    "                if current_word['end_offset'] + 2 == next_word['start_offset']:\n",
    "                    merged_word = {\n",
    "                        'word': f\"{current_word['word']} {next_word['word']}\",\n",
    "                        'start_offset': current_word['start_offset'],\n",
    "                        'end_offset': next_word['end_offset']\n",
    "                    }\n",
    "                    merged_words.append(merged_word)\n",
    "                    i += 2\n",
    "                    continue\n",
    "            merged_words.append(current_word)\n",
    "            i += 1\n",
    "        return merged_words\n",
    "\n",
    "    def get_alignment(self, mode: Literal[\"dict\", \"flat\", \"groups\", \"flat_groups\", \"sort_flat_groups\"] = \"dict\"):\n",
    "        matches = self.find_chunk_positions()\n",
    "        aligned_words = []\n",
    "        remaining_word_offsets = self.text_words_offsets.copy()\n",
    "\n",
    "        for match in matches:\n",
    "            chunk_start = match['start_offset']\n",
    "            chunk_end = match['end_offset']\n",
    "\n",
    "            for i, word_info in enumerate(remaining_word_offsets):\n",
    "                word_start = word_info['start_offset']\n",
    "                word_end = word_info['end_offset']\n",
    "\n",
    "                if word_start <= chunk_start and word_end >= chunk_end:\n",
    "                    aligned_words.append(word_info)\n",
    "\n",
    "                    del remaining_word_offsets[i]\n",
    "                    break\n",
    "\n",
    "        if mode == \"flat\":\n",
    "            output = [word['word'] for word in aligned_words]\n",
    "        elif mode == \"groups\":\n",
    "            output = self.merge_consecutive_words(aligned_words)\n",
    "        elif mode == \"flat_groups\":\n",
    "            merged_words = self.merge_consecutive_words(aligned_words)\n",
    "            output = \" \".join([word['word'] for word in merged_words])\n",
    "        elif mode == \"flat_sorted_groups\":\n",
    "            merged_words = self.merge_consecutive_words(aligned_words)\n",
    "            output = \" \".join(sorted([word['word'] for word in merged_words]))\n",
    "        else:\n",
    "            output = aligned_words\n",
    "        return output\n",
    "\n",
    "    \n",
    "    # def display_results(self):\n",
    "    #     matches = self.find_chunk_positions()\n",
    "    #     for match in matches:\n",
    "    #         print(f\"Chunk ID: {match['chunk_id']}, Chunk: '{match['chunk']}', Start: {match['start_offset']}, End: {match['end_offset']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Version 2: DynamicProgramming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicTextAligner:\n",
    "    def __init__(self, text, subtext):\n",
    "        self.text = text\n",
    "        self.subtext = subtext\n",
    "        self.subtext_chunks = subtext.split()\n",
    "        self.chunk_ids = list(range(len(self.subtext_chunks)))\n",
    "        self.text_words_offsets = self._get_text_word_offsets()\n",
    "    \n",
    "    def _get_text_word_offsets(self):\n",
    "        words = self.text.split()\n",
    "        word_offsets = []\n",
    "        current_position = 0\n",
    "        \n",
    "        for word in words:\n",
    "            start_offset = self.text.find(word, current_position)\n",
    "            end_offset = start_offset + len(word) - 1\n",
    "            word_offsets.append({\n",
    "                'word': word,\n",
    "                'start_offset': start_offset,\n",
    "                'end_offset': end_offset,\n",
    "                'length': len(word)\n",
    "            })\n",
    "            current_position = end_offset + 1\n",
    "\n",
    "        return word_offsets\n",
    "\n",
    "    def find_chunk_positions(self):\n",
    "        results = []\n",
    "        text_len = len(self.text)\n",
    "\n",
    "        # Find all possible matches for each chunk\n",
    "        all_matches = []\n",
    "        for idx, chunk in enumerate(self.subtext_chunks):\n",
    "            chunk_len = len(chunk)\n",
    "            chunk_matches = []\n",
    "\n",
    "            for i in range(text_len - chunk_len + 1):\n",
    "                if self.text[i:i + chunk_len] == chunk:\n",
    "                    # Find which word this matches\n",
    "                    for word_data in self.text_words_offsets:\n",
    "                        if word_data['start_offset'] <= i <= word_data['end_offset']:\n",
    "                            start_offset = i\n",
    "                            end_offset = start_offset + chunk_len - 1\n",
    "                            chunk_matches.append({\n",
    "                                'chunk_id': idx,\n",
    "                                'chunk': chunk,\n",
    "                                'start_offset': start_offset,\n",
    "                                'end_offset': end_offset,\n",
    "                                'word': word_data['word'],\n",
    "                                'word_length': word_data['length']\n",
    "                            })\n",
    "            all_matches.append(chunk_matches)\n",
    "\n",
    "        # Apply constraints to get the best match\n",
    "        filtered_matches = []\n",
    "        previous_end = -1\n",
    "\n",
    "        for i, chunk_matches in enumerate(all_matches):\n",
    "            # Sort by word length (descending) to prioritize longest word matches\n",
    "            chunk_matches = sorted(chunk_matches, key=lambda x: x['word_length'], reverse=True)\n",
    "\n",
    "            if i + 1 < len(all_matches):\n",
    "                next_chunk_matches = all_matches[i + 1]\n",
    "                if next_chunk_matches:\n",
    "                    next_min_start_offset = min([m['start_offset'] for m in next_chunk_matches])\n",
    "                else:\n",
    "                    next_min_start_offset = float('inf')  # No next chunk means no overlap constraint\n",
    "            else:\n",
    "                next_min_start_offset = float('inf')\n",
    "\n",
    "            # Filter matches to ensure end_offset is lower than the next chunk's min start_offset\n",
    "            chunk_matches = [\n",
    "                match for match in chunk_matches if match['end_offset'] < next_min_start_offset\n",
    "            ]\n",
    "\n",
    "            # If multiple matches remain, select the one with the longest word length\n",
    "            if chunk_matches:\n",
    "                best_match = max(chunk_matches, key=lambda x: x['word_length'])\n",
    "                filtered_matches.append(best_match)\n",
    "                previous_end = best_match['end_offset']\n",
    "\n",
    "        return filtered_matches\n",
    "    \n",
    "    def find_chunk_positions(self):\n",
    "        results = []\n",
    "        text_len = len(self.text)\n",
    "\n",
    "        # Find all possible matches for each chunk\n",
    "        all_matches = []\n",
    "        for idx, chunk in enumerate(self.subtext_chunks):\n",
    "            chunk_len = len(chunk)\n",
    "            chunk_matches = []\n",
    "\n",
    "            for i in range(text_len - chunk_len + 1):\n",
    "                if self.text[i:i + chunk_len] == chunk:\n",
    "                    # Find which word this matches\n",
    "                    for word_data in self.text_words_offsets:\n",
    "                        if word_data['start_offset'] <= i <= word_data['end_offset']:\n",
    "                            start_offset = i\n",
    "                            end_offset = start_offset + chunk_len - 1\n",
    "                            chunk_matches.append({\n",
    "                                'chunk_id': idx,\n",
    "                                'chunk': chunk,\n",
    "                                'start_offset': start_offset,\n",
    "                                'end_offset': end_offset,\n",
    "                                'word': word_data['word'],\n",
    "                                'word_length': word_data['length']\n",
    "                            })\n",
    "            all_matches.append(chunk_matches)\n",
    "\n",
    "        # Apply constraints to get the best match for each chunk\n",
    "        filtered_matches = []\n",
    "        previous_end = -1\n",
    "\n",
    "        for i, chunk_matches in enumerate(all_matches):\n",
    "            # Sort by word length (descending) to prioritize longest word matches\n",
    "            chunk_matches = sorted(chunk_matches, key=lambda x: x['word_length'], reverse=True)\n",
    "\n",
    "            if i + 1 < len(all_matches):\n",
    "                next_chunk_matches = all_matches[i + 1]\n",
    "                if next_chunk_matches:\n",
    "                    next_min_start_offset = min([m['start_offset'] for m in next_chunk_matches])\n",
    "                else:\n",
    "                    next_min_start_offset = float('inf')  # No next chunk means no overlap constraint\n",
    "            else:\n",
    "                next_min_start_offset = float('inf')\n",
    "\n",
    "            # Filter matches to ensure end_offset is lower than the next chunk's min start_offset\n",
    "            chunk_matches = [\n",
    "                match for match in chunk_matches if match['end_offset'] < next_min_start_offset\n",
    "            ]\n",
    "\n",
    "            # If multiple matches remain, select the one with the longest word length\n",
    "            if chunk_matches:\n",
    "                for match in chunk_matches:\n",
    "                    if match['start_offset'] > previous_end:\n",
    "                        filtered_matches.append(match)\n",
    "                        previous_end = match['end_offset']\n",
    "\n",
    "        return filtered_matches\n",
    "    \n",
    "    def display_results(self):\n",
    "        matches = self.find_chunk_positions()\n",
    "        for match in matches:\n",
    "            print(f\"Chunk ID: {match['chunk_id']}, Chunk: '{match['chunk']}', Start: {match['start_offset']}, \"\n",
    "                  f\"End: {match['end_offset']}, Word: '{match['word']}', Word Length: {match['word_length']}\")\n",
    "    \n",
    "    def merge_consecutive_words(self, words):\n",
    "        merged_words = []\n",
    "        i = 0\n",
    "        while i < len(words):\n",
    "            current_word = words[i]\n",
    "            while i + 1 < len(words) and current_word['end_offset'] + 2 == words[i + 1]['start_offset']:\n",
    "                next_word = words[i + 1]\n",
    "                current_word['word'] += f\" {next_word['word']}\"\n",
    "                current_word['end_offset'] = next_word['end_offset']\n",
    "                i += 1 \n",
    "            merged_words.append(current_word)\n",
    "            i += 1\n",
    "        return merged_words\n",
    "    \n",
    "    def merge_consecutive_words(self, words):\n",
    "        merged_words = []\n",
    "        i = 0\n",
    "        while i < len(words):\n",
    "            current_word = words[i]\n",
    "            if i + 1 < len(words):\n",
    "                next_word = words[i + 1]\n",
    "                if current_word['end_offset'] + 2 == next_word['start_offset']:\n",
    "                    merged_word = {\n",
    "                        'word': f\"{current_word['word']} {next_word['word']}\",\n",
    "                        'start_offset': current_word['start_offset'],\n",
    "                        'end_offset': next_word['end_offset']\n",
    "                    }\n",
    "                    merged_words.append(merged_word)\n",
    "                    i += 2\n",
    "                    continue\n",
    "            merged_words.append(current_word)\n",
    "            i += 1\n",
    "        return merged_words\n",
    "\n",
    "    def get_alignment(self, mode: Literal[\"dict\", \"flat\", \"groups\", \"flat_groups\", \"sort_flat_groups\"] = \"dict\"):\n",
    "        matches = self.find_chunk_positions()\n",
    "        aligned_words = []\n",
    "        remaining_word_offsets = self.text_words_offsets.copy()\n",
    "\n",
    "        for match in matches:\n",
    "            chunk_start = match['start_offset']\n",
    "            chunk_end = match['end_offset']\n",
    "\n",
    "            for i, word_info in enumerate(remaining_word_offsets):\n",
    "                word_start = word_info['start_offset']\n",
    "                word_end = word_info['end_offset']\n",
    "\n",
    "                if word_start <= chunk_start and word_end >= chunk_end:\n",
    "                    aligned_words.append(word_info)\n",
    "\n",
    "                    del remaining_word_offsets[i]\n",
    "                    break\n",
    "\n",
    "        if mode == \"flat\":\n",
    "            output = [word['word'] for word in aligned_words]\n",
    "        elif mode == \"groups\":\n",
    "            output = self.merge_consecutive_words(aligned_words)\n",
    "        elif mode == \"flat_groups\":\n",
    "            merged_words = self.merge_consecutive_words(aligned_words)\n",
    "            output = \" \".join([word['word'] for word in merged_words])\n",
    "        elif mode == \"flat_sorted_groups\":\n",
    "            merged_words = self.merge_consecutive_words(aligned_words)\n",
    "            output = \" \".join(sorted([word['word'] for word in merged_words]))\n",
    "        else:\n",
    "            output = aligned_words\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Version 3: More like **BLAST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiDirectionalTextAligner:\n",
    "    def __init__(self, text, subtext):\n",
    "        self.text = text\n",
    "        self.subtext = subtext\n",
    "        self.text_chunks = text.split()\n",
    "        self.subtext_chunks = subtext.split()\n",
    "        self.text_word_offsets = self._get_text_word_offsets()\n",
    "\n",
    "    def _get_text_word_offsets(self):\n",
    "        words = self.text_chunks\n",
    "        word_offsets = []\n",
    "        current_position = 0\n",
    "        for word in words:\n",
    "            start_offset = self.text.find(word, current_position)\n",
    "            end_offset = start_offset + len(word) - 1\n",
    "            word_offsets.append({\n",
    "                'word': word,\n",
    "                'start_offset': start_offset,\n",
    "                'end_offset': end_offset,\n",
    "                'length': len(word)\n",
    "            })\n",
    "            current_position = end_offset + 1\n",
    "        return word_offsets\n",
    "\n",
    "    def find_word_by_offsets(self, start_offset, end_offset):\n",
    "        for index, entry in enumerate(self.text_word_offsets):\n",
    "            if entry['start_offset'] <= start_offset <= entry['end_offset'] and \\\n",
    "                entry['start_offset'] <= end_offset <= entry['end_offset']:\n",
    "                return (entry['word'], index)\n",
    "        return None\n",
    "\n",
    "    def _find_matches(self, chunk):\n",
    "        \"\"\"Find all positions in the text where the chunk can match.\"\"\"\n",
    "        matches = []\n",
    "        for i, word in enumerate(self.text_chunks):\n",
    "            start_pos = word.find(chunk)  # Search for chunk anywhere in the word\n",
    "            while start_pos != -1:  # As long as the chunk is found in the word\n",
    "                # Compute the start and end offsets of the match within the full text\n",
    "                start_offset = self.text_word_offsets[i]['start_offset'] + start_pos\n",
    "                end_offset = start_offset + len(chunk) - 1\n",
    "                matches.append({\n",
    "                    'chunk': chunk,\n",
    "                    'start_offset': start_offset,\n",
    "                    'end_offset': end_offset,\n",
    "                    'word': word,\n",
    "                    'word_index': i,\n",
    "                    'match_start_in_word': start_pos,  # Position of chunk in the word\n",
    "                    'word_length': len(word),\n",
    "                })\n",
    "                # Look for the next occurrence of the chunk within the same word (in case it appears multiple times)\n",
    "                start_pos = word.find(chunk, start_pos + 1)\n",
    "        return matches\n",
    "\n",
    "    def _extend_match(self, match, chunk_id):\n",
    "        \"\"\"Extend the match to the left and right, counting insertions and deletions and collecting matches.\"\"\"\n",
    "        insertion_count = 0\n",
    "        deletion_count = 0\n",
    "        chunks_found = 1\n",
    "        matches_found = [match]\n",
    "\n",
    "        # Forward search (right direction)\n",
    "        current_subtext_idx = chunk_id + 1\n",
    "        cursor = match['end_offset'] + 1\n",
    "        while cursor < len(self.text):\n",
    "            if current_subtext_idx != len(self.subtext_chunks):\n",
    "                next_chunk = self.subtext_chunks[current_subtext_idx]\n",
    "\n",
    "                if self.text[cursor] != next_chunk[0]:\n",
    "                    insertion_count += 1\n",
    "                    cursor += 1\n",
    "                else:\n",
    "                    c = 0\n",
    "                    for i, char in enumerate(next_chunk):\n",
    "                        if char == self.text[cursor+i]:\n",
    "                            c += 1\n",
    "                    if c == len(next_chunk):\n",
    "                        chunks_found += 1\n",
    "                        cursor += len(next_chunk)\n",
    "                        new_start_offset = cursor - len(next_chunk)\n",
    "                        new_end_offset = cursor - 1\n",
    "                        text_word, text_index = self.find_word_by_offsets(new_start_offset, new_end_offset)\n",
    "                        matches_found.append({\n",
    "                            'chunk': next_chunk,\n",
    "                            'start_offset': new_start_offset,\n",
    "                            'end_offset': new_end_offset,\n",
    "                            'word': text_word,\n",
    "                            'word_index': text_index,\n",
    "                            'word_length': len(text_word),\n",
    "                        })\n",
    "                        current_subtext_idx += 1\n",
    "                    else:\n",
    "                        deletion_count += (len(next_chunk) - c)\n",
    "                        cursor += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Backward search (left direction)\n",
    "        current_subtext_idx = chunk_id - 1\n",
    "        cursor = match['start_offset'] - 1\n",
    "        while cursor >= 0:\n",
    "            if current_subtext_idx >= 0:\n",
    "                prev_chunk = self.subtext_chunks[current_subtext_idx]\n",
    "\n",
    "                if self.text[cursor] != prev_chunk[-1]:\n",
    "                    insertion_count += 1\n",
    "                    cursor -= 1\n",
    "                else:\n",
    "                    c = 0\n",
    "                    for i, char in enumerate(reversed(prev_chunk)):\n",
    "                        if char == self.text[cursor-i]:\n",
    "                            c += 1\n",
    "                    if c == len(prev_chunk):\n",
    "                        chunks_found += 1\n",
    "                        cursor -= len(prev_chunk)\n",
    "                        new_start_offset = cursor + 1\n",
    "                        new_end_offset = cursor + len(prev_chunk)\n",
    "                        text_word, text_index = self.find_word_by_offsets(new_start_offset, new_end_offset)\n",
    "                        matches_found.append({\n",
    "                            'chunk': prev_chunk,\n",
    "                            'start_offset': new_start_offset,\n",
    "                            'end_offset': new_end_offset,\n",
    "                            'word': text_word,\n",
    "                            'word_index': text_index,\n",
    "                            'word_length': len(text_word),\n",
    "                        })\n",
    "                        current_subtext_idx -= 1\n",
    "                    else:\n",
    "                        deletion_count += (len(prev_chunk) - c)\n",
    "                        cursor -= 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return {\n",
    "            'chunks_found': chunks_found,\n",
    "            'insertion_count': insertion_count,\n",
    "            'deletion_count': deletion_count,\n",
    "            'matches_found': matches_found\n",
    "        }\n",
    "\n",
    "    def align(self):\n",
    "        all_results = []\n",
    "\n",
    "        # For each subtext chunk, find matches and extend them\n",
    "        for chunk_id, chunk in enumerate(self.subtext_chunks):\n",
    "            matches = self._find_matches(chunk)\n",
    "            for match in matches:\n",
    "                result = self._extend_match(match, chunk_id)\n",
    "                all_results.append(result)\n",
    "\n",
    "        return all_results\n",
    "\n",
    "    def display_best_result(self):\n",
    "        results = self.align()\n",
    "\n",
    "        # Scoring: maximize chunks found, minimize insertions and deletions\n",
    "        best_result = max(\n",
    "            results, \n",
    "            key=lambda r: (r['chunks_found'], -r['insertion_count'], -r['deletion_count'])\n",
    "        )\n",
    "\n",
    "        print(f\"Best Match:\")\n",
    "        matches = best_result['matches_found']\n",
    "        for match in matches:\n",
    "            print(f\"Chunk: {match['chunk']}, Reference_Word: '{match['word']}', Start: {match['start_offset']}, End: {match['end_offset']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Match:\n",
      "Chunk: Maryland, Reference_Word: 'Maryland', Start: 186, End: 193\n",
      "Chunk: Baltimore, Reference_Word: 'Baltimore', Start: 87, End: 95\n",
      "Chunk: Maryland, Reference_Word: 'Maryland', Start: 15, End: 22\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"woo is good for this job job\"\n",
    "subtext = \"wo oo ob\"\n",
    "\n",
    "text = \"Other parts of Maryland also saw significant damage from Sundays storms including this Baltimore city neighborhood Dundalk and Catonsville Rain totals spanned from 1 to 10 inches across Maryland ECFlood\"\n",
    "subtext = \"Maryland Baltimore Maryland\"\n",
    "\n",
    "aligner = BiDirectionalTextAligner(text, subtext)\n",
    "aligner.display_best_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk ID: 0, Chunk: 'Maryland', Start: 15, End: 22, Word: 'Maryland', Word Length: 8\n",
      "Chunk ID: 2, Chunk: 'Maryland', Start: 186, End: 193, Word: 'Maryland', Word Length: 8\n"
     ]
    }
   ],
   "source": [
    "text = \"ELLI ELLICOTT CITY, Md. (WJZ)– Last Sunday, heavy down pours caused major flooding in both Howard and Baltimore Counties. Main Street in Ellicott City was slammed by the deluge that led to the death of a Maryland National  via\"\n",
    "subtext = \"LL T Baltimore El lic ott City Maryland\"\n",
    "\n",
    "text = \"UPDATE on A man in his 40s is missing from Ellicott City Maryland where this water was raging down Main Street Video Courtesy Baltimore Sun flooding MarylandFlooding\"\n",
    "subtext = \"El lic ott City Maryland Baltimore\"\n",
    "\n",
    "text = \"Other parts of Maryland also saw significant damage from Sundays storms including this Baltimore city neighborhood Dundalk and Catonsville Rain totals spanned from 1 to 10 inches across Maryland ECFlood\"\n",
    "subtext = \"Maryland Baltimore Maryland\"\n",
    "\n",
    "aligner = DynamicTextAligner(text, subtext)\n",
    "aligner.display_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'Maryland', 'start_offset': 15, 'end_offset': 22, 'length': 8},\n",
       " {'word': 'Maryland', 'start_offset': 186, 'end_offset': 193, 'length': 8}]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligner.get_alignment(mode=\"dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ellicott', 'City', 'Maryland', 'Baltimore']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligner.get_alignment(mode=\"flat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Heuristic 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices(text, locations):\n",
    "    try:\n",
    "        if locations.strip() == '':\n",
    "            return ' '\n",
    "        words = locations.split()\n",
    "        \n",
    "        combinations = []\n",
    "        for length in range(1, 4):\n",
    "            for i in range(len(words) - length + 1):\n",
    "                combination = ' '.join(words[i:i + length])\n",
    "                combinations.append(combination)\n",
    "        indices = []\n",
    "        for comb in combinations:\n",
    "            for match in re.finditer(re.escape(comb), text, re.IGNORECASE):\n",
    "                indices.append((match.start(), match.end(), comb))\n",
    "\n",
    "        # keep only indices with the longest match. \n",
    "        indices = sorted(indices, key=lambda x: len(x[2]), reverse=True)\n",
    "        # drop duplicated start indices \n",
    "        indices = [indices[0]] + [x for i, x in enumerate(indices[1:], 1) if x[0] not in [y[0] for y in indices[:i]]]\n",
    "        # drop indices that are contained in other indices\n",
    "        indices = [x for i, x in enumerate(indices) if not any(x[0] >= y[0] and x[1] <= y[1] for y in indices[:i] + indices[i+1:])]\n",
    "\n",
    "        # return group of words corresponding to the indices in text \n",
    "        words_from_text = []\n",
    "        for start, end, comb in indices:\n",
    "            words_from_text.append(text[start:end])\n",
    "    except IndexError: \n",
    "        words_from_text = find_substring_indices(text, locations)\n",
    "\n",
    "    return \" \".join(sorted(set(words_from_text)))\n",
    "\n",
    "def find_substring_indices(text, locations):\n",
    "    # Generate all possible substrings of the word\n",
    "    substrings = [locations[i:j] for i in range(len(locations)) for j in range(i + 1, len(locations) + 1)]\n",
    "    indices = []\n",
    "    for substring in substrings:\n",
    "        for match in re.finditer(re.escape(substring), text, re.IGNORECASE):\n",
    "            indices.append((match.start(), match.end(), substring))\n",
    "\n",
    "    # keep only indices with the longest match.\n",
    "    indices = sorted(indices, key=lambda x: len(x[2]), reverse=True)\n",
    "\n",
    "    # drop duplicated start indices\n",
    "    indices = [indices[0]] + [x for i, x in enumerate(indices[1:], 1) if x[0] not in [y[0] for y in indices[:i]]]\n",
    "\n",
    "    # drop indices that are contained in other indices\n",
    "    indices = [x for i, x in enumerate(indices) if not any(x[0] >= y[0] and x[1] <= y[1] for y in indices[:i] + indices[i+1:])]\n",
    "\n",
    "    # keep indices of words long of at least 2 characters\n",
    "    indices = [x for x in indices if len(x[2]) >= 3]\n",
    "\n",
    "    # words_from_text \n",
    "    words_from_text = []\n",
    "    for start, end, substring in indices:\n",
    "        words_from_text.append(substring)\n",
    "\n",
    "    # sort words_from_text\n",
    "    words_from_text = sorted(words_from_text)\n",
    "\n",
    "    return words_from_text\n",
    "\n",
    "def heuristic_postprocess_1(row):\n",
    "    _id    = row['tweet_id']\n",
    "    text  = row['raw_prediction']\n",
    "    tweet = row['text']\n",
    "    \n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \" \"\n",
    "    else:\n",
    "        # 1 - Clean Special char\n",
    "        replacements = {\n",
    "            \" ##\": \"\",\n",
    "            \"##\": \"\",\n",
    "            \",\": \"\",\n",
    "            \"U . S .\": \"U.S.\",\n",
    "            \"U . S\": \"U.S.\",\n",
    "            \"U S\": \"U.S.\",\n",
    "            \"L . A .\": \"L.A.\",\n",
    "            \"L . A\": \"L.A.\",\n",
    "            \"L A\": \"L.A.\",\n",
    "            \"P . R .\": \"P.R.\",\n",
    "            \"P . R\": \"P.R.\",\n",
    "            \"P R\": \"P.R.\",\n",
    "            \"N . C .\": \"N.C.\",\n",
    "            \"N . N\": \"N.C.\",\n",
    "            \"N C\": \"N.C.\",\n",
    "            \"D . C .\": \"D.C.\",\n",
    "            \"D . C\": \"D.C.\",\n",
    "            \"D C\": \"D.C.\"\n",
    "        }\n",
    "        for word, replacement in replacements.items():\n",
    "            text = text.replace(word, replacement)\n",
    "        \n",
    "        #\"\"\"\n",
    "        # 2 - Special Replace\n",
    "        text = re.sub(r'\\bM\\b', 'Md.', text)\n",
    "        text = re.sub(r'\\bElliot\\b', '', text)\n",
    "        text = re.sub(r'\\bMat\\b', 'Matti', text)\n",
    "        text = re.sub(r'\\bSD\\b', 'SDMA', text)\n",
    "        text = re.sub(r'\\bZ\\b', 'Zimba', text)\n",
    "        text = re.sub(r'\\btt\\b', 'Hutt', text)\n",
    "        text = re.sub(r'\\bbe\\b', 'Brooklyn', text)\n",
    "        text = re.sub(r'\\bly\\b', 'welly', text)\n",
    "        text = re.sub(r'\\bgree\\b', 'greece', text)\n",
    "        text = re.sub(r'\\bAt\\b', 'Attica', text)\n",
    "        \n",
    "        \n",
    "        # 3 - Join City or County or New as one word\n",
    "        pattern1 = r'\\b(\\w+)\\s(city|CITY|City|county|COUNTY|County)\\b'\n",
    "        pattern2 = r'\\b(New|NEW|new|United|United__Arab|East)\\s(\\w+)\\b'\n",
    "        def replace_func1(match):\n",
    "            first_word = match.group(1)\n",
    "            city_word = match.group(2)\n",
    "            return f'{first_word}__{city_word}'\n",
    "        def replace_func2(match):\n",
    "            first_word = match.group(1)\n",
    "            next_word = match.group(2)\n",
    "            return f'{first_word}__{next_word}'\n",
    "        text = re.sub(pattern1, replace_func1, text)\n",
    "        text = re.sub(pattern2, replace_func2, text)\n",
    "        text = re.sub(pattern2, replace_func2, text)\n",
    "        \n",
    "        # 4 - Remove repeated groups of words\n",
    "        words = text.split()\n",
    "        seen_words = set()\n",
    "        unique_words = []\n",
    "        for word in words:\n",
    "            if word not in seen_words:\n",
    "                seen_words.add(word)\n",
    "                unique_words.append(word)\n",
    "        \n",
    "        # 5 - Sort location in Alphabetic order\n",
    "        unique_words = [place.replace(\"__\", \" \") for place in unique_words]\n",
    "        unique_words = sorted(unique_words)\n",
    "        text = \" \".join(unique_words)\n",
    "        \n",
    "        # 6 - Remove words with length lower than 2\n",
    "        text = \" \".join([word for word in text.split() if len(word) >= 2])\n",
    "        #\"\"\"\n",
    "        \n",
    "        # Desiré processing\n",
    "        #text = find_indices(tweet, text)\n",
    "        \n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Heuristic Text Aligner protopyte**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicTextAligner:\n",
    "    \"\"\"\n",
    "    **LMR-Text Local Alignment Search Class**\n",
    "    Idea: The idea is to inspire from BLAST, Basic Local Alignment Search Tool for genomics data \n",
    "    and develop light and simple alignment search tool for LMR text. We have to take raw \n",
    "    prediction from the model and find a match within the initl tweet to identify the correct \n",
    "    word the model is trying to predict.\n",
    "    \"\"\"\n",
    "    def __init__(self, text, subtext):\n",
    "        self.text = text\n",
    "        self.subtext = subtext\n",
    "        self.subtext_chunks = subtext.split()\n",
    "        self.chunk_ids = list(range(len(self.subtext_chunks)))\n",
    "        self.text_words_offsets = self._get_text_word_offsets()\n",
    "    \n",
    "    def _get_text_word_offsets(self):\n",
    "        words = self.text.split()\n",
    "        word_offsets = []\n",
    "        current_position = 0\n",
    "        \n",
    "        for word in words:\n",
    "            start_offset = self.text.find(word, current_position)\n",
    "            end_offset = start_offset + len(word) - 1\n",
    "            word_offsets.append({\n",
    "                'word': word,\n",
    "                'start_offset': start_offset,\n",
    "                'end_offset': end_offset,\n",
    "                'length': len(word)\n",
    "            })\n",
    "            current_position = end_offset + 1\n",
    "\n",
    "        return word_offsets\n",
    "    \n",
    "    def find_chunk_positions(self):\n",
    "        results = []\n",
    "        text_len = len(self.text)\n",
    "\n",
    "        # Find all possible matches for each chunk\n",
    "        all_matches = []\n",
    "        for idx, chunk in enumerate(self.subtext_chunks):\n",
    "            chunk_len = len(chunk)\n",
    "            chunk_matches = []\n",
    "\n",
    "            for i in range(text_len - chunk_len + 1):\n",
    "                if self.text[i:i + chunk_len] == chunk:\n",
    "                    # Find which word this matches\n",
    "                    for word_data in self.text_words_offsets:\n",
    "                        if word_data['start_offset'] <= i <= word_data['end_offset']:\n",
    "                            start_offset = i\n",
    "                            end_offset = start_offset + chunk_len - 1\n",
    "                            chunk_matches.append({\n",
    "                                'chunk_id': idx,\n",
    "                                'chunk': chunk,\n",
    "                                'start_offset': start_offset,\n",
    "                                'end_offset': end_offset,\n",
    "                                'word': word_data['word'],\n",
    "                                'word_length': word_data['length']\n",
    "                            })\n",
    "            all_matches.append(chunk_matches)\n",
    "\n",
    "        # Apply constraints to get the best match\n",
    "        filtered_matches = []\n",
    "        previous_end = -1\n",
    "\n",
    "        for i, chunk_matches in enumerate(all_matches):\n",
    "            # Sort by word length (descending) to prioritize longest word matches\n",
    "            chunk_matches = sorted(chunk_matches, key=lambda x: x['word_length'], reverse=True)\n",
    "\n",
    "            if i + 1 < len(all_matches):\n",
    "                next_chunk_matches = all_matches[i + 1]\n",
    "                if next_chunk_matches:\n",
    "                    next_min_start_offset = min([m['start_offset'] for m in next_chunk_matches])\n",
    "                else:\n",
    "                    next_min_start_offset = float('inf')  # No next chunk means no overlap constraint\n",
    "            else:\n",
    "                next_min_start_offset = float('inf')\n",
    "\n",
    "            # Filter matches to ensure end_offset is lower than the next chunk's min start_offset\n",
    "            chunk_matches = [\n",
    "                match for match in chunk_matches if match['end_offset'] < next_min_start_offset\n",
    "            ]\n",
    "\n",
    "            # If multiple matches remain, select the one with the longest word length\n",
    "            if chunk_matches:\n",
    "                best_match = max(chunk_matches, key=lambda x: x['word_length'])\n",
    "                filtered_matches.append(best_match)\n",
    "                previous_end = best_match['end_offset']\n",
    "\n",
    "        return filtered_matches\n",
    "    \n",
    "    def merge_consecutive_words(self, words):\n",
    "        merged_words = []\n",
    "        i = 0\n",
    "        while i < len(words):\n",
    "            current_word = words[i]\n",
    "            if i + 1 < len(words):\n",
    "                next_word = words[i + 1]\n",
    "                if current_word['end_offset'] + 2 == next_word['start_offset']:\n",
    "                    merged_word = {\n",
    "                        'word': f\"{current_word['word']} {next_word['word']}\",\n",
    "                        'start_offset': current_word['start_offset'],\n",
    "                        'end_offset': next_word['end_offset']\n",
    "                    }\n",
    "                    merged_words.append(merged_word)\n",
    "                    i += 2\n",
    "                    continue\n",
    "            merged_words.append(current_word)\n",
    "            i += 1\n",
    "        return merged_words\n",
    "\n",
    "    def heuristic_post_processing(self, merged_words):\n",
    "        return \" \".join(sorted(set([word['word'] for word in merged_words])))\n",
    "    \n",
    "        \"\"\"\n",
    "        # 1 - Replace special cases\n",
    "        punctuations = [\",\", \";\", \":\", \"#\", \"(\", \")\", \"\\\"\", \"[\", \"]\", \"?\"]\n",
    "        output = [word['word'].split(\"’\")[0] for word in merged_words]\n",
    "        output = [word.replace(\".,\", \".\") for word in output]\n",
    "        output = [subword for word in output for subword in word.split('/')]\n",
    "        words  = [word.translate(str.maketrans('', '', ''.join(punctuations))) for word in output]\n",
    "\n",
    "        # 2 - Handle hyphens and capital letters\n",
    "        processed_words = []\n",
    "        for word in words:\n",
    "            if word.isupper():\n",
    "                processed_words.append(word)\n",
    "            else:\n",
    "                processed_word = word.replace(\"-\", \" \")\n",
    "                processed_words.append(processed_word)\n",
    "                \n",
    "        # 3 - Process dots in words\n",
    "        final_words = []\n",
    "        for word in processed_words:\n",
    "            if \".\" in word and word.count(\".\") < 2:\n",
    "                if word.endswith(\"Md.\"):\n",
    "                    final_words.append(word)\n",
    "                else:\n",
    "                    final_words.append(word.replace(\".\", \"\"))\n",
    "            else:\n",
    "                final_words.append(word)\n",
    "                \n",
    "        final_words = sorted(final_words)\n",
    "        output = \" \".join(final_words)\n",
    "        return output\n",
    "        \"\"\"\n",
    "\n",
    "    def get_alignment(self, mode: Literal[\"dict\", \"flat\", \"groups\", \"flat_groups\", \"flat_sorted_groups\", \"flat_sorted_groups+heur\"] = \"dict\"):\n",
    "        matches = self.find_chunk_positions()\n",
    "        aligned_words = []\n",
    "        remaining_word_offsets = self.text_words_offsets.copy()\n",
    "\n",
    "        for match in matches:\n",
    "            chunk_start = match['start_offset']\n",
    "            chunk_end = match['end_offset']\n",
    "\n",
    "            for i, word_info in enumerate(remaining_word_offsets):\n",
    "                word_start = word_info['start_offset']\n",
    "                word_end = word_info['end_offset']\n",
    "\n",
    "                if word_start <= chunk_start and word_end >= chunk_end:\n",
    "                    aligned_words.append(word_info)\n",
    "\n",
    "                    del remaining_word_offsets[i]\n",
    "                    break\n",
    "\n",
    "        if mode == \"flat\":\n",
    "            output = [word['word'] for word in aligned_words]\n",
    "        elif mode == \"groups\":\n",
    "            output = self.merge_consecutive_words(aligned_words)\n",
    "        elif mode == \"flat_groups\":\n",
    "            merged_words = self.merge_consecutive_words(aligned_words)\n",
    "            output = \" \".join([word['word'] for word in merged_words])\n",
    "        elif mode == \"flat_sorted_groups\":\n",
    "            merged_words = self.merge_consecutive_words(aligned_words)\n",
    "            output = \" \".join(sorted([word['word'] for word in merged_words]))\n",
    "        elif mode == \"flat_sorted_groups+heur\":\n",
    "            merged_words = self.merge_consecutive_words(aligned_words)\n",
    "            output = self.heuristic_post_processing(merged_words)\n",
    "        else:\n",
    "            output = aligned_words\n",
    "        return output\n",
    "\n",
    "    \n",
    "    def display_results(self):\n",
    "        matches = self.find_chunk_positions()\n",
    "        for match in matches:\n",
    "            print(f\"Chunk ID: {match['chunk_id']}, Chunk: '{match['chunk']}', Start: {match['start_offset']}, End: {match['end_offset']}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def heuristic_pre_processing(sentence):\n",
    "        words = sentence.split()\n",
    "\n",
    "        # 1 - Replace special cases\n",
    "        punctuations = [\",\", \";\", \":\", \"#\", \"(\", \")\", \"\\\"\", \"[\", \"]\", \"?\", \"!\"]\n",
    "        output = [word.split(\"’\")[0] for word in words]\n",
    "        output = [word.replace(\".,\", \".\") for word in output]\n",
    "        output = [subword for word in output for subword in word.split('/')]\n",
    "        words  = [word.translate(str.maketrans('', '', ''.join(punctuations))) for word in output]\n",
    "\n",
    "        # 2 - Handle hyphens and capital letters\n",
    "        processed_words = []\n",
    "        for word in words:\n",
    "            if word.isupper():\n",
    "                processed_words.append(word)\n",
    "            else:\n",
    "                processed_word = word.replace(\"-\", \" \") \n",
    "                processed_words.append(processed_word)\n",
    "                \n",
    "        # 3 - Process dots in words\n",
    "        final_words = []\n",
    "        for word in processed_words:\n",
    "            if \"@\" not in word:\n",
    "                if \".\" in word and word.count(\".\") < 2:\n",
    "                    if word.endswith(\"Md.\"):\n",
    "                        final_words.append(word)\n",
    "                    else:\n",
    "                        final_words.append(word.replace(\".\", \"\"))\n",
    "                else:\n",
    "                    final_words.append(word)\n",
    "                \n",
    "        output = \" \".join(final_words)\n",
    "        return output\n",
    "\n",
    "def TLAST_postprocess(row, with_text=False):\n",
    "    generated_text  = row['raw_prediction']\n",
    "    targeted_text   = heuristic_pre_processing(row['text'])\n",
    "    \n",
    "    if not isinstance(generated_text, str) or not generated_text.strip():\n",
    "        return \" \"\n",
    "    \n",
    "    # Clean Special Clean\n",
    "    replacements = {\n",
    "        # \" ##\": \"\",\n",
    "        # \"##\": \"\",\n",
    "        \"#\": \"\",\n",
    "    }\n",
    "    for word, replacement in replacements.items():\n",
    "        generated_text = generated_text.replace(word, replacement)\n",
    "    \n",
    "    # Call TLAST\n",
    "    cleaned_text = DynamicTextAligner(targeted_text, generated_text).get_alignment(\n",
    "        mode=\"flat_sorted_groups+heur\"\n",
    "    )\n",
    "    if with_text:\n",
    "        return cleaned_text.strip(), targeted_text\n",
    "    return cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./train_inference_tlast3.csv')\n",
    "df = df.drop(columns=[\"Unnamed: 0\", \"prediction\", \"WER\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "      <th>raw_prediction</th>\n",
       "      <th>prediction</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_1001136696589631488</td>\n",
       "      <td>Flash floods struck a Maryland city on Sunday,...</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>Flash floods struck a Maryland city on Sunday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_1001136950345109504</td>\n",
       "      <td>State of emergency declared for Maryland flood...</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>State of emergency declared for Maryland flood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_1001137334056833024</td>\n",
       "      <td>Other parts of Maryland also saw significant d...</td>\n",
       "      <td>Baltimore Maryland</td>\n",
       "      <td>Maryland Baltimore Maryland</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>Other parts of Maryland also saw significant d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_1001138374923579392</td>\n",
       "      <td>Catastrophic Flooding Slams Ellicott City, Mar...</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "      <td>El ##lic ##ott City Maryland</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "      <td>Catastrophic Flooding Slams Ellicott City Mary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_1001138377717157888</td>\n",
       "      <td>WATCH: 1 missing after flash #FLOODING devasta...</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "      <td>El ##lic ##ott City Maryland</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "      <td>WATCH 1 missing after flash FLOODING devastate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ID_1001141769155891200</td>\n",
       "      <td>Watch Live: Aerials of damage after historic f...</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "      <td>El ##lic ##ott City Maryland</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "      <td>Watch Live Aerials of damage after historic fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ID_1001143589630427136</td>\n",
       "      <td>One person is reported missing as a state of e...</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>One person is reported missing as a state of e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ID_1001143679267098624</td>\n",
       "      <td>Monday May 28 - Morning Report: National Guard...</td>\n",
       "      <td>Arlington Maryland</td>\n",
       "      <td>Maryland Arlington</td>\n",
       "      <td>Arlington Maryland</td>\n",
       "      <td>Monday May 28   Morning Report National Guards...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ID_1001144314897018880</td>\n",
       "      <td>One man is still missing after flash flooding ...</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "      <td>El ##lic ##ott City Maryland</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "      <td>One man is still missing after flash flooding ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ID_1001144944717914112</td>\n",
       "      <td>RT @KCCINews: State of emergency declared in M...</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>RT State of emergency declared in Maryland aft...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id                                               text  \\\n",
       "0  ID_1001136696589631488  Flash floods struck a Maryland city on Sunday,...   \n",
       "1  ID_1001136950345109504  State of emergency declared for Maryland flood...   \n",
       "2  ID_1001137334056833024  Other parts of Maryland also saw significant d...   \n",
       "3  ID_1001138374923579392  Catastrophic Flooding Slams Ellicott City, Mar...   \n",
       "4  ID_1001138377717157888  WATCH: 1 missing after flash #FLOODING devasta...   \n",
       "5  ID_1001141769155891200  Watch Live: Aerials of damage after historic f...   \n",
       "6  ID_1001143589630427136  One person is reported missing as a state of e...   \n",
       "7  ID_1001143679267098624  Monday May 28 - Morning Report: National Guard...   \n",
       "8  ID_1001144314897018880  One man is still missing after flash flooding ...   \n",
       "9  ID_1001144944717914112  RT @KCCINews: State of emergency declared in M...   \n",
       "\n",
       "                 location                raw_prediction  \\\n",
       "0                Maryland                      Maryland   \n",
       "1                Maryland                      Maryland   \n",
       "2      Baltimore Maryland   Maryland Baltimore Maryland   \n",
       "3  Ellicott City Maryland  El ##lic ##ott City Maryland   \n",
       "4  Ellicott City Maryland  El ##lic ##ott City Maryland   \n",
       "5  Ellicott City Maryland  El ##lic ##ott City Maryland   \n",
       "6                Maryland                      Maryland   \n",
       "7      Arlington Maryland            Maryland Arlington   \n",
       "8  Ellicott City Maryland  El ##lic ##ott City Maryland   \n",
       "9                Maryland                      Maryland   \n",
       "\n",
       "               prediction                                       cleaned_text  \n",
       "0                Maryland  Flash floods struck a Maryland city on Sunday ...  \n",
       "1                Maryland  State of emergency declared for Maryland flood...  \n",
       "2                Maryland  Other parts of Maryland also saw significant d...  \n",
       "3  Ellicott City Maryland  Catastrophic Flooding Slams Ellicott City Mary...  \n",
       "4  Ellicott City Maryland  WATCH 1 missing after flash FLOODING devastate...  \n",
       "5  Ellicott City Maryland  Watch Live Aerials of damage after historic fl...  \n",
       "6                Maryland  One person is reported missing as a state of e...  \n",
       "7      Arlington Maryland  Monday May 28   Morning Report National Guards...  \n",
       "8  Ellicott City Maryland  One man is still missing after flash flooding ...  \n",
       "9                Maryland  RT State of emergency declared in Maryland aft...  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['prediction', 'cleaned_text']] = df.apply(lambda x: pd.Series(TLAST_postprocess(x, True)), axis=1)\n",
    "#df['prediction'] = df.apply(heuristic_postprocess_1, axis=1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5133498770639104"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jiwer\n",
    "def compute_wer_eval(df, col1='location', col2='prediction'):\n",
    "    def calculate_wer(row):\n",
    "        return jiwer.wer(str(row[col1]), str(row[col2]))\n",
    "    df['WER'] = df.apply(calculate_wer, axis=1)\n",
    "    average_wer = df['WER'].mean()\n",
    "    return df, average_wer\n",
    "\n",
    "# Eval\n",
    "df, average_wer = compute_wer_eval(df, col2='prediction')\n",
    "df.to_csv(\"./train_pred.csv\")\n",
    "average_wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Filter problematic lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "      <th>raw_prediction</th>\n",
       "      <th>prediction</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>WER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Other parts of Maryland also saw significant d...</td>\n",
       "      <td>Baltimore Maryland</td>\n",
       "      <td>Maryland Baltimore Maryland</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>Other parts of Maryland also saw significant d...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Howard County Executive Allan Kittleman said M...</td>\n",
       "      <td>Howard</td>\n",
       "      <td>Howard County</td>\n",
       "      <td>Howard County</td>\n",
       "      <td>Howard County Executive Allan Kittleman said M...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @CristianiCasco: State of Emergency declare...</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>El ##lic ##ott ##C ##ity</td>\n",
       "      <td>Catastrophic EllicottCity</td>\n",
       "      <td>RT State of Emergency declared Catastrophic fl...</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I liked a @YouTube video  CRAZY MARYLAND FLOOD...</td>\n",
       "      <td>Ellicott City MARYLAND Maryland</td>\n",
       "      <td>MA ##R ##Y ##LA ##ND El ##lic ##ott City</td>\n",
       "      <td>CRAZY MARYLAND Ellicott City</td>\n",
       "      <td>I liked a video CRAZY MARYLAND FLOODING   Flas...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>During the devastating Maryland flash floods, ...</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "      <td>Maryland El ##lic ##ott</td>\n",
       "      <td>Ellicott Maryland</td>\n",
       "      <td>During the devastating Maryland flash floods w...</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4478</th>\n",
       "      <td>RT @tomtravel2: Detailed hotel damage assessme...</td>\n",
       "      <td>Caribbean</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4479</th>\n",
       "      <td>Death toll from the 7.1-magnitude earthquake w...</td>\n",
       "      <td>Mexico Mexico City</td>\n",
       "      <td>Mexico Mexico City</td>\n",
       "      <td>City Mexico</td>\n",
       "      <td>Death toll from the 71 magnitude earthquake wh...</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4480</th>\n",
       "      <td>Welcome to Thailand Ambassador Meir Shlomo, an...</td>\n",
       "      <td>Israels Mexico City Thailand</td>\n",
       "      <td>Thailand Israel ##s Mexico</td>\n",
       "      <td>Mexico Thailand Ambassador</td>\n",
       "      <td>Welcome to Thailand Ambassador Meir Shlomo and...</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4481</th>\n",
       "      <td>Help Mexico after the terrible earthquake of S...</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>FuerzaMexico</td>\n",
       "      <td>Help Mexico after the terrible earthquake of S...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4482</th>\n",
       "      <td>RT @DJmag: ὄF @Ultra Mexico to donate ticket s...</td>\n",
       "      <td>Mexico Mexico City</td>\n",
       "      <td>Mexico Mexico City</td>\n",
       "      <td>City Mexico</td>\n",
       "      <td>RT ὄF Mexico to donate ticket sales to Mexico ...</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4483 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     Other parts of Maryland also saw significant d...   \n",
       "1     Howard County Executive Allan Kittleman said M...   \n",
       "2     RT @CristianiCasco: State of Emergency declare...   \n",
       "3     I liked a @YouTube video  CRAZY MARYLAND FLOOD...   \n",
       "4     During the devastating Maryland flash floods, ...   \n",
       "...                                                 ...   \n",
       "4478  RT @tomtravel2: Detailed hotel damage assessme...   \n",
       "4479  Death toll from the 7.1-magnitude earthquake w...   \n",
       "4480  Welcome to Thailand Ambassador Meir Shlomo, an...   \n",
       "4481  Help Mexico after the terrible earthquake of S...   \n",
       "4482  RT @DJmag: ὄF @Ultra Mexico to donate ticket s...   \n",
       "\n",
       "                             location  \\\n",
       "0                  Baltimore Maryland   \n",
       "1                              Howard   \n",
       "2                            Maryland   \n",
       "3     Ellicott City MARYLAND Maryland   \n",
       "4              Ellicott City Maryland   \n",
       "...                               ...   \n",
       "4478                        Caribbean   \n",
       "4479               Mexico Mexico City   \n",
       "4480     Israels Mexico City Thailand   \n",
       "4481                           Mexico   \n",
       "4482               Mexico Mexico City   \n",
       "\n",
       "                                raw_prediction                    prediction  \\\n",
       "0                  Maryland Baltimore Maryland                      Maryland   \n",
       "1                                Howard County                 Howard County   \n",
       "2                     El ##lic ##ott ##C ##ity     Catastrophic EllicottCity   \n",
       "3     MA ##R ##Y ##LA ##ND El ##lic ##ott City  CRAZY MARYLAND Ellicott City   \n",
       "4                      Maryland El ##lic ##ott             Ellicott Maryland   \n",
       "...                                        ...                           ...   \n",
       "4478                                       NaN                                 \n",
       "4479                        Mexico Mexico City                   City Mexico   \n",
       "4480                Thailand Israel ##s Mexico    Mexico Thailand Ambassador   \n",
       "4481                                    Mexico                  FuerzaMexico   \n",
       "4482                        Mexico Mexico City                   City Mexico   \n",
       "\n",
       "                                           cleaned_text       WER  \n",
       "0     Other parts of Maryland also saw significant d...  0.500000  \n",
       "1     Howard County Executive Allan Kittleman said M...  1.000000  \n",
       "2     RT State of Emergency declared Catastrophic fl...  2.000000  \n",
       "3     I liked a video CRAZY MARYLAND FLOODING   Flas...  1.000000  \n",
       "4     During the devastating Maryland flash floods w...  0.333333  \n",
       "...                                                 ...       ...  \n",
       "4478                                                NaN  1.000000  \n",
       "4479  Death toll from the 71 magnitude earthquake wh...  0.666667  \n",
       "4480  Welcome to Thailand Ambassador Meir Shlomo and...  0.750000  \n",
       "4481  Help Mexico after the terrible earthquake of S...  1.000000  \n",
       "4482  RT ὄF Mexico to donate ticket sales to Mexico ...  0.666667  \n",
       "\n",
       "[4483 rows x 6 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered = df[df['location'].notna()]\n",
    "data_filtered = data_filtered[data_filtered['WER'] != 0]\n",
    "data_filtered = data_filtered.drop(columns=['tweet_id'])\n",
    "data_filtered = data_filtered.reset_index(drop=True)\n",
    "data_filtered.to_csv(\"problematic_train_tlast1.csv\")\n",
    "data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
