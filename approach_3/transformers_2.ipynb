{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: 0.5em; background-color: #1876d1; color: #fff; font-weight: bold; font-size: 1.4em;\">\n",
    "    [Approach 3]  Location Mention Recognition - NER BERT Transformer\n",
    "</div>\n",
    "\n",
    "In this Jupyter notebook, we will use Name Entity Recognition to extract from X (Twitter formely) tweets Location Mention from Emergency Situation.\n",
    "\n",
    "Note :\n",
    "* Do NER\n",
    "* Try BERT Model\n",
    "* Extract Location Mention\n",
    "\n",
    "---\n",
    "<b>#Microsoft Learn Challenge, #Zindi, #Hamad Bin Khalifa University </b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (4.66.4)\n"
     ]
    }
   ],
   "source": [
    "#!pip install simpletransformers\n",
    "#!pip install pyspellchecker\n",
    "#!pip install stanza\n",
    "#!pip install nltk\n",
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import stanza, os, sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from simpletransformers.ner import NERModel, NERArgs\n",
    "\n",
    "# utils setup\n",
    "current_directory = os.getcwd()\n",
    "root_directory = os.path.abspath(os.path.join(current_directory, os.pardir))\n",
    "sys.path.append(root_directory)\n",
    "\n",
    "# custom utils\n",
    "from utils.io import Predictions\n",
    "from utils.io import LMR_BILOU_Scrapper\n",
    "from utils.preprocessing import Preprocess"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exploring Data**\n",
    "\n",
    "The provided Train.csv contain many missing value so we have to get data from initial source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: california_wildfires_2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.29file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: canada_wildfires_2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.15file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: cyclone_idai_2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.14file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: ecuador_earthquake_2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.67file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: greece_wildfires_2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.41file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_dorian_2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.05file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_florence_2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.22file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_harvey_2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.60file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_irma_2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.26file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_maria_2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:01<00:00,  1.44file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_matthew_2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.68file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: italy_earthquake_aug_2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.76file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: kaikoura_earthquake_2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.60file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: kerala_floods_2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.35file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: maryland_floods_2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  3.07file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: midwestern_us_floods_2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.35file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: pakistan_earthquake_2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.46file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: puebla_mexico_earthquake_2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.53file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: srilanka_floods_2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.90file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "LMR_BILOU_Scrapper(output_dir=\"../data/self_scrapped/bilou\").run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let concatenate out dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SHAPE:  (363376, 3)\n",
      "DEV   SHAPE:  (52038, 3)\n"
     ]
    }
   ],
   "source": [
    "train_dfs = []\n",
    "dev_dfs   = []\n",
    "path_dfs  = \"../data/self_scrapped/\"\n",
    "for filename in os.listdir(path_dfs):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(path_dfs, filename)\n",
    "        if filename.startswith(\"train\"):\n",
    "            df = pd.read_csv(file_path)\n",
    "            train_dfs.append(df)\n",
    "        elif filename.startswith(\"dev\"):\n",
    "            df = pd.read_csv(file_path)\n",
    "            dev_dfs.append(df)\n",
    "\n",
    "df_train = pd.concat(train_dfs, ignore_index=True) if train_dfs else pd.DataFrame()\n",
    "df_dev = pd.concat(dev_dfs, ignore_index=True) if dev_dfs else pd.DataFrame()\n",
    "\n",
    "print(\"TRAIN SHAPE: \", df_train.shape)\n",
    "print(\"DEV   SHAPE: \", df_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_sentence</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>Nearly</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>half</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>#</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>houses</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>checked</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>#</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>fire</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>stricken</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>areas</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>deemed</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>#</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>uninhabitable</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>#</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>GO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>#</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>PrayForGreece</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>#</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>PrayForAthens</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>#</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>AthensFires</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>἞C἟7</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>GREECE_WILDFIRES_2018_1</td>\n",
       "      <td>RT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>GREECE_WILDFIRES_2018_1</td>\n",
       "      <td>@anadoluagency</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>GREECE_WILDFIRES_2018_1</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>GREECE_WILDFIRES_2018_1</td>\n",
       "      <td>#</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>GREECE_WILDFIRES_2018_1</td>\n",
       "      <td>Greece</td>\n",
       "      <td>U-CTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>GREECE_WILDFIRES_2018_1</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id_sentence            word     tag\n",
       "0   GREECE_WILDFIRES_2018_0          Nearly       O\n",
       "1   GREECE_WILDFIRES_2018_0            half       O\n",
       "2   GREECE_WILDFIRES_2018_0              of       O\n",
       "3   GREECE_WILDFIRES_2018_0               #       O\n",
       "4   GREECE_WILDFIRES_2018_0          houses       O\n",
       "5   GREECE_WILDFIRES_2018_0         checked       O\n",
       "6   GREECE_WILDFIRES_2018_0              in       O\n",
       "7   GREECE_WILDFIRES_2018_0               #       O\n",
       "8   GREECE_WILDFIRES_2018_0            fire       O\n",
       "9   GREECE_WILDFIRES_2018_0               -       O\n",
       "10  GREECE_WILDFIRES_2018_0        stricken       O\n",
       "11  GREECE_WILDFIRES_2018_0           areas       O\n",
       "12  GREECE_WILDFIRES_2018_0          deemed       O\n",
       "13  GREECE_WILDFIRES_2018_0               #       O\n",
       "14  GREECE_WILDFIRES_2018_0   uninhabitable       O\n",
       "15  GREECE_WILDFIRES_2018_0               #       O\n",
       "16  GREECE_WILDFIRES_2018_0              GO       O\n",
       "17  GREECE_WILDFIRES_2018_0               #       O\n",
       "18  GREECE_WILDFIRES_2018_0   PrayForGreece       O\n",
       "19  GREECE_WILDFIRES_2018_0               #       O\n",
       "20  GREECE_WILDFIRES_2018_0   PrayForAthens       O\n",
       "21  GREECE_WILDFIRES_2018_0               #       O\n",
       "22  GREECE_WILDFIRES_2018_0     AthensFires       O\n",
       "23  GREECE_WILDFIRES_2018_0            ἞C἟7       O\n",
       "24  GREECE_WILDFIRES_2018_1              RT       O\n",
       "25  GREECE_WILDFIRES_2018_1  @anadoluagency       O\n",
       "26  GREECE_WILDFIRES_2018_1               :       O\n",
       "27  GREECE_WILDFIRES_2018_1               #       O\n",
       "28  GREECE_WILDFIRES_2018_1          Greece  U-CTRY\n",
       "29  GREECE_WILDFIRES_2018_1               :       O"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_sentence    0\n",
      "word           0\n",
      "tag            0\n",
      "dtype: int64\n",
      "id_sentence    0\n",
      "word           0\n",
      "tag            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train.isnull().sum())\n",
    "print(df_dev.isnull().sum())\n",
    "df_train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = Preprocess.remove_special_characters(df_train, column_name='word')\n",
    "df_dev   = Preprocess.remove_special_characters(df_dev, column_name='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess.treat_hashtags(\"#EddisonHermond missing after catastrophic flood hits #EllicottCity #Maryland; damage believed worse than 20\")\n",
    "#Preprocess.remove_stop_words(\"#EddisonHermond missing after catastrophic flood hits #EllicottCity #Maryland; damage believed worse than 20\")\n",
    "#Preprocess.correct_spelling(\"#EddisonHermond missings after catastrophic flood hits #EllicottCity #Maryland; damage believed worse than 20\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BIO Tagging**\n",
    "\n",
    "BIO stands for Begin, Inside, and Outside. It’s a method for tagging tokens (words or subwords) in a sequence to identify entities within the text. Each token in the text is assigned a tag that indicates whether it is at the beginning of an entity, inside an entity, or outside of any entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"id_sentence\"] = LabelEncoder().fit_transform(df_train[\"id_sentence\"])\n",
    "df_dev[\"id_sentence\"]   = LabelEncoder().fit_transform(df_dev[\"id_sentence\"])\n",
    "df_train[\"tag\"]         = df_train[\"tag\"].str.upper()\n",
    "df_dev[\"tag\"]           = df_dev[\"tag\"].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_sentence</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3542</td>\n",
       "      <td>Nearly</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3542</td>\n",
       "      <td>half</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3542</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3542</td>\n",
       "      <td>#</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3542</td>\n",
       "      <td>houses</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_sentence    word tag\n",
       "0         3542  Nearly   O\n",
       "1         3542    half   O\n",
       "2         3542      of   O\n",
       "3         3542       #   O\n",
       "4         3542  houses   O"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prepare training, dev and test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3542</td>\n",
       "      <td>Nearly</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3542</td>\n",
       "      <td>half</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3542</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3542</td>\n",
       "      <td>#</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3542</td>\n",
       "      <td>houses</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363371</th>\n",
       "      <td>5121</td>\n",
       "      <td>Preparedness</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363372</th>\n",
       "      <td>5121</td>\n",
       "      <td>Plan</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363373</th>\n",
       "      <td>5121</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363374</th>\n",
       "      <td>5121</td>\n",
       "      <td>#</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363375</th>\n",
       "      <td>5121</td>\n",
       "      <td>HurricaneDorian</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>359947 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id            words labels\n",
       "0              3542           Nearly      O\n",
       "1              3542             half      O\n",
       "2              3542               of      O\n",
       "3              3542                #      O\n",
       "4              3542           houses      O\n",
       "...             ...              ...    ...\n",
       "363371         5121     Preparedness      O\n",
       "363372         5121             Plan      O\n",
       "363373         5121                .      O\n",
       "363374         5121                #      O\n",
       "363375         5121  HurricaneDorian      O\n",
       "\n",
       "[359947 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = df_tag[[\"tweet_id\", \"word\"]]\n",
    "# y = df_tag[\"label\"]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "X_train  = df_train[[\"id_sentence\", \"word\"]]\n",
    "X_test   = df_dev[[\"id_sentence\", \"word\"]]\n",
    "y_train  = df_train[\"tag\"]\n",
    "y_test   = df_dev[\"tag\"]\n",
    "\n",
    "train_data = pd.DataFrame({\"sentence_id\": X_train[\"id_sentence\"], \"words\": X_train[\"word\"], \"labels\": y_train})\n",
    "test_data = pd.DataFrame({\"sentence_id\": X_test[\"id_sentence\"], \"words\": X_test[\"word\"], \"labels\": y_test})\n",
    "\n",
    "train_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'U-CTRY',\n",
       " 'B-HPOI',\n",
       " 'I-HPOI',\n",
       " 'L-HPOI',\n",
       " 'B-NBHD',\n",
       " 'L-NBHD',\n",
       " 'U-CITY',\n",
       " 'U-CONT',\n",
       " 'U-STAT',\n",
       " 'B-ISL',\n",
       " 'L-ISL',\n",
       " 'U-ISL',\n",
       " 'U-OTHR',\n",
       " 'B-CITY',\n",
       " 'L-CITY',\n",
       " 'B-NPOI',\n",
       " 'L-NPOI',\n",
       " 'U-NBHD',\n",
       " 'B-CNTY',\n",
       " 'I-CNTY',\n",
       " 'L-CNTY',\n",
       " 'B-OTHR',\n",
       " 'L-OTHR',\n",
       " 'U-DIST',\n",
       " 'B-DIST',\n",
       " 'L-DIST',\n",
       " 'I-CITY',\n",
       " 'B-CTRY',\n",
       " 'L-CTRY',\n",
       " 'U-HPOI',\n",
       " 'I-DIST',\n",
       " 'B-STAT',\n",
       " 'L-STAT',\n",
       " 'I-NBHD',\n",
       " 'U-CNTY',\n",
       " 'I-NPOI',\n",
       " 'B-ST',\n",
       " 'L-ST',\n",
       " 'U-NPOI',\n",
       " 'I-OTHR',\n",
       " 'I-ST',\n",
       " 'I-CTRY',\n",
       " 'B-CONT',\n",
       " 'L-CONT',\n",
       " 'I-STAT',\n",
       " 'U-ST',\n",
       " 'I-ISL']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = pd.concat([df_train, df_dev])[\"tag\"].unique().tolist()\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = NERArgs()\n",
    "model_args.num_train_epochs = 1\n",
    "model_args.learning_rate = 1e-4\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.train_batch_size = 32\n",
    "model_args.eval_batch_size = 32\n",
    "model_args.labels_list = label\n",
    "#model_args.lazy_loading = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = NERModel('bert', \"bert-base-cased\", args=model_args, labels=label, use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a102e98e78048cb9f05fa79e6599bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d304ccedd51140bf9351baf0ad6c43fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eebe07e6ddb941f697d406e00f0253db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 1:   0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(450, 0.15278238881586326)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train_model(train_data, eval_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea8a404f18b44b0a604864a9015ce68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37fb0a9b55884f16bfe01734141ad8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-STAT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-HPOI seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-CNTY seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-CITY seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-STAT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-CTRY seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-HPOI seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-NPOI seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-OTHR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-CITY seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-DIST seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-NBHD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-DIST seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-ST seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-CONT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-CTRY seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-NPOI seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-ISL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-CONT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-CNTY seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-ISL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-NBHD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-OTHR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    }
   ],
   "source": [
    "result, model_outputs, wrong_preds = model.eval_model(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1af56374a549fa8f29eb8b54c0c602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb09275a03c4061b104b5165b950c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions, raw_outputs = model.predict([\n",
    "    \"Elicott City, Maryland, struck by catastrophic flooding; 1 missing.\",\n",
    "    \"Memorial Day weekend floods ravage Maryland town\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'Elicott': 'B-CITY'},\n",
       "  {'City,': 'L-CITY'},\n",
       "  {'Maryland,': 'U-STAT'},\n",
       "  {'struck': 'O'},\n",
       "  {'by': 'O'},\n",
       "  {'catastrophic': 'O'},\n",
       "  {'flooding;': 'O'},\n",
       "  {'1': 'O'},\n",
       "  {'missing.': 'O'}],\n",
       " [{'Memorial': 'O'},\n",
       "  {'Day': 'O'},\n",
       "  {'weekend': 'O'},\n",
       "  {'floods': 'O'},\n",
       "  {'ravage': 'O'},\n",
       "  {'Maryland': 'U-STAT'},\n",
       "  {'town': 'O'}]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Make prediction for Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612caaf6bd0e44118aa14db1f4d4b680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6b69f6c97d4f23943507b8d7ab1827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Prediction:   0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Data and Preprocess\n",
    "# df_context = pd.read_csv('../data/provided/Test.csv')\n",
    "# df_context = Preprocess.remove_special_characters(df_context, column_name='text')\n",
    "# df_context['text'] = df_context['text'].apply(lambda x: Preprocess.treat_hashtags(x))\n",
    "# df_context['text'] = df_context['text'].apply(lambda x: Preprocess.correct_spelling(x))\n",
    "# #df_context['text'] = df_context['text'].apply(lambda x: Preprocess.remove_stop_words(x))\n",
    "# df_context.to_csv(\"../data/provided/Test-processed.csv\")\n",
    "\n",
    "df_context = pd.read_csv('../data/provided/Test-processed.csv')\n",
    "\n",
    "ids = df_context[\"tweet_id\"].values\n",
    "tweets = df_context[\"text\"].values\n",
    "\n",
    "# Make prediction\n",
    "predictions, raw_outputs = model.predict(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to ../submissions/submission_3.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract Location Mention based on model output\n",
    "results = []\n",
    "for sentence in predictions:\n",
    "    result = \" \".join([word for d in sentence for word, tag in d.items() if tag != 'O'])\n",
    "    results.append(result)\n",
    "\n",
    "Predictions.to_csv(ids, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ID_1001155505459486720</td>\n",
       "      <td>SOLDER MISSING IN flood PRAY FOR EDDISON HERMO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ID_1001155756371136512</td>\n",
       "      <td>ut timer Police searching for missing person a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>ID_1001164907587538944</td>\n",
       "      <td>Ellicott City FLOODING pictures marchland Gove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>ID_1001178904617476096</td>\n",
       "      <td>@CBSNews Our Harts goy out to a Fellow Soldier...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>ID_1001179909245587456</td>\n",
       "      <td>CRAZY video Roaring flash floods struck a marc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2491</th>\n",
       "      <td>2935</td>\n",
       "      <td>ID_914995385743167488</td>\n",
       "      <td>Hurricane Maria left devastating damage in pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2492</th>\n",
       "      <td>2936</td>\n",
       "      <td>ID_915002992214110208</td>\n",
       "      <td>Artificial Intelligence Raises pe Million for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2493</th>\n",
       "      <td>2938</td>\n",
       "      <td>ID_915026957758328832</td>\n",
       "      <td>@HannahStocking I live the Mexico earthquake a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2494</th>\n",
       "      <td>2939</td>\n",
       "      <td>ID_915253441726889984</td>\n",
       "      <td>ut @GlobalCalgary: watch National Taco Day in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>2941</td>\n",
       "      <td>ID_916099144116191232</td>\n",
       "      <td>ut @CBSSF: carols lantana Donates $100K To Mex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2496 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                tweet_id  \\\n",
       "0              1  ID_1001155505459486720   \n",
       "1              2  ID_1001155756371136512   \n",
       "2              4  ID_1001164907587538944   \n",
       "3              5  ID_1001178904617476096   \n",
       "4              6  ID_1001179909245587456   \n",
       "...          ...                     ...   \n",
       "2491        2935   ID_914995385743167488   \n",
       "2492        2936   ID_915002992214110208   \n",
       "2493        2938   ID_915026957758328832   \n",
       "2494        2939   ID_915253441726889984   \n",
       "2495        2941   ID_916099144116191232   \n",
       "\n",
       "                                                   text  \n",
       "0     SOLDER MISSING IN flood PRAY FOR EDDISON HERMO...  \n",
       "1     ut timer Police searching for missing person a...  \n",
       "2     Ellicott City FLOODING pictures marchland Gove...  \n",
       "3     @CBSNews Our Harts goy out to a Fellow Soldier...  \n",
       "4     CRAZY video Roaring flash floods struck a marc...  \n",
       "...                                                 ...  \n",
       "2491  Hurricane Maria left devastating damage in pre...  \n",
       "2492  Artificial Intelligence Raises pe Million for ...  \n",
       "2493  @HannahStocking I live the Mexico earthquake a...  \n",
       "2494  ut @GlobalCalgary: watch National Taco Day in ...  \n",
       "2495  ut @CBSSF: carols lantana Donates $100K To Mex...  \n",
       "\n",
       "[2496 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_context = pd.read_csv('../data/provided/Test-processed.csv')\n",
    "df_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
