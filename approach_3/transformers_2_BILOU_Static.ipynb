{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: 0.5em; background-color: #1876d1; color: #fff; font-weight: bold; font-size: 1.4em;\">\n",
    "    [Approach 3]  Location Mention Recognition - NER BERT Transformer\n",
    "</div>\n",
    "\n",
    "In this Jupyter notebook, we will use Name Entity Recognition to extract from X (Twitter formely) tweets Location Mention from Emergency Situation.\n",
    "\n",
    "Note :\n",
    "* Do NER\n",
    "* Try BERT Model\n",
    "* Extract Location Mention\n",
    "\n",
    "---\n",
    "<b>#Microsoft Learn Challenge, #Zindi, #Hamad Bin Khalifa University </b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install simpletransformers\n",
    "#!pip install pyspellchecker\n",
    "#!pip install stanza\n",
    "#!pip install nltk\n",
    "#!pip install python-dotenv\n",
    "#!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import stanza, os, sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from simpletransformers.ner import NERModel, NERArgs\n",
    "\n",
    "# utils setup\n",
    "current_directory = os.getcwd()\n",
    "root_directory = os.path.abspath(os.path.join(current_directory, os.pardir))\n",
    "sys.path.append(root_directory)\n",
    "\n",
    "# custom utils\n",
    "from utils.io import Predictions\n",
    "from utils.io import LMR_BILOU_Scrapper\n",
    "from utils.preprocessing import Preprocess"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exploring Data**\n",
    "\n",
    "The provided Train.csv contain many missing value so we have to get data from initial source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: california_wildfires_2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.21file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: canada_wildfires_2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.34file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: cyclone_idai_2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.36file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: ecuador_earthquake_2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.71file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: greece_wildfires_2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.51file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_dorian_2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.33file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_florence_2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.25file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_harvey_2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.25file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_irma_2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.38file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_maria_2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.57file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_matthew_2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.70file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: italy_earthquake_aug_2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.75file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: kaikoura_earthquake_2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.53file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: kerala_floods_2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.11file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: maryland_floods_2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  3.01file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: midwestern_us_floods_2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.36file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: pakistan_earthquake_2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.56file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: puebla_mexico_earthquake_2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.50file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: srilanka_floods_2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  3.07file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "LMR_BILOU_Scrapper(output_dir=\"../data/self_scrapped/bilou\").run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let concatenate out dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SHAPE:  (363376, 3)\n",
      "DEV   SHAPE:  (52038, 3)\n"
     ]
    }
   ],
   "source": [
    "train_dfs = []\n",
    "dev_dfs   = []\n",
    "path_dfs  = \"../data/self_scrapped/bilou/\"\n",
    "for filename in os.listdir(path_dfs):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(path_dfs, filename)\n",
    "        if filename.startswith(\"train\"):\n",
    "            df = pd.read_csv(file_path)\n",
    "            train_dfs.append(df)\n",
    "        elif filename.startswith(\"dev\"):\n",
    "            df = pd.read_csv(file_path)\n",
    "            dev_dfs.append(df)\n",
    "\n",
    "df_train = pd.concat(train_dfs, ignore_index=True) if train_dfs else pd.DataFrame()\n",
    "df_dev = pd.concat(dev_dfs, ignore_index=True) if dev_dfs else pd.DataFrame()\n",
    "\n",
    "print(\"TRAIN SHAPE: \", df_train.shape)\n",
    "print(\"DEV   SHAPE: \", df_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_sentence</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>Nearly</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>half</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>#</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>houses</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>checked</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>#</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>fire</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>stricken</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>areas</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>deemed</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>#</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>uninhabitable</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>#</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>GO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>#</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>PrayForGreece</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>#</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>PrayForAthens</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>#</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>AthensFires</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>GREECE_WILDFIRES_2018_0</td>\n",
       "      <td>἞C἟7</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>GREECE_WILDFIRES_2018_1</td>\n",
       "      <td>RT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>GREECE_WILDFIRES_2018_1</td>\n",
       "      <td>@anadoluagency</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>GREECE_WILDFIRES_2018_1</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>GREECE_WILDFIRES_2018_1</td>\n",
       "      <td>#</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>GREECE_WILDFIRES_2018_1</td>\n",
       "      <td>Greece</td>\n",
       "      <td>U-CTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>GREECE_WILDFIRES_2018_1</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id_sentence            word     tag\n",
       "0   GREECE_WILDFIRES_2018_0          Nearly       O\n",
       "1   GREECE_WILDFIRES_2018_0            half       O\n",
       "2   GREECE_WILDFIRES_2018_0              of       O\n",
       "3   GREECE_WILDFIRES_2018_0               #       O\n",
       "4   GREECE_WILDFIRES_2018_0          houses       O\n",
       "5   GREECE_WILDFIRES_2018_0         checked       O\n",
       "6   GREECE_WILDFIRES_2018_0              in       O\n",
       "7   GREECE_WILDFIRES_2018_0               #       O\n",
       "8   GREECE_WILDFIRES_2018_0            fire       O\n",
       "9   GREECE_WILDFIRES_2018_0               -       O\n",
       "10  GREECE_WILDFIRES_2018_0        stricken       O\n",
       "11  GREECE_WILDFIRES_2018_0           areas       O\n",
       "12  GREECE_WILDFIRES_2018_0          deemed       O\n",
       "13  GREECE_WILDFIRES_2018_0               #       O\n",
       "14  GREECE_WILDFIRES_2018_0   uninhabitable       O\n",
       "15  GREECE_WILDFIRES_2018_0               #       O\n",
       "16  GREECE_WILDFIRES_2018_0              GO       O\n",
       "17  GREECE_WILDFIRES_2018_0               #       O\n",
       "18  GREECE_WILDFIRES_2018_0   PrayForGreece       O\n",
       "19  GREECE_WILDFIRES_2018_0               #       O\n",
       "20  GREECE_WILDFIRES_2018_0   PrayForAthens       O\n",
       "21  GREECE_WILDFIRES_2018_0               #       O\n",
       "22  GREECE_WILDFIRES_2018_0     AthensFires       O\n",
       "23  GREECE_WILDFIRES_2018_0            ἞C἟7       O\n",
       "24  GREECE_WILDFIRES_2018_1              RT       O\n",
       "25  GREECE_WILDFIRES_2018_1  @anadoluagency       O\n",
       "26  GREECE_WILDFIRES_2018_1               :       O\n",
       "27  GREECE_WILDFIRES_2018_1               #       O\n",
       "28  GREECE_WILDFIRES_2018_1          Greece  U-CTRY\n",
       "29  GREECE_WILDFIRES_2018_1               :       O"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_sentence    0\n",
      "word           6\n",
      "tag            0\n",
      "dtype: int64\n",
      "id_sentence    0\n",
      "word           0\n",
      "tag            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train.isnull().sum())\n",
    "print(df_dev.isnull().sum())\n",
    "df_train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = Preprocess.remove_non_ascii(df_train, column_name='word')\n",
    "df_dev   = Preprocess.remove_non_ascii(df_dev, column_name='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess.treat_hashtags(\"#EddisonHermond missing after catastrophic flood hits #EllicottCity #Maryland; damage believed worse than 20\")\n",
    "#Preprocess.remove_stop_words(\"#EddisonHermond missing after catastrophic flood hits #EllicottCity #Maryland; damage believed worse than 20\")\n",
    "#Preprocess.correct_spelling(\"#EddisonHermond missings after catastrophic flood hits #EllicottCity #Maryland; damage believed worse than 20\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BIO Tagging**\n",
    "\n",
    "BIO stands for Begin, Inside, and Outside. It’s a method for tagging tokens (words or subwords) in a sequence to identify entities within the text. Each token in the text is assigned a tag that indicates whether it is at the beginning of an entity, inside an entity, or outside of any entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"id_sentence\"] = LabelEncoder().fit_transform(df_train[\"id_sentence\"])\n",
    "df_dev[\"id_sentence\"]   = LabelEncoder().fit_transform(df_dev[\"id_sentence\"])\n",
    "df_train[\"tag\"]         = df_train[\"tag\"].str.upper()\n",
    "df_dev[\"tag\"]           = df_dev[\"tag\"].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_sentence</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3542</td>\n",
       "      <td>Nearly</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3542</td>\n",
       "      <td>half</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3542</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3542</td>\n",
       "      <td>#</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3542</td>\n",
       "      <td>houses</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_sentence    word tag\n",
       "0         3542  Nearly   O\n",
       "1         3542    half   O\n",
       "2         3542      of   O\n",
       "3         3542       #   O\n",
       "4         3542  houses   O"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prepare training, dev and test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3542</td>\n",
       "      <td>Nearly</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3542</td>\n",
       "      <td>half</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3542</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3542</td>\n",
       "      <td>#</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3542</td>\n",
       "      <td>houses</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363371</th>\n",
       "      <td>5121</td>\n",
       "      <td>Preparedness</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363372</th>\n",
       "      <td>5121</td>\n",
       "      <td>Plan</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363373</th>\n",
       "      <td>5121</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363374</th>\n",
       "      <td>5121</td>\n",
       "      <td>#</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363375</th>\n",
       "      <td>5121</td>\n",
       "      <td>HurricaneDorian</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>363370 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id            words labels\n",
       "0              3542           Nearly      O\n",
       "1              3542             half      O\n",
       "2              3542               of      O\n",
       "3              3542                #      O\n",
       "4              3542           houses      O\n",
       "...             ...              ...    ...\n",
       "363371         5121     Preparedness      O\n",
       "363372         5121             Plan      O\n",
       "363373         5121                .      O\n",
       "363374         5121                #      O\n",
       "363375         5121  HurricaneDorian      O\n",
       "\n",
       "[363370 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = df_tag[[\"tweet_id\", \"word\"]]\n",
    "# y = df_tag[\"label\"]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "X_train  = df_train[[\"id_sentence\", \"word\"]]\n",
    "X_test   = df_dev[[\"id_sentence\", \"word\"]]\n",
    "y_train  = df_train[\"tag\"]\n",
    "y_test   = df_dev[\"tag\"]\n",
    "\n",
    "train_data = pd.DataFrame({\"sentence_id\": X_train[\"id_sentence\"], \"words\": X_train[\"word\"], \"labels\": y_train})\n",
    "test_data = pd.DataFrame({\"sentence_id\": X_test[\"id_sentence\"], \"words\": X_test[\"word\"], \"labels\": y_test})\n",
    "\n",
    "train_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'U-CTRY',\n",
       " 'B-HPOI',\n",
       " 'I-HPOI',\n",
       " 'L-HPOI',\n",
       " 'B-NBHD',\n",
       " 'L-NBHD',\n",
       " 'U-CITY',\n",
       " 'U-CONT',\n",
       " 'U-STAT',\n",
       " 'B-ISL',\n",
       " 'L-ISL',\n",
       " 'U-ISL',\n",
       " 'U-OTHR',\n",
       " 'B-CITY',\n",
       " 'L-CITY',\n",
       " 'B-NPOI',\n",
       " 'L-NPOI',\n",
       " 'U-NBHD',\n",
       " 'B-CNTY',\n",
       " 'I-CNTY',\n",
       " 'L-CNTY',\n",
       " 'B-OTHR',\n",
       " 'L-OTHR',\n",
       " 'U-DIST',\n",
       " 'B-DIST',\n",
       " 'L-DIST',\n",
       " 'I-CITY',\n",
       " 'B-CTRY',\n",
       " 'L-CTRY',\n",
       " 'U-HPOI',\n",
       " 'I-DIST',\n",
       " 'B-STAT',\n",
       " 'L-STAT',\n",
       " 'I-NBHD',\n",
       " 'U-CNTY',\n",
       " 'I-NPOI',\n",
       " 'B-ST',\n",
       " 'L-ST',\n",
       " 'U-NPOI',\n",
       " 'I-OTHR',\n",
       " 'I-ST',\n",
       " 'I-CTRY',\n",
       " 'B-CONT',\n",
       " 'L-CONT',\n",
       " 'I-STAT',\n",
       " 'U-ST',\n",
       " 'I-ISL']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = pd.concat([df_train, df_dev])[\"tag\"].unique().tolist()\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgenereux-akotenou\u001b[0m (\u001b[33mgenereux-akotenou-local\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126cd3a6652c4a978221442ea8b0f37b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011145411577955303, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/genereux/Documents/OPENCORE/DATA_CTF/MSL-Location-Mention-Recognition/approach_3/wandb/run-20240824_192120-l0ico439</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/genereux-akotenou-local/lmr-bilou/runs/l0ico439' target=\"_blank\">lmr-bilou</a></strong> to <a href='https://wandb.ai/genereux-akotenou-local/lmr-bilou' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/genereux-akotenou-local/lmr-bilou' target=\"_blank\">https://wandb.ai/genereux-akotenou-local/lmr-bilou</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/genereux-akotenou-local/lmr-bilou/runs/l0ico439' target=\"_blank\">https://wandb.ai/genereux-akotenou-local/lmr-bilou/runs/l0ico439</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/genereux-akotenou-local/lmr-bilou/runs/l0ico439?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x341946610>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logging\n",
    "import wandb\n",
    "wandb.init(project=\"lmr-bilou\", name=\"lmr-bilou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = NERArgs()\n",
    "model_args.num_train_epochs = 1\n",
    "model_args.learning_rate = 4e-4\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.train_batch_size = 32\n",
    "model_args.eval_batch_size = 32\n",
    "model_args.labels_list = label\n",
    "\n",
    "model_args.logging_steps = 50\n",
    "model_args.save_steps = 100\n",
    "model_args.evaluate_during_training = True\n",
    "\n",
    "\n",
    "model_args.wandb_project = \"lmr-bilou\"\n",
    "model_args.wandb_kwargs = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = NERModel('bert', \"bert-base-cased\", args=model_args, labels=label, use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087632d2b3714254938258aaa6922e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea7f6f54caf40239c4507954923c5fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:l0ico439) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efeb2482e4a2466ca8ef015551734b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lmr-bilou</strong> at: <a href='https://wandb.ai/genereux-akotenou-local/lmr-bilou/runs/l0ico439' target=\"_blank\">https://wandb.ai/genereux-akotenou-local/lmr-bilou/runs/l0ico439</a><br/> View project at: <a href='https://wandb.ai/genereux-akotenou-local/lmr-bilou' target=\"_blank\">https://wandb.ai/genereux-akotenou-local/lmr-bilou</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240824_192120-l0ico439/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:l0ico439). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680b324911b3491c9ecc4c4bfc9e0b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0111674523110398, max=1.0))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/genereux/Documents/OPENCORE/DATA_CTF/MSL-Location-Mention-Recognition/approach_3/wandb/run-20240824_192249-gl0oce5i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/genereux-akotenou-local/lmr-bilou/runs/gl0oce5i' target=\"_blank\">splendid-eon-3</a></strong> to <a href='https://wandb.ai/genereux-akotenou-local/lmr-bilou' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/genereux-akotenou-local/lmr-bilou' target=\"_blank\">https://wandb.ai/genereux-akotenou-local/lmr-bilou</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/genereux-akotenou-local/lmr-bilou/runs/gl0oce5i' target=\"_blank\">https://wandb.ai/genereux-akotenou-local/lmr-bilou/runs/gl0oce5i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b435b90bad824ec9b1aefd66f503b7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 1:   0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.train_model(train_data, eval_data=test_data)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1970ce4c1044459f9201c04761ef80e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f6e09fc7cd4aee95e36ebb2a40fbe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-STAT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-HPOI seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-CNTY seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-CITY seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-STAT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-CTRY seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-HPOI seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-NPOI seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-OTHR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-CITY seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-DIST seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-NBHD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-DIST seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-ST seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-CONT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-CTRY seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-NPOI seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-ISL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-CONT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-CNTY seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-ISL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-NBHD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-OTHR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    }
   ],
   "source": [
    "result, model_outputs, wrong_preds = model.eval_model(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6a38e87d7f4513acfa6db40129467d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b53a2e4b504ca19f210479febff100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions, raw_outputs = model.predict([\n",
    "    \"Elicott City, Maryland, struck by catastrophic flooding; 1 missing.\",\n",
    "    \"Memorial Day weekend floods ravage Maryland town\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'Elicott': 'B-CITY'},\n",
       "  {'City,': 'L-CITY'},\n",
       "  {'Maryland,': 'U-STAT'},\n",
       "  {'struck': 'O'},\n",
       "  {'by': 'O'},\n",
       "  {'catastrophic': 'O'},\n",
       "  {'flooding;': 'O'},\n",
       "  {'1': 'O'},\n",
       "  {'missing.': 'O'}],\n",
       " [{'Memorial': 'O'},\n",
       "  {'Day': 'O'},\n",
       "  {'weekend': 'O'},\n",
       "  {'floods': 'O'},\n",
       "  {'ravage': 'O'},\n",
       "  {'Maryland': 'U-STAT'},\n",
       "  {'town': 'O'}]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Make prediction for Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d912c7163dd4ee08bba579a51ae3c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e601f4c450aa42ed9d0ed291e2b554ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Prediction:   0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Data and Preprocess\n",
    "df_context = pd.read_csv('../data/provided/Test.csv')\n",
    "df_context = Preprocess.remove_non_ascii(df_context, column_name='text')\n",
    "# df_context['text'] = df_context['text'].apply(lambda x: Preprocess.treat_hashtags(x))\n",
    "# df_context['text'] = df_context['text'].apply(lambda x: Preprocess.correct_spelling(x))\n",
    "# #df_context['text'] = df_context['text'].apply(lambda x: Preprocess.remove_stop_words(x))\n",
    "# df_context.to_csv(\"../data/provided/Test-processed.csv\")\n",
    "\n",
    "#df_context = pd.read_csv('../data/provided/Test-processed-2.csv')\n",
    "\n",
    "ids = df_context[\"tweet_id\"].values\n",
    "tweets = df_context[\"text\"].values\n",
    "\n",
    "# Make prediction\n",
    "predictions, raw_outputs = model.predict(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to ../submissions/submission_7.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract Location Mention based on model output\n",
    "results = []\n",
    "for sentence in predictions:\n",
    "    result = \" \".join([word for d in sentence for word, tag in d.items() if tag != 'O'])\n",
    "    if result == \"\":\n",
    "        result = \" \" \n",
    "    results.append(result)\n",
    "\n",
    "Predictions.to_csv(ids, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_context = pd.read_csv('../data/provided/Test-processed.csv')\n",
    "df_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
