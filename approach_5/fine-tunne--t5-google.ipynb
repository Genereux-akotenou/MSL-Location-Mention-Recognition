{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: 0.5em; background-color: #1876d1; color: #fff; font-weight: bold; font-size: 1.4em;\">\n",
    "    [Approach 3]  Location Mention Recognition - Fine-tunning LLM\n",
    "</div>\n",
    "\n",
    "In this Jupyter notebook, we will use LLM to extract from X (Twitter formely) tweets Location Mention from Emergency Situation.\n",
    "\n",
    "Step :\n",
    "* Retreive dataset \n",
    "* Prepare prompt for fine-tunning\n",
    "* Tested model: <span style=\"color: red;\">01-Mistral-7B-Instruct v02</span>\n",
    "\n",
    "---\n",
    "<b>#Microsoft Learn Challenge, #Zindi, #Hamad Bin Khalifa University </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install trl\n",
    "# !pip install peft\n",
    "# !pip install transformers accelerate bitsandbytes>0.37.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general utils\n",
    "import werpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os, sys, requests\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# utils setup\n",
    "current_directory = os.getcwd()\n",
    "root_directory = os.path.abspath(os.path.join(current_directory, os.pardir))\n",
    "sys.path.append(root_directory)\n",
    "\n",
    "# logging & warning\n",
    "import wandb, warnings\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"fine-tunne--t5-google.ipynb\"\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.7\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ['MALLOC_STACK_LOGGING'] = '0'\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# custom utils\n",
    "from utils.io import LMR_XML_Scrapper\n",
    "from utils.preprocessing import Preprocess\n",
    "\n",
    "# hugging face utils\n",
    "import torch\n",
    "from peft import LoraConfig\n",
    "from datasets import Dataset\n",
    "from dataclasses import dataclass\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers.utils import PaddingStrategy\n",
    "from transformers.integrations import WandbCallback\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import TFAutoModel, BitsAndBytesConfig\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preparing Data**\n",
    "\n",
    "We have to dwonload the dataset and format in such way it an be used by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LMR_XML_Scrapper(output_dir=\"../data/self_scrapped/raw-llm\").run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let concatenate out dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SHAPE:  (14392, 3)\n",
      "TEST  SHAPE:  (4066, 3)\n",
      "DEV   SHAPE:  (2056, 3)\n"
     ]
    }
   ],
   "source": [
    "train_dfs = []\n",
    "dev_dfs   = []\n",
    "test_dfs  = []\n",
    "path_dfs  = \"../data/self_scrapped/raw-llm\"\n",
    "for filename in os.listdir(path_dfs):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(path_dfs, filename)\n",
    "        if filename.startswith(\"train\"):\n",
    "            df = pd.read_csv(file_path)\n",
    "            train_dfs.append(df)\n",
    "        elif filename.startswith(\"dev\"):\n",
    "            df = pd.read_csv(file_path)\n",
    "            dev_dfs.append(df)\n",
    "        elif filename.startswith(\"test_unlabeled\"):\n",
    "            df = pd.read_csv(file_path)\n",
    "            test_dfs.append(df)\n",
    "\n",
    "df_train = pd.concat(train_dfs, ignore_index=True) if train_dfs else pd.DataFrame()\n",
    "df_test  = pd.concat(test_dfs, ignore_index=True) if test_dfs else pd.DataFrame()\n",
    "df_dev   = pd.concat(dev_dfs, ignore_index=True) if dev_dfs else pd.DataFrame()\n",
    "\n",
    "df_train.to_csv(\"../data/transformed/lmr_train.csv\")\n",
    "#df_test.to_csv(\"../data/transformed/lmr_test.csv\")\n",
    "df_dev.to_csv(\"../data/transformed/lmr_dev.csv\")\n",
    "\n",
    "print(\"TRAIN SHAPE: \", df_train.shape)\n",
    "print(\"TEST  SHAPE: \", df_test.shape)\n",
    "print(\"DEV   SHAPE: \", df_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We observe in sentencess that we have hashtag, no-ascii character, stopword , ... we have to clean data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>plain_text</th>\n",
       "      <th>xml_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_1022420413882744832</td>\n",
       "      <td>Nearly half of #houses checked in #fire-stricken areas deemed #uninhabitable #GO #PrayForGreece #PrayForAthens #AthensFires ἞C἟7</td>\n",
       "      <td>Nearly half of #houses checked in #fire-stricken areas deemed #uninhabitable #GO #PrayForGreece #PrayForAthens #AthensFires ἞C἟7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_1021778661895294976</td>\n",
       "      <td>RT @anadoluagency: #Greece: Death toll from wildfires hits 74</td>\n",
       "      <td>RT @anadoluagency: #&lt;COUNTRY&gt;Greece&lt;/COUNTRY&gt;: Death toll from wildfires hits 74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_1022015997740503042</td>\n",
       "      <td>When the essence of cooperation meets the sad reality of lifeThe IPA partner country offers financial aid to Greece to handle disaster @InterregIPACBC #Greecefires</td>\n",
       "      <td>When the essence of cooperation meets the sad reality of lifeThe IPA partner country offers financial aid to &lt;COUNTRY&gt;Greece&lt;/COUNTRY&gt; to handle disaster @InterregIPACBC #Greecefires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_1022557424585240576</td>\n",
       "      <td>We are live from the Lureio Idrima the orphanage and nursing home operared by the nuns of the Holy Trinity Monastery that was destroyed by the fire in Neos Voutzas. Here too the scene is apocalyptic.</td>\n",
       "      <td>We are live from the Lureio Idrima the orphanage and nursing home operared by the nuns of the &lt;HUMAN-MADE-POINT-OF-INTEREST&gt;Holy Trinity Monastery&lt;/HUMAN-MADE-POINT-OF-INTEREST&gt; that was destroyed by the fire in &lt;NEIGHBORHOOD&gt;Neos Voutzas&lt;/NEIGHBORHOOD&gt;. Here too the scene is apocalyptic.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_1021749412639457280</td>\n",
       "      <td>RT @AP: Greek prime minister declares 3-day national mourning period for dozens killed by wildfires near Athens.</td>\n",
       "      <td>RT @AP: Greek prime minister declares 3-day national mourning period for dozens killed by wildfires near &lt;CITY&gt;Athens&lt;/CITY&gt;.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id  \\\n",
       "0  ID_1022420413882744832   \n",
       "1  ID_1021778661895294976   \n",
       "2  ID_1022015997740503042   \n",
       "3  ID_1022557424585240576   \n",
       "4  ID_1021749412639457280   \n",
       "\n",
       "                                                                                                                                                                                                plain_text  \\\n",
       "0                                                                         Nearly half of #houses checked in #fire-stricken areas deemed #uninhabitable #GO #PrayForGreece #PrayForAthens #AthensFires ἞C἟7   \n",
       "1                                                                                                                                            RT @anadoluagency: #Greece: Death toll from wildfires hits 74   \n",
       "2                                      When the essence of cooperation meets the sad reality of lifeThe IPA partner country offers financial aid to Greece to handle disaster @InterregIPACBC #Greecefires   \n",
       "3  We are live from the Lureio Idrima the orphanage and nursing home operared by the nuns of the Holy Trinity Monastery that was destroyed by the fire in Neos Voutzas. Here too the scene is apocalyptic.   \n",
       "4                                                                                         RT @AP: Greek prime minister declares 3-day national mourning period for dozens killed by wildfires near Athens.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                            xml_text  \n",
       "0                                                                                                                                                                   Nearly half of #houses checked in #fire-stricken areas deemed #uninhabitable #GO #PrayForGreece #PrayForAthens #AthensFires ἞C἟7  \n",
       "1                                                                                                                                                                                                                   RT @anadoluagency: #<COUNTRY>Greece</COUNTRY>: Death toll from wildfires hits 74  \n",
       "2                                                                                                             When the essence of cooperation meets the sad reality of lifeThe IPA partner country offers financial aid to <COUNTRY>Greece</COUNTRY> to handle disaster @InterregIPACBC #Greecefires  \n",
       "3  We are live from the Lureio Idrima the orphanage and nursing home operared by the nuns of the <HUMAN-MADE-POINT-OF-INTEREST>Holy Trinity Monastery</HUMAN-MADE-POINT-OF-INTEREST> that was destroyed by the fire in <NEIGHBORHOOD>Neos Voutzas</NEIGHBORHOOD>. Here too the scene is apocalyptic.  \n",
       "4                                                                                                                                                                      RT @AP: Greek prime minister declares 3-day national mourning period for dozens killed by wildfires near <CITY>Athens</CITY>.  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = Preprocess.remove_non_ascii(df_train, column_name='plain_text')\n",
    "df_train = Preprocess.remove_non_ascii(df_train, column_name='xml_text')\n",
    "df_dev   = Preprocess.remove_non_ascii(df_dev, column_name='plain_text')\n",
    "df_dev   = Preprocess.remove_non_ascii(df_dev, column_name='xml_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>plain_text</th>\n",
       "      <th>xml_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_1022420413882744832</td>\n",
       "      <td>Nearly half of #houses checked in #fire-stricken areas deemed #uninhabitable #GO #PrayForGreece #PrayForAthens #AthensFires C7</td>\n",
       "      <td>Nearly half of #houses checked in #fire-stricken areas deemed #uninhabitable #GO #PrayForGreece #PrayForAthens #AthensFires C7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_1021778661895294976</td>\n",
       "      <td>RT @anadoluagency: #Greece: Death toll from wildfires hits 74</td>\n",
       "      <td>RT @anadoluagency: #&lt;COUNTRY&gt;Greece&lt;/COUNTRY&gt;: Death toll from wildfires hits 74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_1022015997740503042</td>\n",
       "      <td>When the essence of cooperation meets the sad reality of lifeThe IPA partner country offers financial aid to Greece to handle disaster @InterregIPACBC #Greecefires</td>\n",
       "      <td>When the essence of cooperation meets the sad reality of lifeThe IPA partner country offers financial aid to &lt;COUNTRY&gt;Greece&lt;/COUNTRY&gt; to handle disaster @InterregIPACBC #Greecefires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_1022557424585240576</td>\n",
       "      <td>We are live from the Lureio Idrima the orphanage and nursing home operared by the nuns of the Holy Trinity Monastery that was destroyed by the fire in Neos Voutzas. Here too the scene is apocalyptic.</td>\n",
       "      <td>We are live from the Lureio Idrima the orphanage and nursing home operared by the nuns of the &lt;HUMAN-MADE-POINT-OF-INTEREST&gt;Holy Trinity Monastery&lt;/HUMAN-MADE-POINT-OF-INTEREST&gt; that was destroyed by the fire in &lt;NEIGHBORHOOD&gt;Neos Voutzas&lt;/NEIGHBORHOOD&gt;. Here too the scene is apocalyptic.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_1021749412639457280</td>\n",
       "      <td>RT @AP: Greek prime minister declares 3-day national mourning period for dozens killed by wildfires near Athens.</td>\n",
       "      <td>RT @AP: Greek prime minister declares 3-day national mourning period for dozens killed by wildfires near &lt;CITY&gt;Athens&lt;/CITY&gt;.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id  \\\n",
       "0  ID_1022420413882744832   \n",
       "1  ID_1021778661895294976   \n",
       "2  ID_1022015997740503042   \n",
       "3  ID_1022557424585240576   \n",
       "4  ID_1021749412639457280   \n",
       "\n",
       "                                                                                                                                                                                                plain_text  \\\n",
       "0                                                                           Nearly half of #houses checked in #fire-stricken areas deemed #uninhabitable #GO #PrayForGreece #PrayForAthens #AthensFires C7   \n",
       "1                                                                                                                                            RT @anadoluagency: #Greece: Death toll from wildfires hits 74   \n",
       "2                                      When the essence of cooperation meets the sad reality of lifeThe IPA partner country offers financial aid to Greece to handle disaster @InterregIPACBC #Greecefires   \n",
       "3  We are live from the Lureio Idrima the orphanage and nursing home operared by the nuns of the Holy Trinity Monastery that was destroyed by the fire in Neos Voutzas. Here too the scene is apocalyptic.   \n",
       "4                                                                                         RT @AP: Greek prime minister declares 3-day national mourning period for dozens killed by wildfires near Athens.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                            xml_text  \n",
       "0                                                                                                                                                                     Nearly half of #houses checked in #fire-stricken areas deemed #uninhabitable #GO #PrayForGreece #PrayForAthens #AthensFires C7  \n",
       "1                                                                                                                                                                                                                   RT @anadoluagency: #<COUNTRY>Greece</COUNTRY>: Death toll from wildfires hits 74  \n",
       "2                                                                                                             When the essence of cooperation meets the sad reality of lifeThe IPA partner country offers financial aid to <COUNTRY>Greece</COUNTRY> to handle disaster @InterregIPACBC #Greecefires  \n",
       "3  We are live from the Lureio Idrima the orphanage and nursing home operared by the nuns of the <HUMAN-MADE-POINT-OF-INTEREST>Holy Trinity Monastery</HUMAN-MADE-POINT-OF-INTEREST> that was destroyed by the fire in <NEIGHBORHOOD>Neos Voutzas</NEIGHBORHOOD>. Here too the scene is apocalyptic.  \n",
       "4                                                                                                                                                                      RT @AP: Greek prime minister declares 3-day national mourning period for dozens killed by wildfires near <CITY>Athens</CITY>.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get a look of extracted tags and prepare list of entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_type</th>\n",
       "      <th>xml_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>State</td>\n",
       "      <td>STATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>County</td>\n",
       "      <td>COUNTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>City/town</td>\n",
       "      <td>CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Road/street</td>\n",
       "      <td>ROAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Island</td>\n",
       "      <td>ISLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Human-made Point-of-Interest</td>\n",
       "      <td>HUMAN-MADE-POINT-OF-INTEREST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Continent</td>\n",
       "      <td>CONTINENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Neighborhood</td>\n",
       "      <td>NEIGHBORHOOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Natural Point-of-Interest</td>\n",
       "      <td>NATURAL-POINT-OF-INTEREST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>District</td>\n",
       "      <td>DISTRICT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Country</td>\n",
       "      <td>COUNTRY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   original_type                       xml_tag\n",
       "0                          State                         STATE\n",
       "1                         County                        COUNTY\n",
       "2                      City/town                          CITY\n",
       "3                    Road/street                          ROAD\n",
       "4                         Island                        ISLAND\n",
       "5   Human-made Point-of-Interest  HUMAN-MADE-POINT-OF-INTEREST\n",
       "6                      Continent                     CONTINENT\n",
       "7                   Neighborhood                  NEIGHBORHOOD\n",
       "8      Natural Point-of-Interest     NATURAL-POINT-OF-INTEREST\n",
       "9                       District                      DISTRICT\n",
       "10                       Country                       COUNTRY"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_list = pd.read_csv(\"../utils/tag_description.csv\")\n",
    "tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tags_description = [\n",
    "    \"CONTINENT: A large continuous landmass on the Earth's surface\",\n",
    "    \"COUNTRY: A nation or territory recognized as an independent state\",\n",
    "    \"COUNTY: A geographical region within a country, often a subdivision of a state or province.\",\n",
    "    \"DISTRICT: An administrative division within a city, county, or country.\",\n",
    "    \"STATE: A larger administrative division within a country\",\n",
    "    \"CITY: A large and significant urban area\",\n",
    "    \"ROAD: A street, avenue, or highway that connects different locations.\",\n",
    "    \"ISLAND: A landmass completely surrounded by water.\",\n",
    "    \"NEIGHBORHOOD: A localized community within a city or town.\",\n",
    "    \"HUMAN-MADE-POINT-OF-INTEREST: A landmark created by humans, such as monuments or buildings.\",\n",
    "    \"NATURAL-POINT-OF-INTEREST: A location of natural significance, like mountains or rivers.\",\n",
    "]\"\"\"\n",
    "tags_description = [\n",
    "    \"CONTINENT: A large landmass on Earth.\",\n",
    "    \"COUNTRY: An independent nation.\",\n",
    "    \"COUNTY: A region within a country.\",\n",
    "    \"DISTRICT: An administrative area within a city\",\n",
    "    \"STATE: A large administrative division within a country.\",\n",
    "    \"CITY: A significant urban area.\",\n",
    "    \"ROAD: A street, avenue connecting locations.\",\n",
    "    \"ISLAND: A landmass surrounded by water.\",\n",
    "    \"NEIGHBORHOOD: A community within a city.\",\n",
    "    \"HUMAN-MADE-POINT-OF-INTEREST: A landmark created by humans, like monuments.\",\n",
    "    \"NATURAL-POINT-OF-INTEREST: A naturally significant location, like mountains.\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Init model utils**\n",
    "\n",
    "Init model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, 'float16')\n",
    "quant_config =  BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No GPU found. A GPU is needed for quantization.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m#tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\", additional_special_tokens=special_tokens)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m#model     = TFAutoModel.from_pretrained(\"google-t5/t5-small\", return_dict=True)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[39m#tokenizer = T5Tokenizer.from_pretrained('t5-small', additional_special_tokens=special_tokens)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m#model = T5ForConditionalGeneration.from_pretrained('t5-small', return_dict=True)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mmistralai/Mistral-7B-Instruct-v0.2\u001b[39m\u001b[39m\"\u001b[39m, additional_special_tokens\u001b[39m=\u001b[39mspecial_tokens)\n\u001b[0;32m---> 10\u001b[0m model     \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mmistralai/Mistral-7B-Instruct-v0.2\u001b[39m\u001b[39m\"\u001b[39m, quantization_config\u001b[39m=\u001b[39mquant_config, device_map\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    564\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    565\u001b[0m     )\n\u001b[1;32m    566\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:3202\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3199\u001b[0m     hf_quantizer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   3201\u001b[0m \u001b[39mif\u001b[39;00m hf_quantizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 3202\u001b[0m     hf_quantizer\u001b[39m.\u001b[39mvalidate_environment(\n\u001b[1;32m   3203\u001b[0m         torch_dtype\u001b[39m=\u001b[39mtorch_dtype, from_tf\u001b[39m=\u001b[39mfrom_tf, from_flax\u001b[39m=\u001b[39mfrom_flax, device_map\u001b[39m=\u001b[39mdevice_map\n\u001b[1;32m   3204\u001b[0m     )\n\u001b[1;32m   3205\u001b[0m     torch_dtype \u001b[39m=\u001b[39m hf_quantizer\u001b[39m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3206\u001b[0m     device_map \u001b[39m=\u001b[39m hf_quantizer\u001b[39m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:62\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidate_environment\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     61\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m---> 62\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo GPU found. A GPU is needed for quantization.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_accelerate_available() \u001b[39mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[1;32m     64\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mand the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No GPU found. A GPU is needed for quantization."
     ]
    }
   ],
   "source": [
    "special_tokens = ['<STATE>', '</STATE>', '<ROAD>', '</ROAD>', '<COUNTY>', '</COUNTY>', '<CONTINENT>', '</CONTINENT>', '<NATURAL-POINT-OF-INTEREST>', '</NATURAL-POINT-OF-INTEREST>', '<NEIGHBORHOOD>', '</NEIGHBORHOOD>', '<COUNTRY>', '</COUNTRY>', '<CITY>', '</CITY>', '<DISTRICT>', '</DISTRICT>', '<ISLAND>', '</ISLAND>', '<HUMAN-MADE-POINT-OF-INTEREST>', '</HUMAN-MADE-POINT-OF-INTEREST>', '[INST]', '[/INST]', '[PAD]']\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\", additional_special_tokens=special_tokens)\n",
    "#model     = TFAutoModel.from_pretrained(\"google-t5/t5-small\", return_dict=True)\n",
    "\n",
    "#tokenizer = T5Tokenizer.from_pretrained('t5-small', additional_special_tokens=special_tokens)\n",
    "#model = T5ForConditionalGeneration.from_pretrained('t5-small', return_dict=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", additional_special_tokens=special_tokens)\n",
    "model     = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", quantization_config=quant_config, device_map='auto')\n",
    "#model     = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32125, 512)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = \"[PAD]\"\n",
    "tokenizer.padding_side = \"right\"\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Including Chain-Of-Thought in the prompt design**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_template(messages):\n",
    "    formatted_messages = []\n",
    "    for message in messages:\n",
    "        if message['role'] == 'user':\n",
    "            formatted_messages.append(f\"[INST] {message['content']} [/INST]\")\n",
    "        elif message['role'] == 'assistant':\n",
    "            formatted_messages.append(f\"{message['content']}\")\n",
    "    return '<s>' + ''.join(formatted_messages) + '</s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] usr_msg1 [/INST]asst_msg1[INST] usr_msg2 [/INST]asst_msg2</s>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"usr_msg1\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"asst_msg1\"},\n",
    "    {\"role\": \"user\", \"content\": \"usr_msg2\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"asst_msg2\"},\n",
    "]\n",
    "#tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "apply_chat_template(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formating_prompt(rule_set: List[str], input_str: str, label_str: str, tokenizer: PreTrainedTokenizerBase) -> torch.Tensor:\n",
    "    rule_str = \"\\n\".join(rule_set)\n",
    "\n",
    "    # Message 1\n",
    "    usr_msg1 = \"Identify and tag all location mentions in the microblogging post using the provided location entity types.\" \\\n",
    "               f\"\\n\\nEntity List:\\n{rule_str}\\n\\n\" \\\n",
    "               \"Are the instructions clear?\"\n",
    "    asst_msg1 = \"Yes, I will tag all location entities as specified, keeping the rest of the text unchanged.\"\n",
    "\n",
    "    # Message 2\n",
    "    usr_msg2 = \"Florida Bahamas flooding shuts down train lines.\"\n",
    "    asst_msg2 = \"<STATE>Florida</STATE> <ISLAND>Bahamas</ISLAND> flooding shuts down train lines.\"\n",
    "\n",
    "    # Message 3\n",
    "    usr_msg3 = \"Explain why your answer is correct.\"\n",
    "    asst_msg3 = \"I tagged 'Florida' as <STATE> and 'Bahamas' as <ISLAND>, following the location entities list.\"\n",
    "\n",
    "    # Message 4\n",
    "    usr_msg4 = \"Now, tag another user post according to the same instructions. No explanation needed.\"\n",
    "    asst_msg4 = \"Sure, please provide the user post.\"\n",
    "\n",
    "    # Encode in conversation\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": usr_msg1},\n",
    "        {\"role\": \"assistant\", \"content\": asst_msg1},\n",
    "        {\"role\": \"user\", \"content\": usr_msg2},\n",
    "        {\"role\": \"assistant\", \"content\": asst_msg2},\n",
    "        {\"role\": \"user\", \"content\": usr_msg3},\n",
    "        {\"role\": \"assistant\", \"content\": asst_msg3},\n",
    "        {\"role\": \"user\", \"content\": usr_msg4},\n",
    "        {\"role\": \"assistant\", \"content\": asst_msg4},\n",
    "        {\"role\": \"user\", \"content\": input_str},\n",
    "        {\"role\": \"assistant\", \"content\": label_str},\n",
    "    ]\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_df(df: pd.DataFrame, rule_set: List[str], tokenizer: PreTrainedTokenizerBase) -> pd.DataFrame:\n",
    "    prompt_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        input_str = row['plain_text']\n",
    "        label_str = row['xml_text']\n",
    "        prompt_dict = formating_prompt(rule_set, input_str, label_str, tokenizer)\n",
    "        prompt_list.append(prompt_dict)\n",
    "    \n",
    "    dataset = Dataset.from_dict({\"prompt\": prompt_list})\n",
    "    #dataset = dataset.map(lambda x: {\"formatted_prompt\": tokenizer.apply_chat_template(x[\"prompt\"], tokenize=False, add_generation_prompt=False)})\n",
    "    dataset = dataset.map(lambda x: {\"formatted_prompt\": apply_chat_template(x[\"prompt\"])})\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preparing model for fine-tunning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f87c65ff3d473eba59f4f65c18da0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14392 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9108fa721dea40baa440be52ae4a378b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2056 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = {} \n",
    "dataset['train'] = create_prompt_df(df_train, tags_description, tokenizer)\n",
    "dataset['eval']  = create_prompt_df(df_dev, tags_description, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3135"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_formatted_prompt_length = max(len(prompt) for prompt in dataset['train']['formatted_prompt'])\n",
    "max_formatted_prompt_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CustomDataCollatorWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "\n",
    "    Args:\n",
    "        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n",
    "            The tokenizer used for encoding the data.\n",
    "        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "\n",
    "            - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence is provided).\n",
    "            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
    "              acceptable input length for the model if that argument is not provided.\n",
    "            - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different lengths).\n",
    "        max_length (`int`, *optional*):\n",
    "            Maximum length of the returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (`int`, *optional*):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "        return_tensors (`str`, *optional*, defaults to `\"pt\"`):\n",
    "            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        \n",
    "        # Set loss mask for all pad tokens\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        # Compute loss mask for appropriate tokens only\n",
    "        for i in range(batch['input_ids'].shape[0]):\n",
    "            \n",
    "            # Decode the training input\n",
    "            text_content = self.tokenizer.decode(batch['input_ids'][i][1:])  # slicing from [1:] is important because tokenizer adds bos token\n",
    "            \n",
    "            # Extract substrings for prompt text in the training input\n",
    "            # The training input ends at the last user msg ending in [/INST]\n",
    "            prompt_gen_boundary = text_content.rfind(\"[/INST]\") + len(\"[/INST]\")\n",
    "            prompt_text = text_content[:prompt_gen_boundary]\n",
    "            \n",
    "            # print(f\"\"\"PROMPT TEXT:\\n{prompt_text}\"\"\")\n",
    "            \n",
    "            # retokenize the prompt text only\n",
    "            prompt_text_tokenized = self.tokenizer(\n",
    "                prompt_text,\n",
    "                return_overflowing_tokens=False,\n",
    "                return_length=False,\n",
    "            )\n",
    "            # compute index where prompt text ends in the training input\n",
    "            prompt_tok_idx = len(prompt_text_tokenized['input_ids'])\n",
    "            \n",
    "            # Set loss mask for all tokens in prompt text\n",
    "            labels[i][range(prompt_tok_idx)] = -100\n",
    "            \n",
    "            # print(\"================DEBUGGING INFORMATION===============\")\n",
    "            # for idx, tok in enumerate(labels[i]):\n",
    "            #     token_id = batch['input_ids'][i][idx]\n",
    "            #     decoded_token_id = self.tokenizer.decode(batch['input_ids'][i][idx])\n",
    "            #     print(f\"\"\"TOKID: {token_id} | LABEL: {tok} || DECODED: {decoded_token_id}\"\"\")\n",
    "                    \n",
    "        batch[\"labels\"] = labels\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1345"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = pd.concat([df_train, df_dev])['plain_text'].apply(len).max() + 400\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgenereux-akotenou\u001b[0m (\u001b[33mgenereux-akotenou-local\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/genereux/Documents/OPENCORE/DATA_CTF/MSL-Location-Mention-Recognition/approach_5/wandb/run-20240903_002830-o8n6hgkb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/genereux-akotenou-local/T5-GOOGLE-MPS-TRAIN/runs/o8n6hgkb' target=\"_blank\">Location Mention Recognition</a></strong> to <a href='https://wandb.ai/genereux-akotenou-local/T5-GOOGLE-MPS-TRAIN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/genereux-akotenou-local/T5-GOOGLE-MPS-TRAIN' target=\"_blank\">https://wandb.ai/genereux-akotenou-local/T5-GOOGLE-MPS-TRAIN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/genereux-akotenou-local/T5-GOOGLE-MPS-TRAIN/runs/o8n6hgkb' target=\"_blank\">https://wandb.ai/genereux-akotenou-local/T5-GOOGLE-MPS-TRAIN/runs/o8n6hgkb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"T5-GOOGLE-MPS-TRAIN\", name=\"Location Mention Recognition\")\n",
    "wandb.config.update({\n",
    "    \"max_seq_length\": max_seq_length,\n",
    "    \"output_dir\": \"T5-Output\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e238a689d8a24aad85beb24d8979499f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14392 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c57392a90da449ca7a73c5e71eeee9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2056 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are adding a <class 'transformers.integrations.integration_utils.WandbCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "WandbCallback\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Args\n",
    "tokenizer.padding_side = 'right'\n",
    "max_seq_length = pd.concat([df_train, df_dev])['plain_text'].apply(len).max()\n",
    "training_arguments = SFTConfig(\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    packing=False,\n",
    "    dataset_text_field=\"formatted_prompt\",\n",
    "    output_dir=\"T5-Output\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    #gradient_checkpointing=True,\n",
    "    report_to=\"wandb\",\n",
    "    logging_dir=\"./logs\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=100,\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model.to(torch.device('mps')),\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['eval'],\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    data_collator=CustomDataCollatorWithPadding(\n",
    "        tokenizer=tokenizer, \n",
    "        padding=\"longest\", \n",
    "        max_length=max_seq_length, \n",
    "        return_tensors=\"pt\"\n",
    "    ),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[WandbCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46c9319af8c46a89971f0e4bf627658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10794 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e75fd4d0fc144499e917f3ed97b2b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(92142) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92147) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92154) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92160) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92171) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92181) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92189) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92195) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92202) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92208) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92221) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92227) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92246) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92254) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92263) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92288) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92323) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92354) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92360) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92398) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92404) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92422) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92430) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92440) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92451) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92464) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92472) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92478) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92489) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92497) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92502) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92513) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92520) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92529) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92534) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92544) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92549) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92557) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92562) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92568) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92577) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92582) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92593) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92623) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92632) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92637) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92651) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92660) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92665) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92673) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92678) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92687) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92693) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92701) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92706) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92715) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92721) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92729) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92734) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92743) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92748) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92756) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92761) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92769) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92775) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92783) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92788) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92793) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92803) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92810) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92821) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92826) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92835) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92840) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92848) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92853) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92862) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92867) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92875) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92880) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92889) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92894) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92902) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(92907) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Invalid buffer size: 8.03 GB",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:451\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneftune_noise_alpha \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    449\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trl_activate_neftune(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel)\n\u001b[0;32m--> 451\u001b[0m output \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mtrain(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    453\u001b[0m \u001b[39m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[39m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneftune_noise_alpha \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1886\u001b[0m         args\u001b[39m=\u001b[39margs,\n\u001b[1;32m   1887\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1888\u001b[0m         trial\u001b[39m=\u001b[39mtrial,\n\u001b[1;32m   1889\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1890\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:2291\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2288\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mepoch \u001b[39m=\u001b[39m epoch \u001b[39m+\u001b[39m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m+\u001b[39m steps_skipped) \u001b[39m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2289\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m-> 2291\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   2292\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2293\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_substep_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:2721\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2719\u001b[0m metrics \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2720\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2721\u001b[0m     metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate(ignore_keys\u001b[39m=\u001b[39mignore_keys_for_eval)\n\u001b[1;32m   2722\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2724\u001b[0m     \u001b[39m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:3572\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3569\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   3571\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3572\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m   3573\u001b[0m     eval_dataloader,\n\u001b[1;32m   3574\u001b[0m     description\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEvaluation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3575\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;00m\n\u001b[1;32m   3576\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;00m\n\u001b[1;32m   3577\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   3578\u001b[0m     ignore_keys\u001b[39m=\u001b[39mignore_keys,\n\u001b[1;32m   3579\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39mmetric_key_prefix,\n\u001b[1;32m   3580\u001b[0m )\n\u001b[1;32m   3582\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[1;32m   3583\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric_key_prefix\u001b[39m}\u001b[39;00m\u001b[39m_jit_compilation_time\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:3779\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3777\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgather_function((logits))\n\u001b[1;32m   3778\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mbatch_eval_metrics \u001b[39mor\u001b[39;00m description \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPrediction\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 3779\u001b[0m         all_preds\u001b[39m.\u001b[39madd(logits)\n\u001b[1;32m   3780\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3781\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mpad_across_processes(labels, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, pad_index\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m100\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:326\u001b[0m, in \u001b[0;36mEvalLoopContainer.add\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors \u001b[39m=\u001b[39m tensors \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_nested_concat \u001b[39melse\u001b[39;00m [tensors]\n\u001b[1;32m    325\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_nested_concat:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors \u001b[39m=\u001b[39m nested_concat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors, tensors, padding_index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_index)\n\u001b[1;32m    327\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    328\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors\u001b[39m.\u001b[39mappend(tensors)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:138\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mtype\u001b[39m(tensors) \u001b[39m==\u001b[39m \u001b[39mtype\u001b[39m(\n\u001b[1;32m    135\u001b[0m     new_tensors\n\u001b[1;32m    136\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensors)\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(new_tensors)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m--> 138\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[39m=\u001b[39mpadding_index) \u001b[39mfor\u001b[39;00m t, n \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    139\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[39m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:138\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mtype\u001b[39m(tensors) \u001b[39m==\u001b[39m \u001b[39mtype\u001b[39m(\n\u001b[1;32m    135\u001b[0m     new_tensors\n\u001b[1;32m    136\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensors)\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(new_tensors)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m--> 138\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[39m=\u001b[39mpadding_index) \u001b[39mfor\u001b[39;00m t, n \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    139\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[39m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:140\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[39m=\u001b[39mpadding_index) \u001b[39mfor\u001b[39;00m t, n \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    139\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[39m=\u001b[39mpadding_index)\n\u001b[1;32m    141\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, Mapping):\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensors)(\n\u001b[1;32m    143\u001b[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001b[39m=\u001b[39mpadding_index) \u001b[39mfor\u001b[39;00m k, t \u001b[39min\u001b[39;00m tensors\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m    144\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:105\u001b[0m, in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m    102\u001b[0m new_shape \u001b[39m=\u001b[39m (tensor1\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m tensor2\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39mmax\u001b[39m(tensor1\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], tensor2\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])) \u001b[39m+\u001b[39m tensor1\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m:]\n\u001b[1;32m    104\u001b[0m \u001b[39m# Now let's fill the result tensor\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m result \u001b[39m=\u001b[39m tensor1\u001b[39m.\u001b[39mnew_full(new_shape, padding_index)\n\u001b[1;32m    106\u001b[0m result[: tensor1\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], : tensor1\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m tensor1\n\u001b[1;32m    107\u001b[0m result[tensor1\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] :, : tensor2\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m tensor2\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Invalid buffer size: 8.03 GB"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Make Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_context = pd.read_csv('../data/provided/Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model     = trainer.model\n",
    "pretrained_tokenizer = trainer.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (f\"Extract from this input all location mention by surrounding these location with xml tag. Sentence: '{df_context.iloc[0].text}'\")\n",
    "text = \"Translate in french: 'SCHOOL'\"\n",
    "inputs = pretrained_tokenizer.encode(text,\n",
    "                                     return_tensors='pt',\n",
    "                                     max_length=512,\n",
    "                                     truncation=True).to(torch.device('cuda'))\n",
    "\n",
    "generated_ids = pretrained_model.generate(inputs, \n",
    "                                          max_length=80, \n",
    "                                          min_length=40, \n",
    "                                          length_penalty=5.0, \n",
    "                                          num_beams=2)\n",
    "\n",
    "generated = pretrained_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(\"[INPUT_]: \", text, '\\n')\n",
    "print(\"[RESULT]: \", generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, T5Config\n",
    "\n",
    "def load_pretrained_(model_path='T5-LMR'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_pretrained(model_path='T5-LMR'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    config = T5Config.from_pretrained(model_path)\n",
    "    config.vocab_size = tokenizer.vocab_size  # Adjust vocab size based on tokenizer\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path, config=config, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_pretrained(model_path='T5-LMR'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\", ignore_mismatched_sizes=True)\n",
    "    return model, tokenizer\n",
    "    \n",
    "\"\"\"\n",
    "def make_inference_with_pretrained(model, tokenizer, prompt, max_seq_length=max_seq_length):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_seq_length)\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs.to(device))\n",
    "    logits = outputs.logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    predicted_text = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(predicted_text)\n",
    "\"\"\"\n",
    "\n",
    "def make_inference_with_pretrained_(model, tokenizer, prompt, max_seq_length=max_seq_length):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_seq_length)\n",
    "    decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]])\n",
    "    \n",
    "    # Move model and inputs to the appropriate device\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    decoder_input_ids = decoder_input_ids.to(device)\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=inputs['input_ids'], decoder_input_ids=decoder_input_ids)\n",
    "    \n",
    "    # Get logits and predict the most likely token IDs\n",
    "    logits = outputs.logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # Decode the predicted token IDs to text\n",
    "    predicted_text = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    #print(predicted_text)\n",
    "    return predicted_text\n",
    "\n",
    "def make_inference_with_pretrained(model, tokenizer, prompt, max_seq_length=max_seq_length, max_length=max_seq_length):\n",
    "    # Tokenize input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_seq_length)\n",
    "    \n",
    "    # Move model and inputs to the appropriate device\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    # Perform inference using the model's generate method for autoregressive decoding\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            max_length=max_length,\n",
    "            num_beams=5,  # Beam search for better results; you can adjust or remove this\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    print(generated_ids)\n",
    "    # Decode the generated token IDs to text\n",
    "    predicted_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return predicted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlr_model, mlr_tokenizer = load_pretrained(model_path=\"T5-Output/checkpoint-6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_context = pd.read_csv('../data/provided/Test.csv')\n",
    "df_context.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"Extract from this input all location mention by surrounding these location with xml tag. Sentence: '{df_context.iloc[0].text}'\"\n",
    "text = f\"Translate in french: 'You are strong'\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_inference_with_pretrained(trainer.model, trainer.tokenizer, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('Mistral-7B-LMR', torch_dtype=torch.float16, device_map=\"auto\")\n",
    "#model.to(\"cuda\")\n",
    "\n",
    "prompt = \"please help and donate to local charities\"\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_seq_length)\n",
    "generated_ids = model.generate(tokens, max_new_tokens=1000, do_sample=True)\n",
    "\n",
    "# decode with mistral tokenizer\n",
    "result = tokenizer.decode(generated_ids[0].tolist())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
