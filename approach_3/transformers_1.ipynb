{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: 0.5em; background-color: #1876d1; color: #fff; font-weight: bold; font-size: 1.4em;\">\n",
    "    [Approach 3]  Location Mention Recognition - NER BERT Transformer\n",
    "</div>\n",
    "\n",
    "In this Jupyter notebook, we will use Name Entity Recognition to extract from X (Twitter formely) tweets Location Mention from Emergency Situation.\n",
    "\n",
    "Note :\n",
    "* Do NER\n",
    "* Try BERT Model\n",
    "* Extract Location Mention\n",
    "\n",
    "---\n",
    "<b>#Microsoft Learn Challenge, #Zindi, #Hamad Bin Khalifa University </b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import stanza, os, sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from simpletransformers.ner import NERModel, NERArgs\n",
    "\n",
    "# utils setup\n",
    "current_directory = os.getcwd()\n",
    "root_directory = os.path.abspath(os.path.join(current_directory, os.pardir))\n",
    "sys.path.append(root_directory)\n",
    "\n",
    "# custom utils\n",
    "from utils.io import Predictions\n",
    "from utils.io import LMR_Scrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exploring Data**\n",
    "\n",
    "The provided Train.csv contain many missing value so we have to get data from initial source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: california_wildfires_2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.29file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: canada_wildfires_2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.49file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: cyclone_idai_2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.25file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: ecuador_earthquake_2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.53file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: greece_wildfires_2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.44file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_dorian_2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.20file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_florence_2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.26file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_harvey_2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.41file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_irma_2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.54file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_maria_2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.44file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_matthew_2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.70file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: italy_earthquake_aug_2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.78file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: kaikoura_earthquake_2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.58file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: kerala_floods_2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.32file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: maryland_floods_2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.70file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: midwestern_us_floods_2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.33file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: pakistan_earthquake_2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.41file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: puebla_mexico_earthquake_2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.40file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: srilanka_floods_2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 2/2 [00:00<00:00,  2.83file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "LMR_Scrapper(output_dir=\"../data/self_scrappe/\").run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let concatenate out dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_1001136212718088192</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EllicottCity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_1001136696589631488</td>\n",
       "      <td>Flash floods struck a Maryland city on Sunday,...</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_1001136950345109504</td>\n",
       "      <td>State of emergency declared for Maryland flood...</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_1001137334056833024</td>\n",
       "      <td>Other parts of Maryland also saw significant d...</td>\n",
       "      <td>Baltimore Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_1001138374923579392</td>\n",
       "      <td>Catastrophic Flooding Slams Ellicott City, Mar...</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ID_1001138377717157888</td>\n",
       "      <td>WATCH: 1 missing after flash #FLOODING devasta...</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ID_1001139323075416064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ID_1001139644023693312</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ID_1001140017207459840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ID_1001140276377935872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ID_1001140804503601152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Baltimore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ID_1001141041926492160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ID_1001141195576258560</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ellicott Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ID_1001141289323319296</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ID_1001141769155891200</td>\n",
       "      <td>Watch Live: Aerials of damage after historic f...</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ID_1001141974148239360</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ID_1001142381012508672</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ID_1001143589630427136</td>\n",
       "      <td>One person is reported missing as a state of e...</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ID_1001143679267098624</td>\n",
       "      <td>Monday May 28 - Morning Report: National Guard...</td>\n",
       "      <td>Arlington Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ID_1001144018451955712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ellicott Maryland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tweet_id                                               text  \\\n",
       "0   ID_1001136212718088192                                                NaN   \n",
       "1   ID_1001136696589631488  Flash floods struck a Maryland city on Sunday,...   \n",
       "2   ID_1001136950345109504  State of emergency declared for Maryland flood...   \n",
       "3   ID_1001137334056833024  Other parts of Maryland also saw significant d...   \n",
       "4   ID_1001138374923579392  Catastrophic Flooding Slams Ellicott City, Mar...   \n",
       "5   ID_1001138377717157888  WATCH: 1 missing after flash #FLOODING devasta...   \n",
       "6   ID_1001139323075416064                                                NaN   \n",
       "7   ID_1001139644023693312                                                NaN   \n",
       "8   ID_1001140017207459840                                                NaN   \n",
       "9   ID_1001140276377935872                                                NaN   \n",
       "10  ID_1001140804503601152                                                NaN   \n",
       "11  ID_1001141041926492160                                                NaN   \n",
       "12  ID_1001141195576258560                                                NaN   \n",
       "13  ID_1001141289323319296                                                NaN   \n",
       "14  ID_1001141769155891200  Watch Live: Aerials of damage after historic f...   \n",
       "15  ID_1001141974148239360                                                NaN   \n",
       "16  ID_1001142381012508672                                                NaN   \n",
       "17  ID_1001143589630427136  One person is reported missing as a state of e...   \n",
       "18  ID_1001143679267098624  Monday May 28 - Morning Report: National Guard...   \n",
       "19  ID_1001144018451955712                                                NaN   \n",
       "\n",
       "                  location  \n",
       "0             EllicottCity  \n",
       "1                 Maryland  \n",
       "2                 Maryland  \n",
       "3       Baltimore Maryland  \n",
       "4   Ellicott City Maryland  \n",
       "5   Ellicott City Maryland  \n",
       "6   Ellicott City Maryland  \n",
       "7                      NaN  \n",
       "8                 Maryland  \n",
       "9                 Maryland  \n",
       "10               Baltimore  \n",
       "11                Maryland  \n",
       "12       Ellicott Maryland  \n",
       "13                Maryland  \n",
       "14  Ellicott City Maryland  \n",
       "15                     NaN  \n",
       "16                Maryland  \n",
       "17                Maryland  \n",
       "18      Arlington Maryland  \n",
       "19       Ellicott Maryland  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv('../data/Train.csv')\n",
    "df_raw.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (73072, 3) \n",
      "------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tweet_id        0\n",
       "text        56624\n",
       "location    29612\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape: \", df_raw.shape, \"\\n------------------\")\n",
    "df_raw.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11849, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_raw.fillna(method='ffill', inplace=True)\n",
    "df_raw.dropna(inplace=True)\n",
    "df_raw.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BIO Tagging**\n",
    "\n",
    "BIO stands for Begin, Inside, and Outside. It’s a method for tagging tokens (words or subwords) in a sequence to identify entities within the text. Each token in the text is assigned a tag that indicates whether it is at the beginning of an entity, inside an entity, or outside of any entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize', verbose=False)\n",
    "\n",
    "def generate_bio_tags(text, location):\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    loc_words = location.split()\n",
    "    \n",
    "    for sentence in doc.sentences:\n",
    "        for i, token in enumerate(sentence.tokens):\n",
    "            token_text = token.text\n",
    "            tokens.append(token_text)\n",
    "            \n",
    "            if token_text in loc_words:\n",
    "                if loc_words[0] == token_text:\n",
    "                    tags.append('B-geo')\n",
    "                else:\n",
    "                    tags.append('I-geo')\n",
    "            else:\n",
    "                tags.append('O')\n",
    "    \n",
    "    return tokens, tags\n",
    "\n",
    "# Apply to each row in the TrainSet\n",
    "results = []\n",
    "for index, row in df_raw.iterrows():\n",
    "    tweet_id = row['tweet_id']\n",
    "    text = row['text']\n",
    "    location = row['location']\n",
    "    tokens, tags = generate_bio_tags(text, location)\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        results.append({'tweet_id': tweet_id, 'word': token, 'label': tag})\n",
    "\n",
    "# Get a look of the new df\n",
    "df_tag = pd.DataFrame(results)\n",
    "df_tag[\"tweet_id\"] = LabelEncoder().fit_transform(df_tag[\"tweet_id\"])\n",
    "df_tag[\"label\"]    = df_tag[\"label\"].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>word</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Flash</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>floods</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>struck</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>B-GEO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>city</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>on</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>washing</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>out</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>streets</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>and</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>tossing</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>cars</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>like</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>bath</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>toys</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>State</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tweet_id      word  label\n",
       "0          0     Flash      O\n",
       "1          0    floods      O\n",
       "2          0    struck      O\n",
       "3          0         a      O\n",
       "4          0  Maryland  B-GEO\n",
       "5          0      city      O\n",
       "6          0        on      O\n",
       "7          0    Sunday      O\n",
       "8          0         ,      O\n",
       "9          0   washing      O\n",
       "10         0       out      O\n",
       "11         0   streets      O\n",
       "12         0       and      O\n",
       "13         0   tossing      O\n",
       "14         0      cars      O\n",
       "15         0      like      O\n",
       "16         0      bath      O\n",
       "17         0      toys      O\n",
       "18         0         .      O\n",
       "19         1     State      O"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tag.head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prepare training, dev and test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3673</th>\n",
       "      <td>152</td>\n",
       "      <td>Volunteer</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162856</th>\n",
       "      <td>5019</td>\n",
       "      <td>5</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261904</th>\n",
       "      <td>9678</td>\n",
       "      <td>Irma</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97087</th>\n",
       "      <td>3119</td>\n",
       "      <td>drive</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282994</th>\n",
       "      <td>10770</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37028</th>\n",
       "      <td>1254</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84592</th>\n",
       "      <td>2691</td>\n",
       "      <td>are</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9826</th>\n",
       "      <td>358</td>\n",
       "      <td>#</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238547</th>\n",
       "      <td>8508</td>\n",
       "      <td>your</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245583</th>\n",
       "      <td>8865</td>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>243913 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id      words labels\n",
       "3673            152  Volunteer      O\n",
       "162856         5019          5      O\n",
       "261904         9678       Irma      O\n",
       "97087          3119      drive      O\n",
       "282994        10770        the      O\n",
       "...             ...        ...    ...\n",
       "37028          1254         to      O\n",
       "84592          2691        are      O\n",
       "9826            358          #      O\n",
       "238547         8508       your      O\n",
       "245583         8865          -      O\n",
       "\n",
       "[243913 rows x 3 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_tag[[\"tweet_id\", \"word\"]]\n",
    "y = df_tag[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "train_data = pd.DataFrame({\"sentence_id\": X_train[\"tweet_id\"], \"words\": X_train[\"word\"], \"labels\": y_train})\n",
    "test_data = pd.DataFrame({\"sentence_id\": X_test[\"tweet_id\"], \"words\": X_test[\"word\"], \"labels\": y_test})\n",
    "\n",
    "train_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-GEO', 'I-GEO']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = df_tag[\"label\"].unique().tolist()\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = NERArgs()\n",
    "model_args.num_train_epochs = 1\n",
    "model_args.learning_rate = 1e-4\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.train_batch_size = 32\n",
    "model_args.eval_batch_size = 32\n",
    "model_args.labels_list = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = NERModel('bert', \"bert-base-cased\", args=model_args, use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699b5521abe248679ecc40cf86458740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a586add8a5c949f2a0b1acc40009b4b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054e3981bf9b40fea8866384caf412ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 1:   0%|          | 0/371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(371, 0.0998445101222902)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train_model(train_data, eval_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f96e919686f24765ad770daadfe219b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7724021922604486803dc3220ef4028a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/364 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result, model_outputs, wrong_preds = model.eval_model(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5cf87471ba46aaac558741971fb321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb51e35ca51445fa3cf81135b9ccd92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions, raw_outputs = model.predict([\n",
    "    \"Elicott City, Maryland, struck by catastrophic flooding; 1 missing.\",\n",
    "    \"Memorial Day weekend floods ravage Maryland town\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'Elicott': 'O'},\n",
       "  {'City,': 'I-GEO'},\n",
       "  {'Maryland,': 'I-GEO'},\n",
       "  {'struck': 'O'},\n",
       "  {'by': 'O'},\n",
       "  {'catastrophic': 'O'},\n",
       "  {'flooding;': 'O'},\n",
       "  {'1': 'O'},\n",
       "  {'missing.': 'O'}],\n",
       " [{'Memorial': 'O'},\n",
       "  {'Day': 'O'},\n",
       "  {'weekend': 'O'},\n",
       "  {'floods': 'O'},\n",
       "  {'ravage': 'O'},\n",
       "  {'Maryland': 'B-GEO'},\n",
       "  {'town': 'O'}]]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Make prediction for Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_context = pd.read_csv('../data/Test.csv')\n",
    "ids = df_context[\"tweet_id\"].values\n",
    "tweets = df_context[\"text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91049fd8648e4444a1928580adc73570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74d3681d6544190ba2a918824861977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Prediction:   0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions, raw_outputs = model.predict(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for sentence in predictions:\n",
    "    result = \" \".join([word for d in sentence for word, tag in d.items() if tag != 'O'])\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to submissions/submission_2.csv\n"
     ]
    }
   ],
   "source": [
    "Predictions.to_csv(ids, results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
