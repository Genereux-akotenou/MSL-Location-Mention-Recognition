{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: 0.5em; background-color: #1876d1; color: #fff; font-weight: bold; font-size: 1.4em;\">\n",
    "    [Approach 3]  Location Mention Recognition - NER BERT Transformer\n",
    "</div>\n",
    "\n",
    "In this Jupyter notebook, we will use Name Entity Recognition to extract from X (Twitter formely) tweets Location Mention from Emergency Situation.\n",
    "\n",
    "Note :\n",
    "* Do NER\n",
    "* Try BERT Model\n",
    "* Extract Location Mention\n",
    "* The BIO format is very specific. It requires an understanding of tokens being “Inside” (I) and “Outside” (O) a particular entity label. This would add unnecessary complexity in formulating a task description prompt. Idea is to try IO formating approach\n",
    "\n",
    "---\n",
    "<b>#Microsoft Learn Challenge, #Zindi, #Hamad Bin Khalifa University </b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install simpletransformers\n",
    "#!pip install pyspellchecker\n",
    "#!pip install stanza\n",
    "#!pip install nltk\n",
    "#!pip install python-dotenv\n",
    "#!pip install werpy\n",
    "#!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general utils\n",
    "import werpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import stanza, os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from simpletransformers.ner import NERModel, NERArgs\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "\n",
    "# utils setup\n",
    "current_directory = os.getcwd()\n",
    "root_directory = os.path.abspath(os.path.join(current_directory, os.pardir))\n",
    "sys.path.append(root_directory)\n",
    "\n",
    "# logging\n",
    "import wandb\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"transformers_3_BIO_and_IO.ipynb\"\n",
    "\n",
    "# custom utils\n",
    "from utils.io import Predictions\n",
    "from utils.metrics import LMR_Metrics\n",
    "from utils.io import LMR_BILOU_Scrapper, LMR_JSON_Scrapper\n",
    "from utils.preprocessing import Preprocess"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exploring Data**\n",
    "\n",
    "The provided Train.csv contain many missing value so we have to get data from initial source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LMR_JSON_Scrapper(output_dir=\"../data/self_scrapped/raw\").run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let concatenate out dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SHAPE:  (14392, 3)\n",
      "TEST  SHAPE:  (4066, 3)\n",
      "DEV   SHAPE:  (2056, 3)\n"
     ]
    }
   ],
   "source": [
    "train_dfs = []\n",
    "dev_dfs   = []\n",
    "test_dfs  = []\n",
    "path_dfs  = \"../data/self_scrapped/raw\"\n",
    "for filename in os.listdir(path_dfs):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(path_dfs, filename)\n",
    "        if filename.startswith(\"train\"):\n",
    "            df = pd.read_csv(file_path)\n",
    "            train_dfs.append(df)\n",
    "        elif filename.startswith(\"dev\"):\n",
    "            df = pd.read_csv(file_path)\n",
    "            dev_dfs.append(df)\n",
    "        elif filename.startswith(\"test_unlabeled\"):\n",
    "            df = pd.read_csv(file_path)\n",
    "            test_dfs.append(df)\n",
    "\n",
    "df_train = pd.concat(train_dfs, ignore_index=True) if train_dfs else pd.DataFrame()\n",
    "df_test  = pd.concat(test_dfs, ignore_index=True) if test_dfs else pd.DataFrame()\n",
    "df_dev   = pd.concat(dev_dfs, ignore_index=True) if dev_dfs else pd.DataFrame()\n",
    "\n",
    "print(\"TRAIN SHAPE: \", df_train.shape)\n",
    "print(\"TEST  SHAPE: \", df_test.shape)\n",
    "print(\"DEV   SHAPE: \", df_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = Preprocess.build_bilou_encoding(df_train, text_col=\"text\", save_in=None)\n",
    "df_dev = Preprocess.build_bilou_encoding(df_dev, text_col=\"text\", save_in=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "train_dataset = df_train.groupby('sentence_id').agg({\n",
    "    'words': lambda x: ' '.join(x),\n",
    "    'labels': lambda x: ' '.join(x)\n",
    "}).reset_index()\n",
    "train_dataset = train_dataset.drop(columns=['sentence_id'])\n",
    "train_dataset.to_csv('../data/new/kaggle/train_bilou.csv', index=False)\n",
    "\n",
    "# Dev\n",
    "test_dataset = df_dev.groupby('sentence_id').agg({\n",
    "    'words': lambda x: ' '.join(x),\n",
    "    'labels': lambda x: ' '.join(x)\n",
    "}).reset_index()\n",
    "test_dataset = test_dataset.drop(columns=['sentence_id'])\n",
    "test_dataset.to_csv('../data/new/kaggle/dev_bilou.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_1001136212718088192</td>\n",
       "      <td>EllicottCity</td>\n",
       "      <td>B-CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_1001136212718088192</td>\n",
       "      <td>is</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_1001136212718088192</td>\n",
       "      <td>known</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_1001136212718088192</td>\n",
       "      <td>for</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_1001136212718088192</td>\n",
       "      <td>its</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532265</th>\n",
       "      <td>ID_916205068281462784</td>\n",
       "      <td>fellow</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532266</th>\n",
       "      <td>ID_916205068281462784</td>\n",
       "      <td>people</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532267</th>\n",
       "      <td>ID_916205068281462784</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532268</th>\n",
       "      <td>ID_916205068281462784</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>B-COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532269</th>\n",
       "      <td>ID_916205068281462784</td>\n",
       "      <td>2D</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>532270 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sentence_id         words     labels\n",
       "0       ID_1001136212718088192  EllicottCity     B-CITY\n",
       "1       ID_1001136212718088192            is          O\n",
       "2       ID_1001136212718088192         known          O\n",
       "3       ID_1001136212718088192           for          O\n",
       "4       ID_1001136212718088192           its          O\n",
       "...                        ...           ...        ...\n",
       "532265   ID_916205068281462784        fellow          O\n",
       "532266   ID_916205068281462784        people          O\n",
       "532267   ID_916205068281462784            in          O\n",
       "532268   ID_916205068281462784        Mexico  B-COUNTRY\n",
       "532269   ID_916205068281462784            2D          O\n",
       "\n",
       "[532270 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tag_train = df_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We observe in sentencess that we have hashtag, no-ascii character, stopword , ... we have to clean data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>location_mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_1022420413882744832</td>\n",
       "      <td>Nearly half of #houses checked in #fire-stricken areas deemed #uninhabitable #GO #PrayForGreece #PrayForAthens #AthensFires ἞C἟7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_1021778661895294976</td>\n",
       "      <td>RT @anadoluagency: #Greece: Death toll from wildfires hits 74</td>\n",
       "      <td>Greece=&gt;COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_1022015997740503042</td>\n",
       "      <td>When the essence of cooperation meets the sad reality of lifeThe IPA partner country offers financial aid to Greece to handle disaster @InterregIPACBC #Greecefires</td>\n",
       "      <td>Greece=&gt;COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_1022557424585240576</td>\n",
       "      <td>We are live from the Lureio Idrima the orphanage and nursing home operared by the nuns of the Holy Trinity Monastery that was destroyed by the fire in Neos Voutzas. Here too the scene is apocalyptic.</td>\n",
       "      <td>Holy Trinity Monastery=&gt;HUMAN-MADE POINT-OF-INTEREST * Neos Voutzas=&gt;NEIGHBORHOOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_1021749412639457280</td>\n",
       "      <td>RT @AP: Greek prime minister declares 3-day national mourning period for dozens killed by wildfires near Athens.</td>\n",
       "      <td>Athens=&gt;CITY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id  \\\n",
       "0  ID_1022420413882744832   \n",
       "1  ID_1021778661895294976   \n",
       "2  ID_1022015997740503042   \n",
       "3  ID_1022557424585240576   \n",
       "4  ID_1021749412639457280   \n",
       "\n",
       "                                                                                                                                                                                                      text  \\\n",
       "0                                                                         Nearly half of #houses checked in #fire-stricken areas deemed #uninhabitable #GO #PrayForGreece #PrayForAthens #AthensFires ἞C἟7   \n",
       "1                                                                                                                                            RT @anadoluagency: #Greece: Death toll from wildfires hits 74   \n",
       "2                                      When the essence of cooperation meets the sad reality of lifeThe IPA partner country offers financial aid to Greece to handle disaster @InterregIPACBC #Greecefires   \n",
       "3  We are live from the Lureio Idrima the orphanage and nursing home operared by the nuns of the Holy Trinity Monastery that was destroyed by the fire in Neos Voutzas. Here too the scene is apocalyptic.   \n",
       "4                                                                                         RT @AP: Greek prime minister declares 3-day national mourning period for dozens killed by wildfires near Athens.   \n",
       "\n",
       "                                                                   location_mentions  \n",
       "0                                                                                NaN  \n",
       "1                                                                    Greece=>COUNTRY  \n",
       "2                                                                    Greece=>COUNTRY  \n",
       "3  Holy Trinity Monastery=>HUMAN-MADE POINT-OF-INTEREST * Neos Voutzas=>NEIGHBORHOOD  \n",
       "4                                                                       Athens=>CITY  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are some sentences without a location mention. We need to look closer. It could be normal if there is no corresponding location found in the tweet, or it might be an error from the labeling task. Note that for the test set, it is normal for all location_mentions to be NaN. (😎 Yeah, we have to predict this value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_id                0\n",
      "text                    0\n",
      "location_mentions    4026\n",
      "dtype: int64\n",
      "tweet_id                0\n",
      "text                    0\n",
      "location_mentions    4066\n",
      "dtype: int64\n",
      "tweet_id               0\n",
      "text                   0\n",
      "location_mentions    573\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train.isnull().sum())\n",
    "print(df_test.isnull().sum())\n",
    "print(df_dev.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preprocessing Data**\n",
    "\n",
    "- Remove special character\n",
    "- Treat HASHTAG, USERTAG\n",
    "- Remove stop word\n",
    "- Tokenization\n",
    "- Stemming\n",
    "- BIO Tagging\n",
    "\n",
    "##### **<> BIO Tagging**\n",
    "\n",
    "BIO stands for Begin, Inside, and Outside. It’s a method for tagging tokens (words or subwords) in a sequence to identify entities within the text. Each token in the text is assigned a tag that indicates whether it is at the beginning of an entity, inside an entity, or outside of any entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "train_path = \"../data/transformed/train.io2-tag.csv\"\n",
    "if not os.path.exists(train_path):\n",
    "    df_train = Preprocess.remove_non_ascii(df_train, column_name='text')\n",
    "    # df_train = Preprocess.remove_usertag(df_train, column_name='text')\n",
    "    # df_train = Preprocess.reformat_hashtag(df_train, column_name='text')\n",
    "    # df_train = Preprocess.remove_stop_words(df_train, column_name='text', new_col=\"text_transformed\", transformation=[\n",
    "    #     \"tokenize\", \"lemma\", \"lower\"], save_in=\"../data/transformed/train.lemma.csv\")\n",
    "    df_tag_train = Preprocess.build_io_encoding(df_train, text_col=\"text\", save_in=train_path)\n",
    "else:\n",
    "    df_tag_train = pd.read_csv(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEV\n",
    "dev_path = \"../data/transformed/dev.io2-tag.csv\"\n",
    "if not os.path.exists(dev_path):\n",
    "    df_dev = Preprocess.remove_non_ascii(df_dev, column_name='text')\n",
    "    # df_dev = Preprocess.remove_usertag(df_dev, column_name='text')\n",
    "    # df_dev = Preprocess.reformat_hashtag(df_dev, column_name='text')\n",
    "    # df_dev = Preprocess.remove_stop_words(df_dev, column_name='text', new_col=\"text_transformed\", transformation=[\n",
    "    #     \"tokenize\", \"lemma\", \"lower\"], save_in=\"../data/transformed/dev.lemma.csv\")\n",
    "    df_tag_dev = Preprocess.build_io_encoding(df_dev, text_col=\"text\", save_in=dev_path)\n",
    "else:\n",
    "    df_tag_dev = pd.read_csv(dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "test_path = \"../data/transformed/test.lemma.csv\"\n",
    "if not os.path.exists(test_path):\n",
    "    df_test = Preprocess.remove_non_ascii(df_test, column_name='text')\n",
    "    df_test = Preprocess.remove_usertag(df_test, column_name='text')\n",
    "    df_test = Preprocess.reformat_hashtag(df_test, column_name='text')\n",
    "    df_test = Preprocess.remove_stop_words(df_test, column_name='text', new_col=\"text_transformed\", transformation=[\n",
    "        \"tokenize\", \"lemma\", \"lower\"], save_in=test_path)\n",
    "else:\n",
    "    df_test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_1022420413882744832</td>\n",
       "      <td>Nearly</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_1022420413882744832</td>\n",
       "      <td>half</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_1022420413882744832</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_1022420413882744832</td>\n",
       "      <td>houses</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_1022420413882744832</td>\n",
       "      <td>checked</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ID_1022420413882744832</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ID_1022420413882744832</td>\n",
       "      <td>fire</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ID_1022420413882744832</td>\n",
       "      <td>stricken</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ID_1022420413882744832</td>\n",
       "      <td>areas</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ID_1022420413882744832</td>\n",
       "      <td>deemed</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ID_1022420413882744832</td>\n",
       "      <td>uninhabitable</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ID_1022420413882744832</td>\n",
       "      <td>GO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ID_1022420413882744832</td>\n",
       "      <td>PrayForGreece</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ID_1022420413882744832</td>\n",
       "      <td>PrayForAthens</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ID_1022420413882744832</td>\n",
       "      <td>AthensFires</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ID_1022420413882744832</td>\n",
       "      <td>C7</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ID_1021778661895294976</td>\n",
       "      <td>RT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ID_1021778661895294976</td>\n",
       "      <td>anadoluagency</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ID_1021778661895294976</td>\n",
       "      <td>Greece</td>\n",
       "      <td>I-COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ID_1021778661895294976</td>\n",
       "      <td>Death</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ID_1021778661895294976</td>\n",
       "      <td>toll</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ID_1021778661895294976</td>\n",
       "      <td>from</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ID_1021778661895294976</td>\n",
       "      <td>wildfires</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ID_1021778661895294976</td>\n",
       "      <td>hits</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ID_1021778661895294976</td>\n",
       "      <td>74</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ID_1022015997740503042</td>\n",
       "      <td>When</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ID_1022015997740503042</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ID_1022015997740503042</td>\n",
       "      <td>essence</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ID_1022015997740503042</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ID_1022015997740503042</td>\n",
       "      <td>cooperation</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               sentence_id          words     labels\n",
       "0   ID_1022420413882744832         Nearly          O\n",
       "1   ID_1022420413882744832           half          O\n",
       "2   ID_1022420413882744832             of          O\n",
       "3   ID_1022420413882744832         houses          O\n",
       "4   ID_1022420413882744832        checked          O\n",
       "5   ID_1022420413882744832             in          O\n",
       "6   ID_1022420413882744832           fire          O\n",
       "7   ID_1022420413882744832       stricken          O\n",
       "8   ID_1022420413882744832          areas          O\n",
       "9   ID_1022420413882744832         deemed          O\n",
       "10  ID_1022420413882744832  uninhabitable          O\n",
       "11  ID_1022420413882744832             GO          O\n",
       "12  ID_1022420413882744832  PrayForGreece          O\n",
       "13  ID_1022420413882744832  PrayForAthens          O\n",
       "14  ID_1022420413882744832    AthensFires          O\n",
       "15  ID_1022420413882744832             C7          O\n",
       "16  ID_1021778661895294976             RT          O\n",
       "17  ID_1021778661895294976  anadoluagency          O\n",
       "18  ID_1021778661895294976         Greece  I-COUNTRY\n",
       "19  ID_1021778661895294976          Death          O\n",
       "20  ID_1021778661895294976           toll          O\n",
       "21  ID_1021778661895294976           from          O\n",
       "22  ID_1021778661895294976      wildfires          O\n",
       "23  ID_1021778661895294976           hits          O\n",
       "24  ID_1021778661895294976             74          O\n",
       "25  ID_1022015997740503042           When          O\n",
       "26  ID_1022015997740503042            the          O\n",
       "27  ID_1022015997740503042        essence          O\n",
       "28  ID_1022015997740503042             of          O\n",
       "29  ID_1022015997740503042    cooperation          O"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tag_train.head(30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prepare training, dev and test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tag_train[\"sentence_id\"] = LabelEncoder().fit_transform(df_tag_train[\"sentence_id\"])\n",
    "df_tag_dev[\"sentence_id\"]   = LabelEncoder().fit_transform(df_tag_dev[\"sentence_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>EllicottCity</td>\n",
       "      <td>B-CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>known</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>for</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>its</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id         words  labels\n",
       "0            0  EllicottCity  B-CITY\n",
       "1            0            is       O\n",
       "2            0         known       O\n",
       "3            0           for       O\n",
       "4            0           its       O"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tag_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>EllicottCity</td>\n",
       "      <td>B-CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>known</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>for</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>its</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532265</th>\n",
       "      <td>43459</td>\n",
       "      <td>fellow</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532266</th>\n",
       "      <td>43459</td>\n",
       "      <td>people</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532267</th>\n",
       "      <td>43459</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532268</th>\n",
       "      <td>43459</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>B-COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532269</th>\n",
       "      <td>43459</td>\n",
       "      <td>2D</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>532270 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id         words     labels\n",
       "0                 0  EllicottCity     B-CITY\n",
       "1                 0            is          O\n",
       "2                 0         known          O\n",
       "3                 0           for          O\n",
       "4                 0           its          O\n",
       "...             ...           ...        ...\n",
       "532265        43459        fellow          O\n",
       "532266        43459        people          O\n",
       "532267        43459            in          O\n",
       "532268        43459        Mexico  B-COUNTRY\n",
       "532269        43459            2D          O\n",
       "\n",
       "[532270 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train  = df_tag_train[[\"sentence_id\", \"words\"]]\n",
    "X_test   = df_tag_dev[[\"sentence_id\", \"words\"]]\n",
    "y_train  = df_tag_train[\"labels\"]\n",
    "y_test   = df_tag_dev[\"labels\"]\n",
    "\n",
    "train_data = pd.DataFrame({\"sentence_id\": X_train[\"sentence_id\"], \"words\": X_train[\"words\"], \"labels\": y_train})\n",
    "test_data = pd.DataFrame({\"sentence_id\": X_test[\"sentence_id\"], \"words\": X_test[\"words\"], \"labels\": y_test})\n",
    "\n",
    "train_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let count NER label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O</td>\n",
       "      <td>511322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-STATE</td>\n",
       "      <td>22004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B-COUNTRY</td>\n",
       "      <td>13583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B-CITY</td>\n",
       "      <td>9933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B-ISLAND</td>\n",
       "      <td>6386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I-ISLAND</td>\n",
       "      <td>3287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I-COUNTRY</td>\n",
       "      <td>1873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I-CITY</td>\n",
       "      <td>1851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I-STATE</td>\n",
       "      <td>1574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B-COUNTY</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>B-DISTRICT</td>\n",
       "      <td>494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I-COUNTY</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I-HUMAN-MADE</td>\n",
       "      <td>436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>B-NEIGHBORHOOD</td>\n",
       "      <td>422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I-NEIGHBORHOOD</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>B-HUMAN-MADE</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>B-CONTINENT</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I-NATURAL</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>B-NATURAL</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>I-ROAD</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>B-ROAD</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>I-DISTRICT</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>I-OTHER</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>B-OTHER</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I-CONTINENT</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Label  Frequency\n",
       "0                O     511322\n",
       "1          B-STATE      22004\n",
       "2        B-COUNTRY      13583\n",
       "3           B-CITY       9933\n",
       "4         B-ISLAND       6386\n",
       "5         I-ISLAND       3287\n",
       "6        I-COUNTRY       1873\n",
       "7           I-CITY       1851\n",
       "8          I-STATE       1574\n",
       "9         B-COUNTY        663\n",
       "10      B-DISTRICT        494\n",
       "11        I-COUNTY        481\n",
       "12    I-HUMAN-MADE        436\n",
       "13  B-NEIGHBORHOOD        422\n",
       "14  I-NEIGHBORHOOD        397\n",
       "15    B-HUMAN-MADE        366\n",
       "16     B-CONTINENT        328\n",
       "17       I-NATURAL        237\n",
       "18       B-NATURAL        219\n",
       "19          I-ROAD        181\n",
       "20          B-ROAD         66\n",
       "21      I-DISTRICT         61\n",
       "22         I-OTHER         58\n",
       "23         B-OTHER         25\n",
       "24     I-CONTINENT         16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label = pd.concat([df_tag_train, df_tag_dev])[\"labels\"].unique().tolist()\n",
    "label_counts = pd.concat([df_tag_train, df_tag_dev])[\"labels\"].value_counts().reset_index()\n",
    "label_counts.columns = [\"Label\", \"Frequency\"]\n",
    "display(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let define model **Args** and hyperparameters optimisation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "sweep_config = {\n",
    "    \"method\": \"bayes\",  # grid, random\n",
    "    \"metric\": {\"name\": \"wer\", \"goal\": \"minimize\"},\n",
    "    \"parameters\": {\n",
    "        \"num_train_epochs\": {\"values\": [1, 2]},\n",
    "        # \"learning_rate\": {\"min\": 5e-5, \"max\": 4e-4},\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize a W&B sweep with the config defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: qf80j295\n",
      "Sweep URL: https://wandb.ai/genereux-akotenou-local/LMR-IO2/sweeps/qf80j295\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"LMR-IO2\")\n",
    "#%%capture\n",
    "# wandb.init(project=\"LMR\", name=\"Location-Mention-Recognition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = NERArgs()\n",
    "\n",
    "# general\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.train_batch_size = 32\n",
    "model_args.eval_batch_size = 16\n",
    "model_args.labels_list = label\n",
    "model_args.use_multiprocessing = True\n",
    "model_args.wandb_project = \"LMR-IO2\"\n",
    "\n",
    "# for eaarly stoping\n",
    "# model_args.use_early_stopping = True\n",
    "# model_args.early_stopping_delta = 0.01\n",
    "# model_args.early_stopping_metric = \"wer\"\n",
    "# model_args.early_stopping_metric_minimize = False\n",
    "# model_args.early_stopping_patience = 3\n",
    "model_args.evaluate_during_training_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval():\n",
    "    wandb.init(name=\"Location-Mention-Recognition\")\n",
    "    model = NERModel(\n",
    "        \"bert\", \n",
    "        \"bert-base-cased\", #bert-large-uncased\n",
    "        use_cuda=False,\n",
    "        args=model_args, \n",
    "        sweep_config=wandb.config)\n",
    "\n",
    "    # Train the model\n",
    "    print('### TRAINING')\n",
    "    # train_data1, _ = train_test_split(train_data, test_size=0.99998)\n",
    "    model.train_model(\n",
    "        train_data, \n",
    "        eval_data=test_data, \n",
    "        wer=LMR_Metrics.wer_type\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print('### EVALUATION')\n",
    "    result, model_outputs, wrong_preds = model.eval_model(test_data, wer=LMR_Metrics.wer_type)\n",
    "\n",
    "    # Log metrics to wandb\n",
    "    wandb.log({\"eval_result\": result, \"model_outputs\": model_outputs})\n",
    "\n",
    "    # Sync wandb\n",
    "    wandb.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find transformers_3.ipynb.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: z6i5a9zx with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00023034296211370871\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find transformers_3.ipynb.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/genereux/Documents/OPENCORE/DATA_CTF/MSL-Location-Mention-Recognition/approach_3/wandb/run-20240827_215746-z6i5a9zx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/genereux-akotenou-local/LMR-IO/runs/z6i5a9zx' target=\"_blank\">Location-Mention-Recognition</a></strong> to <a href='https://wandb.ai/genereux-akotenou-local/LMR-IO' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/genereux-akotenou-local/LMR-IO/sweeps/fy1qbllr' target=\"_blank\">https://wandb.ai/genereux-akotenou-local/LMR-IO/sweeps/fy1qbllr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/genereux-akotenou-local/LMR-IO' target=\"_blank\">https://wandb.ai/genereux-akotenou-local/LMR-IO</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/genereux-akotenou-local/LMR-IO/sweeps/fy1qbllr' target=\"_blank\">https://wandb.ai/genereux-akotenou-local/LMR-IO/sweeps/fy1qbllr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/genereux-akotenou-local/LMR-IO/runs/z6i5a9zx' target=\"_blank\">https://wandb.ai/genereux-akotenou-local/LMR-IO/runs/z6i5a9zx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TRAINING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "wandb.agent(sweep_id, train_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let define model **Args** and hyperparameters optimisation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = NERArgs()\n",
    "\n",
    "# general\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.train_batch_size = 64\n",
    "model_args.eval_batch_size = 32\n",
    "model_args.labels_list = label\n",
    "model_args.use_multiprocessing = True\n",
    "model_args.num_train_epochs = 2\n",
    "model_args.learning_rate = 4e-4\n",
    "\n",
    "# for eaarly stoping\n",
    "# model_args.use_early_stopping = True\n",
    "# model_args.early_stopping_delta = 0.01\n",
    "# model_args.early_stopping_metric = \"wer\"\n",
    "# model_args.early_stopping_metric_minimize = False\n",
    "# model_args.early_stopping_patience = 5\n",
    "# model_args.wandb_project = \"LMR-IO\"\n",
    "model_args.evaluate_during_training_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sentence_id                   words                            labels  \\\n",
      "0        ID_1        [I, love, Paris]                    O O B-LOCATION   \n",
      "1        ID_2  [He, went, to, London]                  O O O B-LOCATION   \n",
      "2        ID_3      [This, is, Berlin]                    O O B-LOCATION   \n",
      "3        ID_4       [New, York, City]  B-LOCATION I-LOCATION I-LOCATION   \n",
      "\n",
      "   has_entity  \n",
      "0           1  \n",
      "1           1  \n",
      "2           1  \n",
      "3           1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def stratified_split_ner(df, entity_col=\"labels\", test_size=0.2):\n",
    "    # Créer une colonne avec la présence de chaque type d'entité\n",
    "    df['has_entity'] = df[entity_col].apply(lambda x: int(any(tag.startswith('B-') or tag.startswith('I-') for tag in x.split())))\n",
    "    \n",
    "    # Réaliser une division stratifiée basée sur la présence d'entités\n",
    "    print(df)\n",
    "    train, test = train_test_split(df, test_size=test_size, stratify=df['has_entity'])\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "# Utilisation\n",
    "df = pd.DataFrame({\n",
    "    'sentence_id': ['ID_1', 'ID_2', 'ID_3', 'ID_4'],\n",
    "    'words': [['I', 'love', 'Paris'], ['He', 'went', 'to', 'London'], ['This', 'is', 'Berlin'], ['New', 'York', 'City']],\n",
    "    'labels': ['O O B-LOCATION', 'O O O B-LOCATION', 'O O B-LOCATION', 'B-LOCATION I-LOCATION I-LOCATION']\n",
    "})\n",
    "\n",
    "train, test = stratified_split_ner(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "      <th>has_entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_2</td>\n",
       "      <td>[He, went, to, London]</td>\n",
       "      <td>O O O B-LOCATION</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_1</td>\n",
       "      <td>[I, love, Paris]</td>\n",
       "      <td>O O B-LOCATION</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_4</td>\n",
       "      <td>[New, York, City]</td>\n",
       "      <td>B-LOCATION I-LOCATION I-LOCATION</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence_id                   words                            labels  \\\n",
       "1        ID_2  [He, went, to, London]                  O O O B-LOCATION   \n",
       "0        ID_1        [I, love, Paris]                    O O B-LOCATION   \n",
       "3        ID_4       [New, York, City]  B-LOCATION I-LOCATION I-LOCATION   \n",
       "\n",
       "   has_entity  \n",
       "1           1  \n",
       "0           1  \n",
       "3           1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "      <th>has_entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_3</td>\n",
       "      <td>[This, is, Berlin]</td>\n",
       "      <td>O O B-LOCATION</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence_id               words          labels  has_entity\n",
       "2        ID_3  [This, is, Berlin]  O O B-LOCATION           1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### TRAINING\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea60ea9bc98467d9f1c6742c9311f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca70005286b74bbfb62f995ec77edc7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b9fd2309ca49ccbd0f37d4c100afb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 2:   0%|          | 0/680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f38b4c757374e3e9fca49b0fadf2895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9e74b31e8a4a19b4841ad4809c08a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec9f57df0c749e18441b57cb6b88577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 2:   0%|          | 0/680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cee6865cfc143c5a3012b050a1c57be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a4ad0b929647868472c5906254e8b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a069be75854f918e1fadb0717b3109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12633d83e5f4566be9fcbf78c2fb64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1360,\n",
       " defaultdict(list,\n",
       "             {'global_step': [680, 1000, 1360],\n",
       "              'train_loss': [0.013492707163095474,\n",
       "               0.08003070205450058,\n",
       "               0.01464491244405508],\n",
       "              'eval_loss': [0.44833912138755505,\n",
       "               0.4386223302437709,\n",
       "               0.4364334000990941],\n",
       "              'precision': [0.652946273830156,\n",
       "               0.706430568499534,\n",
       "               0.7135036496350365],\n",
       "              'recall': [0.732620320855615,\n",
       "               0.7369956246961594,\n",
       "               0.7603305785123967],\n",
       "              'f1_score': [0.6904925544100802,\n",
       "               0.7213894837021175,\n",
       "               0.736173217227583],\n",
       "              'wer': [0.8983337382489047,\n",
       "               0.7721650940722028,\n",
       "               0.8305438006735043]}))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NERModel(\n",
    "    \"bert\", \n",
    "    \"bert-base-cased\", #bert-large-uncased\n",
    "    use_cuda=False,\n",
    "    args=model_args, \n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print('\\n### TRAINING')\n",
    "model.train_model(\n",
    "    train_data, \n",
    "    eval_data=test_data, \n",
    "    wer=LMR_Metrics.wer_type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab0d9a631eb456499722425ad1e40b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01aa232b592e43f5b0c30c051cccfeec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "#print('\\n### EVALUATION')\n",
    "#result, model_outputs, wrong_preds = model.eval_model(test_data, wer=LMR_Metrics.wer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.06285895355618917,\n",
       " 'precision': 0.8029925187032418,\n",
       " 'recall': 0.813953488372093,\n",
       " 'f1_score': 0.8084358523725833,\n",
       " 'wer': 0.41041911334521874}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quick prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344a5cab6be24157b963f5725538aa59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8627c6a1c8467cadb9532b1649e124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions, raw_outputs = model.predict([\n",
    "    \"Elicott City, Maryland, struck by catastrophic flooding; 1 missing.\",\n",
    "    \"Memorial Day weekend floods ravage Maryland town\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'Elicott': 'O'},\n",
       "  {'City,': 'O'},\n",
       "  {'Maryland,': 'B-STATE'},\n",
       "  {'struck': 'O'},\n",
       "  {'by': 'O'},\n",
       "  {'catastrophic': 'O'},\n",
       "  {'flooding;': 'O'},\n",
       "  {'1': 'O'},\n",
       "  {'missing.': 'O'}],\n",
       " [{'Memorial': 'O'},\n",
       "  {'Day': 'O'},\n",
       "  {'weekend': 'O'},\n",
       "  {'floods': 'O'},\n",
       "  {'ravage': 'O'},\n",
       "  {'Maryland': 'B-STATE'},\n",
       "  {'town': 'O'}]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Make prediction for Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79a74f45e6b427ea5c3e46617c9e2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4fb7bc3a5184a59bf6a9e6f8f57d7bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Prediction:   0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Data and Preprocess\n",
    "# df_context = pd.read_csv('../data/provided/Test.csv')\n",
    "# df_context = Preprocess.remove_special_characters(df_context, column_name='text')\n",
    "# df_context['text'] = df_context['text'].apply(lambda x: Preprocess.treat_hashtags(x))\n",
    "# df_context['text'] = df_context['text'].apply(lambda x: Preprocess.correct_spelling(x))\n",
    "# #df_context['text'] = df_context['text'].apply(lambda x: Preprocess.remove_stop_words(x))\n",
    "# df_context.to_csv(\"../data/provided/Test-processed.csv\")\n",
    "\n",
    "df_context = pd.read_csv('../data/provided/Test.csv')\n",
    "df_context = Preprocess.remove_non_ascii(df_context, column_name='text')\n",
    "# df_context = Preprocess.remove_usertag(df_context, column_name='text')\n",
    "# df_context = Preprocess.reformat_hashtag(df_context, column_name='text')\n",
    "# df_context = Preprocess.remove_stop_words(df_context, column_name='text', new_col=\"text_transformed\", transformation=[\n",
    "#     \"tokenize\", \"lemma\", \"lower\"], save_in=\"../data/provided/Test-processed.csv\")\n",
    "\n",
    "#df_context = pd.read_csv('../data/provided/Test-processed.csv')\n",
    "\n",
    "ids = df_context[\"tweet_id\"].values\n",
    "tweets = df_context[\"text\"].values #text_transformed\n",
    "\n",
    "# Make prediction\n",
    "predictions, raw_outputs = model.predict(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to ../submissions/submission_12.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract Location Mention based on model output\n",
    "results = []\n",
    "for sentence in predictions:\n",
    "    result = \" \".join([word for d in sentence for word, tag in d.items() if tag != 'O'])\n",
    "    if result == \"\":\n",
    "        result = \" \"\n",
    "    results.append(result)\n",
    "\n",
    "Predictions.to_csv(ids, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_1001154804658286592</td>\n",
       "      <td>New England New Orleans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_1001155505459486720</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_1001155756371136512</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_1001159445194399744</td>\n",
       "      <td>Maryland Ellicott City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_1001164907587538944</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id                 location\n",
       "0  ID_1001154804658286592  New England New Orleans\n",
       "1  ID_1001155505459486720                         \n",
       "2  ID_1001155756371136512   Ellicott City Maryland\n",
       "3  ID_1001159445194399744   Maryland Ellicott City\n",
       "4  ID_1001164907587538944   Ellicott City Maryland"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def postprocessing(path=\"../submissions/submission_12.csv\"):\n",
    "    df = pd.read_csv(path)\n",
    "    remove_chars = ['\"', ',', '.', ')', '[', ']', '(', '#', '?']\n",
    "    translation_table = str.maketrans('', '', ''.join(remove_chars))\n",
    "    df['location'] = df['location'].apply(lambda x: x.translate(translation_table).strip())\n",
    "    df.loc[df.location.apply(len) < 2, 'location'] = ' '\n",
    "    return df\n",
    "\n",
    "df_cleaned = postprocessing()\n",
    "df_cleaned.to_csv(\"../submissions/submission_12-processed.csv\", index=False)\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
