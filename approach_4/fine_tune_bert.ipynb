{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: 0.5em; background-color: #1876d1; color: #fff; font-weight: bold; font-size: 1.4em;\">\n",
    "    [Approach 4]  Location Mention Recognition - NER BERT Transformer\n",
    "</div>\n",
    "\n",
    "In this Jupyter notebook, we will use Name Entity Recognition to extract from X (Twitter formely) tweets Location Mention from Emergency Situation.\n",
    "\n",
    "Note :\n",
    "* Do NER\n",
    "* Try BERT Model\n",
    "* Extract Location Mention\n",
    "\n",
    "---\n",
    "<b>#Microsoft Learn Challenge, #Zindi, #Hamad Bin Khalifa University </b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.11/site-packages (4.41.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jiwer\n",
      "  Downloading jiwer-3.0.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: accelerate in /opt/anaconda3/lib/python3.11/site-packages (0.30.1)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in /opt/anaconda3/lib/python3.11/site-packages (from jiwer) (8.1.7)\n",
      "Collecting rapidfuzz<4,>=3 (from jiwer)\n",
      "  Downloading rapidfuzz-3.9.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.11/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/anaconda3/lib/python3.11/site-packages (from accelerate) (2.3.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.13.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/9.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0mm\n",
      "\u001b[?25h\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 561, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 527, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 102, in read\n",
      "    self.__buf.write(data)\n",
      "  File \"/opt/anaconda3/lib/python3.11/tempfile.py\", line 500, in func_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "OSError: [Errno 28] No space left on device\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_internal/cli/base_command.py\", line 180, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_internal/cli/req_command.py\", line 245, in wrapper\n",
      "    return func(self, options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_internal/commands/install.py\", line 377, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "                      ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 179, in resolve\n",
      "    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_internal/operations/prepare.py\", line 552, in prepare_linked_requirements_more\n",
      "    self._complete_partial_requirements(\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_internal/operations/prepare.py\", line 467, in _complete_partial_requirements\n",
      "    for link, (filepath, _) in batch_download:\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_internal/network/download.py\", line 183, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_internal/cli/progress_bars.py\", line 53, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_internal/network/utils.py\", line 63, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 622, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 560, in read\n",
      "    with self._error_catcher():\n",
      "  File \"/opt/anaconda3/lib/python3.11/contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 455, in _error_catcher\n",
      "    raise ProtocolError(\"Connection broken: %r\" % e, e)\n",
      "pip._vendor.urllib3.exceptions.ProtocolError: (\"Connection broken: OSError(28, 'No space left on device')\", OSError(28, 'No space left on device'))\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: tf-keras in /opt/anaconda3/lib/python3.11/site-packages (2.16.0)\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16 in /opt/anaconda3/lib/python3.11/site-packages (from tf-keras) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.63.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.2.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.36.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.23.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16->tf-keras) (0.41.2)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.0.7)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.1.2)\n",
      "zsh:1: no matches found: transformers[torch]\n",
      "Requirement already satisfied: accelerate in /opt/anaconda3/lib/python3.11/site-packages (0.30.1)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /opt/anaconda3/lib/python3.11/site-packages (from accelerate) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.11/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.11/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/anaconda3/lib/python3.11/site-packages (from accelerate) (2.3.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/anaconda3/lib/python3.11/site-packages (from accelerate) (0.23.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.13.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.30.1\n",
      "    Uninstalling accelerate-0.30.1:\n",
      "      Successfully uninstalled accelerate-0.30.1\n",
      "  Rolling back uninstall of accelerate\n",
      "  Moving to /opt/anaconda3/bin/accelerate\n",
      "   from /private/var/folders/w8/b_cflrn97k9c15rcrn5t8mmr0000gn/T/pip-uninstall-8lraw8ig/accelerate\n",
      "  Moving to /opt/anaconda3/bin/accelerate-config\n",
      "   from /private/var/folders/w8/b_cflrn97k9c15rcrn5t8mmr0000gn/T/pip-uninstall-8lraw8ig/accelerate-config\n",
      "  Moving to /opt/anaconda3/bin/accelerate-estimate-memory\n",
      "   from /private/var/folders/w8/b_cflrn97k9c15rcrn5t8mmr0000gn/T/pip-uninstall-8lraw8ig/accelerate-estimate-memory\n",
      "  Moving to /opt/anaconda3/bin/accelerate-launch\n",
      "   from /private/var/folders/w8/b_cflrn97k9c15rcrn5t8mmr0000gn/T/pip-uninstall-8lraw8ig/accelerate-launch\n",
      "  Moving to /opt/anaconda3/lib/python3.11/site-packages/accelerate-0.30.1.dist-info/\n",
      "   from /opt/anaconda3/lib/python3.11/site-packages/~ccelerate-0.30.1.dist-info\n",
      "  Moving to /opt/anaconda3/lib/python3.11/site-packages/accelerate/\n",
      "   from /opt/anaconda3/lib/python3.11/site-packages/~ccelerate\n",
      "\u001b[31m  ERROR: Failed to restore /opt/anaconda3/bin/accelerate\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31m  ERROR: Failed to restore /opt/anaconda3/bin/accelerate-config\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31m  ERROR: Failed to restore /opt/anaconda3/bin/accelerate-estimate-memory\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31m  ERROR: Failed to restore /opt/anaconda3/bin/accelerate-launch\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31m  ERROR: Failed to restore /opt/anaconda3/lib/python3.11/site-packages/accelerate-0.30.1.dist-info/\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not install packages due to an OSError: [Errno 28] No space left on device: '/opt/anaconda3/bin/accelerate'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install simpletransformers\n",
    "#!pip install pyspellchecker\n",
    "#!pip install stanza\n",
    "#!pip install nltk\n",
    "#!pip install python-dotenv\n",
    "#!pip install werpy\n",
    "#!pip install wandb\n",
    "#!pip install transformers jiwer accelerate -U\n",
    "#!pip install tf-keras\n",
    "#!pip install transformers[torch]\n",
    "#!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-25 08:17:04.080326: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# general utils\n",
    "import werpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import stanza, os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from simpletransformers.ner import NERModel, NERArgs\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "\n",
    "# utils setup\n",
    "current_directory = os.getcwd()\n",
    "root_directory = os.path.abspath(os.path.join(current_directory, os.pardir))\n",
    "sys.path.append(root_directory)\n",
    "\n",
    "# logging\n",
    "import wandb\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"transformers_3.ipynb\"\n",
    "\n",
    "# custom utils\n",
    "from utils.io import Predictions\n",
    "from utils.metrics import LMR_Metrics\n",
    "from utils.io import LMR_BILOU_Scrapper, LMR_JSON_Scrapper\n",
    "from utils.preprocessing import Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import jiwer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exploring Data**\n",
    "\n",
    "The provided Train.csv contain many missing value so we have to get data from initial source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: california_wildfires_2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 3/3 [00:01<00:00,  2.29file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: canada_wildfires_2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 3/3 [00:01<00:00,  2.46file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: cyclone_idai_2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 3/3 [00:01<00:00,  2.26file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: ecuador_earthquake_2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 3/3 [00:01<00:00,  1.80file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: greece_wildfires_2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 3/3 [00:01<00:00,  2.47file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_dorian_2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 3/3 [00:01<00:00,  2.29file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_florence_2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 3/3 [00:01<00:00,  2.26file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_harvey_2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 3/3 [00:01<00:00,  2.30file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_irma_2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 3/3 [00:01<00:00,  2.44file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_maria_2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 3/3 [00:01<00:00,  2.46file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_matthew_2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 3/3 [00:01<00:00,  1.84file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: italy_earthquake_aug_2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 3/3 [00:01<00:00,  2.76file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: kaikoura_earthquake_2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 3/3 [00:01<00:00,  2.48file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: kerala_floods_2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 3/3 [00:01<00:00,  2.08file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: maryland_floods_2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 3/3 [00:01<00:00,  2.17file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: midwestern_us_floods_2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 3/3 [00:01<00:00,  2.26file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: pakistan_earthquake_2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 3/3 [00:01<00:00,  2.52file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: puebla_mexico_earthquake_2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 3/3 [00:01<00:00,  1.91file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: srilanka_floods_2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|██████████| 3/3 [00:01<00:00,  2.62file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "LMR_JSON_Scrapper(output_dir=\"../data/self_scrapped/raw\").run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let concatenate out dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SHAPE:  (14392, 3)\n",
      "TEST  SHAPE:  (4066, 3)\n",
      "DEV   SHAPE:  (2056, 3)\n"
     ]
    }
   ],
   "source": [
    "train_dfs = []\n",
    "dev_dfs   = []\n",
    "test_dfs  = []\n",
    "path_dfs  = \"../data/self_scrapped/raw\"\n",
    "for filename in os.listdir(path_dfs):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(path_dfs, filename)\n",
    "        if filename.startswith(\"train\"):\n",
    "            df = pd.read_csv(file_path)\n",
    "            train_dfs.append(df)\n",
    "        elif filename.startswith(\"dev\"):\n",
    "            df = pd.read_csv(file_path)\n",
    "            dev_dfs.append(df)\n",
    "        elif filename.startswith(\"test_unlabeled\"):\n",
    "            df = pd.read_csv(file_path)\n",
    "            test_dfs.append(df)\n",
    "\n",
    "df_train = pd.concat(train_dfs, ignore_index=True) if train_dfs else pd.DataFrame()\n",
    "df_test  = pd.concat(test_dfs, ignore_index=True) if test_dfs else pd.DataFrame()\n",
    "df_dev   = pd.concat(dev_dfs, ignore_index=True) if dev_dfs else pd.DataFrame()\n",
    "\n",
    "print(\"TRAIN SHAPE: \", df_train.shape)\n",
    "print(\"TEST  SHAPE: \", df_test.shape)\n",
    "print(\"DEV   SHAPE: \", df_dev.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We observe in sentencess that we have hashtag, no-ascii character, stopword , ... we have to clean data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>location_mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_1107545667596107776</td>\n",
       "      <td>Please assist and donate to the various organizations assisting those affected by #CycloneIdai. Every helping hand counts. #Zimbabwe</td>\n",
       "      <td>Zimbabwe=&gt;COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_1110219030256930816</td>\n",
       "      <td>I fear that the emergency situation caused by #cycloneidai is distracting us from the escalating insurgency in Cabo Delgado #Mozambique.</td>\n",
       "      <td>Cabo Delgado=&gt;DISTRICT * Mozambique=&gt;COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_1115677497046253568</td>\n",
       "      <td>Last Thursday police officer Constable Edward Dhumukwa (32) stationed at the Silver Stream command centre was arrested and appeared in court for alleged looting of donations valued at tens of thousands of United States dollars earmarked for #cycloneIdai victims in Chipinge.</td>\n",
       "      <td>United States=&gt;COUNTRY * Chipinge.=&gt;CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_1113080125321510913</td>\n",
       "      <td>Thanks to staff and patrons of @ZimLibrary_zw and citizens of Zimre Park for the generous clothing donations to our brothers and sisters affected by Cyclone Idai @UNZimbabwe @HigherLifeFDN @WFP @ChengetoAfrica @IRCEurope #CycloneIdai</td>\n",
       "      <td>Zimre Park=&gt;DISTRICT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_1109118377065365504</td>\n",
       "      <td>Too old to walk without a stick. Too weak to carry a brick. Torrents caused by #CycloneIdai destroyed his house in the middle of the night. For this man, rendered homeless in Chikwawa District, #Malawi, it wont be easy to rebuild. @cadecom @RedCross @SDGoals @WorldBank</td>\n",
       "      <td>Chikwawa=&gt;DISTRICT * Malawi=&gt;COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ID_1107632482575364097</td>\n",
       "      <td>We are kindly appealing to all members of the public to join hands with us &amp; drop off your donations for the #CycloneIDAI Disaster Relief effort at 7 Kenilworth Road, Newlands, Harare. Thank you so much to everyone who has donated so far. Together we can make a difference.</td>\n",
       "      <td>Newlands=&gt;COUNTRY * Harare=&gt;CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ID_1108243131504447489</td>\n",
       "      <td>#UPDATE We are pleased to announce that US$33,500 has been raised so far! On behalf of the #CycloneIdai victims, we THANK YOU all for the unparalleled selflessness &amp; generosity. Your retweet is as good as a donation! Together we can reach US$50,000!</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ID_1109454892576452609</td>\n",
       "      <td>As the sun sets, a 2nd truck is loaded and ready to head out! Thank you to everyone who donated goods help those affected by #CycloneIdai Lets continue to mobilise our efforts and support our fellow Zimbabweans ἟F἟CᾑDἿD</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ID_1110235833456607232</td>\n",
       "      <td>UPDATE: #CycloneIdai 24.03.2019 Due to the damaged Umvumvumvu Bridge, communications were made with Biriri High School representatives for an alternative to deliver the goods at Nyanyadzi High School. Thank You So Much to EVERYONE who contributed to the cause #MUFC #WeAreUnited</td>\n",
       "      <td>Umvumvumvu Bridge=&gt;HUMAN-MADE POINT-OF-INTEREST * Biriri High School=&gt;HUMAN-MADE POINT-OF-INTEREST * Nyanyadzi High School=&gt;HUMAN-MADE POINT-OF-INTEREST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ID_1107501351024631808</td>\n",
       "      <td>Bodies of #CycloneIdai victims pile up at Ngangu Roman Catholic as they wait for burial orders.</td>\n",
       "      <td>Ngangu Roman Catholic=&gt;HUMAN-MADE POINT-OF-INTEREST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ID_1109037497491902465</td>\n",
       "      <td>With One maternal death &amp; 4 birth complications already reported in Nsanje due to the floods, @UNFPA has quickly responded to order Reproductive Health kits for safe delivery &amp; prevent more maternal deaths &amp; complications among pregnant women #MalawiFloods #CycloneIdai</td>\n",
       "      <td>Nsanje=&gt;CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ID_1113047040693600259</td>\n",
       "      <td>UN Refugee Agency airlifts relief support to survivors of #CycloneIdai read more here  @MoHCCZim @CWGH1 @UNZimbabwe @WFP_Zimbabwe @UNFPA_Zimbabwe @UNICEFZIMBABWE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ID_1106866560872914944</td>\n",
       "      <td>The current tropical cyclone Idai reminds me of Rosita Mabuiango, who was born in a tree 19 years ago after her mother and family were caught in Mozambiques worst floods that killed 800 people. Rosita, who turned 19 on 1 March, wants to be a petrochemical engineer.</td>\n",
       "      <td>Mozambiques=&gt;COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ID_1118055352728264704</td>\n",
       "      <td>#Belarus to send humanitarian aid to cyclone-hit #Mozambique, #Zimbabwe</td>\n",
       "      <td>Belarus=&gt;COUNTRY * Mozambique=&gt;COUNTRY * Zimbabwe=&gt;COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ID_1111310652491087873</td>\n",
       "      <td>@UNFPAMocambique was able to deliver more dignity kits to Beira today. These kits will help women and girls affected by #CycloneIdai take care of essential hygiene and protection needs; giving them a little less to worry about as they try to rebuild their lives.</td>\n",
       "      <td>Beira=&gt;CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ID_1112708389342248961</td>\n",
       "      <td>110 Thousand people with disabilities affected by the impact of destructions from Cyclone Idai and subsequent flooding in Mozambique. Any response to be sustainable must address their need</td>\n",
       "      <td>Mozambique=&gt;COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ID_1107021987132649472</td>\n",
       "      <td>When such disasters strikes @WFP is the first to arrive and the last to leave. Our teams are on the ground monitoring and assisting the victims of #cycloneidai. We are certain we will deliver and #Mozambique #Malawi and #Zimbabwe will recover.</td>\n",
       "      <td>Mozambique=&gt;COUNTRY * Malawi=&gt;COUNTRY * Zimbabwe=&gt;COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ID_1109534109871865856</td>\n",
       "      <td>Distribution of relief continues to be coordinated by CPU from Chipinge Gvt complex. Thank you to all volunteers in this time of need. #CycloneIdai #Manicaland #Chipinge #Zimbabwe</td>\n",
       "      <td>Manicaland=&gt;CITY * Chipinge=&gt;CITY * Zimbabwe=&gt;COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ID_1109322734541172737</td>\n",
       "      <td>Urgent International help needed. Millions have no food, no blanket, nothing. Soon will have health crises because already fragile infrastructure was destroyed: Mozambique Cyclone Rescuers Struggle to Reach Victims in ‘Inland Ocean’</td>\n",
       "      <td>Mozambique=&gt;COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ID_1107656149648789504</td>\n",
       "      <td>This is the Buzi district in Mozambique after #CycloneIdai. Thousands of people have had to cling to trees and climbed onto roofs in an effort to try stay afloat. Hundreds of people are feared dead. #PrayForMozambique</td>\n",
       "      <td>Mozambique=&gt;COUNTRY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tweet_id  \\\n",
       "0   ID_1107545667596107776   \n",
       "1   ID_1110219030256930816   \n",
       "2   ID_1115677497046253568   \n",
       "3   ID_1113080125321510913   \n",
       "4   ID_1109118377065365504   \n",
       "5   ID_1107632482575364097   \n",
       "6   ID_1108243131504447489   \n",
       "7   ID_1109454892576452609   \n",
       "8   ID_1110235833456607232   \n",
       "9   ID_1107501351024631808   \n",
       "10  ID_1109037497491902465   \n",
       "11  ID_1113047040693600259   \n",
       "12  ID_1106866560872914944   \n",
       "13  ID_1118055352728264704   \n",
       "14  ID_1111310652491087873   \n",
       "15  ID_1112708389342248961   \n",
       "16  ID_1107021987132649472   \n",
       "17  ID_1109534109871865856   \n",
       "18  ID_1109322734541172737   \n",
       "19  ID_1107656149648789504   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                      text  \\\n",
       "0                                                                                                                                                     Please assist and donate to the various organizations assisting those affected by #CycloneIdai. Every helping hand counts. #Zimbabwe   \n",
       "1                                                                                                                                                 I fear that the emergency situation caused by #cycloneidai is distracting us from the escalating insurgency in Cabo Delgado #Mozambique.   \n",
       "2       Last Thursday police officer Constable Edward Dhumukwa (32) stationed at the Silver Stream command centre was arrested and appeared in court for alleged looting of donations valued at tens of thousands of United States dollars earmarked for #cycloneIdai victims in Chipinge.   \n",
       "3                                                Thanks to staff and patrons of @ZimLibrary_zw and citizens of Zimre Park for the generous clothing donations to our brothers and sisters affected by Cyclone Idai @UNZimbabwe @HigherLifeFDN @WFP @ChengetoAfrica @IRCEurope #CycloneIdai   \n",
       "4            Too old to walk without a stick. Too weak to carry a brick. Torrents caused by #CycloneIdai destroyed his house in the middle of the night. For this man, rendered homeless in Chikwawa District, #Malawi, it wont be easy to rebuild. @cadecom @RedCross @SDGoals @WorldBank   \n",
       "5        We are kindly appealing to all members of the public to join hands with us & drop off your donations for the #CycloneIDAI Disaster Relief effort at 7 Kenilworth Road, Newlands, Harare. Thank you so much to everyone who has donated so far. Together we can make a difference.   \n",
       "6                                #UPDATE We are pleased to announce that US$33,500 has been raised so far! On behalf of the #CycloneIdai victims, we THANK YOU all for the unparalleled selflessness & generosity. Your retweet is as good as a donation! Together we can reach US$50,000!   \n",
       "7                                                              As the sun sets, a 2nd truck is loaded and ready to head out! Thank you to everyone who donated goods help those affected by #CycloneIdai Lets continue to mobilise our efforts and support our fellow Zimbabweans ἟F἟CᾑDἿD   \n",
       "8   UPDATE: #CycloneIdai 24.03.2019 Due to the damaged Umvumvumvu Bridge, communications were made with Biriri High School representatives for an alternative to deliver the goods at Nyanyadzi High School. Thank You So Much to EVERYONE who contributed to the cause #MUFC #WeAreUnited   \n",
       "9                                                                                                                                                                                          Bodies of #CycloneIdai victims pile up at Ngangu Roman Catholic as they wait for burial orders.   \n",
       "10           With One maternal death & 4 birth complications already reported in Nsanje due to the floods, @UNFPA has quickly responded to order Reproductive Health kits for safe delivery & prevent more maternal deaths & complications among pregnant women #MalawiFloods #CycloneIdai   \n",
       "11                                                                                                                       UN Refugee Agency airlifts relief support to survivors of #CycloneIdai read more here  @MoHCCZim @CWGH1 @UNZimbabwe @WFP_Zimbabwe @UNFPA_Zimbabwe @UNICEFZIMBABWE   \n",
       "12               The current tropical cyclone Idai reminds me of Rosita Mabuiango, who was born in a tree 19 years ago after her mother and family were caught in Mozambiques worst floods that killed 800 people. Rosita, who turned 19 on 1 March, wants to be a petrochemical engineer.   \n",
       "13                                                                                                                                                                                                                 #Belarus to send humanitarian aid to cyclone-hit #Mozambique, #Zimbabwe   \n",
       "14                  @UNFPAMocambique was able to deliver more dignity kits to Beira today. These kits will help women and girls affected by #CycloneIdai take care of essential hygiene and protection needs; giving them a little less to worry about as they try to rebuild their lives.   \n",
       "15                                                                                            110 Thousand people with disabilities affected by the impact of destructions from Cyclone Idai and subsequent flooding in Mozambique. Any response to be sustainable must address their need   \n",
       "16                                     When such disasters strikes @WFP is the first to arrive and the last to leave. Our teams are on the ground monitoring and assisting the victims of #cycloneidai. We are certain we will deliver and #Mozambique #Malawi and #Zimbabwe will recover.   \n",
       "17                                                                                                     Distribution of relief continues to be coordinated by CPU from Chipinge Gvt complex. Thank you to all volunteers in this time of need. #CycloneIdai #Manicaland #Chipinge #Zimbabwe   \n",
       "18                                                Urgent International help needed. Millions have no food, no blanket, nothing. Soon will have health crises because already fragile infrastructure was destroyed: Mozambique Cyclone Rescuers Struggle to Reach Victims in ‘Inland Ocean’   \n",
       "19                                                               This is the Buzi district in Mozambique after #CycloneIdai. Thousands of people have had to cling to trees and climbed onto roofs in an effort to try stay afloat. Hundreds of people are feared dead. #PrayForMozambique   \n",
       "\n",
       "                                                                                                                                           location_mentions  \n",
       "0                                                                                                                                          Zimbabwe=>COUNTRY  \n",
       "1                                                                                                               Cabo Delgado=>DISTRICT * Mozambique=>COUNTRY  \n",
       "2                                                                                                                   United States=>COUNTRY * Chipinge.=>CITY  \n",
       "3                                                                                                                                       Zimre Park=>DISTRICT  \n",
       "4                                                                                                                       Chikwawa=>DISTRICT * Malawi=>COUNTRY  \n",
       "5                                                                                                                           Newlands=>COUNTRY * Harare=>CITY  \n",
       "6                                                                                                                                                        NaN  \n",
       "7                                                                                                                                                        NaN  \n",
       "8   Umvumvumvu Bridge=>HUMAN-MADE POINT-OF-INTEREST * Biriri High School=>HUMAN-MADE POINT-OF-INTEREST * Nyanyadzi High School=>HUMAN-MADE POINT-OF-INTEREST  \n",
       "9                                                                                                        Ngangu Roman Catholic=>HUMAN-MADE POINT-OF-INTEREST  \n",
       "10                                                                                                                                              Nsanje=>CITY  \n",
       "11                                                                                                                                                       NaN  \n",
       "12                                                                                                                                      Mozambiques=>COUNTRY  \n",
       "13                                                                                                Belarus=>COUNTRY * Mozambique=>COUNTRY * Zimbabwe=>COUNTRY  \n",
       "14                                                                                                                                               Beira=>CITY  \n",
       "15                                                                                                                                       Mozambique=>COUNTRY  \n",
       "16                                                                                                 Mozambique=>COUNTRY * Malawi=>COUNTRY * Zimbabwe=>COUNTRY  \n",
       "17                                                                                                     Manicaland=>CITY * Chipinge=>CITY * Zimbabwe=>COUNTRY  \n",
       "18                                                                                                                                       Mozambique=>COUNTRY  \n",
       "19                                                                                                                                       Mozambique=>COUNTRY  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are some sentences without a location mention. We need to look closer. It could be normal if there is no corresponding location found in the tweet, or it might be an error from the labeling task. Note that for the test set, it is normal for all location_mentions to be NaN. (😎 Yeah, we have to predict this value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_id                0\n",
      "text                    0\n",
      "location_mentions    4026\n",
      "dtype: int64\n",
      "tweet_id                0\n",
      "text                    0\n",
      "location_mentions    4066\n",
      "dtype: int64\n",
      "tweet_id               0\n",
      "text                   0\n",
      "location_mentions    573\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train.isnull().sum())\n",
    "print(df_test.isnull().sum())\n",
    "print(df_dev.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preprocessing Data**\n",
    "\n",
    "- Remove special character\n",
    "- Treat HASHTAG, USERTAG\n",
    "- Remove stop word\n",
    "- Tokenization\n",
    "- Stemming\n",
    "- BIO Tagging\n",
    "\n",
    "##### **<> BIO Tagging**\n",
    "\n",
    "BIO stands for Begin, Inside, and Outside. It’s a method for tagging tokens (words or subwords) in a sequence to identify entities within the text. Each token in the text is assigned a tag that indicates whether it is at the beginning of an entity, inside an entity, or outside of any entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "train_path = \"../data/transformed/train.tag.csv\"\n",
    "if not os.path.exists(train_path):\n",
    "    df_train = Preprocess.remove_non_ascii(df_train, column_name='text')\n",
    "    df_train = Preprocess.remove_usertag(df_train, column_name='text')\n",
    "    df_train = Preprocess.reformat_hashtag(df_train, column_name='text')\n",
    "    df_train = Preprocess.remove_stop_words(df_train, column_name='text', new_col=\"text_transformed\", transformation=[\n",
    "        \"tokenize\", \"lemma\", \"lower\"], save_in=\"../data/transformed/train.lemma.csv\")\n",
    "    df_tag_train = Preprocess.build_bilou_encoding(df_train, text_col=\"text_transformed\", save_in=train_path)\n",
    "else:\n",
    "    df_tag_train = pd.read_csv(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEV\n",
    "dev_path = \"../data/transformed/dev.tag.csv\"\n",
    "if not os.path.exists(dev_path):\n",
    "    df_dev = Preprocess.remove_non_ascii(df_dev, column_name='text')\n",
    "    df_dev = Preprocess.remove_usertag(df_dev, column_name='text')\n",
    "    df_dev = Preprocess.reformat_hashtag(df_dev, column_name='text')\n",
    "    df_dev = Preprocess.remove_stop_words(df_dev, column_name='text', new_col=\"text_transformed\", transformation=[\n",
    "        \"tokenize\", \"lemma\", \"lower\"], save_in=\"../data/transformed/dev.lemma.csv\")\n",
    "    df_tag_dev = Preprocess.build_bilou_encoding(df_dev, text_col=\"text_transformed\", save_in=dev_path)\n",
    "else:\n",
    "    df_tag_dev = pd.read_csv(dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "test_path = \"../data/transformed/test.lemma.csv\"\n",
    "if not os.path.exists(test_path):\n",
    "    df_test = Preprocess.remove_non_ascii(df_test, column_name='text')\n",
    "    df_test = Preprocess.remove_usertag(df_test, column_name='text')\n",
    "    df_test = Preprocess.reformat_hashtag(df_test, column_name='text')\n",
    "    df_test = Preprocess.remove_stop_words(df_test, column_name='text', new_col=\"text_transformed\", transformation=[\n",
    "        \"tokenize\", \"lemma\", \"lower\"], save_in=test_path)\n",
    "else:\n",
    "    df_test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>684</td>\n",
       "      <td>nearly</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>684</td>\n",
       "      <td>half</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>684</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>684</td>\n",
       "      <td>house</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>684</td>\n",
       "      <td>check</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>684</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>684</td>\n",
       "      <td>fire</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>684</td>\n",
       "      <td>stricken</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>684</td>\n",
       "      <td>area</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>684</td>\n",
       "      <td>deem</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>684</td>\n",
       "      <td>uninhabitable</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>684</td>\n",
       "      <td>go</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>684</td>\n",
       "      <td>c7</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>401</td>\n",
       "      <td>rt</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>401</td>\n",
       "      <td>greece</td>\n",
       "      <td>U-COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>401</td>\n",
       "      <td>death</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>401</td>\n",
       "      <td>toll</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>401</td>\n",
       "      <td>from</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>401</td>\n",
       "      <td>wildfire</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>401</td>\n",
       "      <td>hit</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>401</td>\n",
       "      <td>74</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>536</td>\n",
       "      <td>when</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>536</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>536</td>\n",
       "      <td>essence</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>536</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>536</td>\n",
       "      <td>cooperation</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>536</td>\n",
       "      <td>meet</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>536</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>536</td>\n",
       "      <td>sad</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>536</td>\n",
       "      <td>reality</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  sentence_id          words     labels\n",
       "0            0          684         nearly          O\n",
       "1            1          684           half          O\n",
       "2            2          684             of          O\n",
       "3            3          684          house          O\n",
       "4            4          684          check          O\n",
       "5            5          684             in          O\n",
       "6            6          684           fire          O\n",
       "7            7          684       stricken          O\n",
       "8            8          684           area          O\n",
       "9            9          684           deem          O\n",
       "10          10          684  uninhabitable          O\n",
       "11          11          684             go          O\n",
       "12          12          684             c7          O\n",
       "13          13          401             rt          O\n",
       "14          14          401         greece  U-COUNTRY\n",
       "15          15          401          death          O\n",
       "16          16          401           toll          O\n",
       "17          17          401           from          O\n",
       "18          18          401       wildfire          O\n",
       "19          19          401            hit          O\n",
       "20          20          401             74          O\n",
       "21          21          536           when          O\n",
       "22          22          536            the          O\n",
       "23          23          536        essence          O\n",
       "24          24          536             of          O\n",
       "25          25          536    cooperation          O\n",
       "26          26          536           meet          O\n",
       "27          27          536            the          O\n",
       "28          28          536            sad          O\n",
       "29          29          536        reality          O"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tag_train.head(30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prepare training, dev and test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tag_train[\"sentence_id\"] = LabelEncoder().fit_transform(df_tag_train[\"sentence_id\"])\n",
    "df_tag_dev[\"sentence_id\"]   = LabelEncoder().fit_transform(df_tag_dev[\"sentence_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>684</td>\n",
       "      <td>nearly</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>684</td>\n",
       "      <td>half</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>684</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>684</td>\n",
       "      <td>house</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>684</td>\n",
       "      <td>check</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  sentence_id   words labels\n",
       "0           0          684  nearly      O\n",
       "1           1          684    half      O\n",
       "2           2          684      of      O\n",
       "3           3          684   house      O\n",
       "4           4          684   check      O"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tag_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>684</td>\n",
       "      <td>nearly</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>684</td>\n",
       "      <td>half</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>684</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>684</td>\n",
       "      <td>house</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>684</td>\n",
       "      <td>check</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292721</th>\n",
       "      <td>5841</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292722</th>\n",
       "      <td>5841</td>\n",
       "      <td>ddrc</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292723</th>\n",
       "      <td>5841</td>\n",
       "      <td>patient</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292724</th>\n",
       "      <td>5841</td>\n",
       "      <td>preparedness</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292725</th>\n",
       "      <td>5841</td>\n",
       "      <td>plan</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>292726 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id         words labels\n",
       "0               684        nearly      O\n",
       "1               684          half      O\n",
       "2               684            of      O\n",
       "3               684         house      O\n",
       "4               684         check      O\n",
       "...             ...           ...    ...\n",
       "292721         5841           the      O\n",
       "292722         5841          ddrc      O\n",
       "292723         5841       patient      O\n",
       "292724         5841  preparedness      O\n",
       "292725         5841          plan      O\n",
       "\n",
       "[292726 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train  = df_tag_train[[\"sentence_id\", \"words\"]]\n",
    "X_test   = df_tag_dev[[\"sentence_id\", \"words\"]]\n",
    "y_train  = df_tag_train[\"labels\"]\n",
    "y_test   = df_tag_dev[\"labels\"]\n",
    "\n",
    "train_data = pd.DataFrame({\"sentence_id\": X_train[\"sentence_id\"], \"words\": X_train[\"words\"], \"labels\": y_train})\n",
    "test_data = pd.DataFrame({\"sentence_id\": X_test[\"sentence_id\"], \"words\": X_test[\"words\"], \"labels\": y_test})\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have to collaps word per sentence and do same for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINSET\n",
    "train_data['labels_list'] = train_data['labels']\n",
    "train_dataset = train_data.groupby('sentence_id').agg({\n",
    "    'words': ' '.join,\n",
    "    'labels': ','.join,\n",
    "    'labels_list': lambda x: list(x) \n",
    "}).reset_index()\n",
    "train_dataset.rename(columns={'words': 'sentence', 'labels': 'word_labels'}, inplace=True)\n",
    "\n",
    "# DEVSET\n",
    "test_data['labels_list'] = test_data['labels']\n",
    "test_dataset = test_data.groupby('sentence_id').agg({\n",
    "    'words': ' '.join,\n",
    "    'labels': ','.join,\n",
    "    'labels_list': lambda x: list(x) \n",
    "}).reset_index()\n",
    "test_dataset.rename(columns={'words': 'sentence', 'labels': 'word_labels'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_labels</th>\n",
       "      <th>labels_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>flash flood strike a maryland city on sunday wash out street and toss car like bath toy</td>\n",
       "      <td>O,O,O,O,U-STATE,O,O,O,O,O,O,O,O,O,O,O,O</td>\n",
       "      <td>[O, O, O, O, U-STATE, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>state of emergency declare for maryland flooding via</td>\n",
       "      <td>O,O,O,O,O,U-STATE,O,O</td>\n",
       "      <td>[O, O, O, O, O, U-STATE, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>other part of maryland also see significant damage from sunday storm include this baltimore city neighborhood dundalk and catonsville rain total span from 1 to 10 inch across maryland ecflood</td>\n",
       "      <td>O,O,O,U-STATE,O,O,O,O,O,O,O,O,O,U-CITY,O,O,O,O,O,O,O,O,O,O,O,O,O,O,U-STATE,O</td>\n",
       "      <td>[O, O, O, U-STATE, O, O, O, O, O, O, O, O, O, U-CITY, O, O, O, O, O, O, O, O, O, O, O, O, O, O, U-STATE, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>catastrophic flooding slam ellicott city maryland water rescues report the weather channel via</td>\n",
       "      <td>O,O,O,B-CITY,L-CITY,U-STATE,O,O,O,O,O,O,O</td>\n",
       "      <td>[O, O, O, B-CITY, L-CITY, U-STATE, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>watch 1 miss after flash flooding devastate ellicott city maryland gpwx</td>\n",
       "      <td>O,O,O,O,O,O,O,B-CITY,L-CITY,U-STATE,O</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-CITY, L-CITY, U-STATE, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  \\\n",
       "0            0   \n",
       "1            1   \n",
       "2            2   \n",
       "3            3   \n",
       "4            4   \n",
       "\n",
       "                                                                                                                                                                                          sentence  \\\n",
       "0                                                                                                          flash flood strike a maryland city on sunday wash out street and toss car like bath toy   \n",
       "1                                                                                                                                             state of emergency declare for maryland flooding via   \n",
       "2  other part of maryland also see significant damage from sunday storm include this baltimore city neighborhood dundalk and catonsville rain total span from 1 to 10 inch across maryland ecflood   \n",
       "3                                                                                                   catastrophic flooding slam ellicott city maryland water rescues report the weather channel via   \n",
       "4                                                                                                                          watch 1 miss after flash flooding devastate ellicott city maryland gpwx   \n",
       "\n",
       "                                                                    word_labels  \\\n",
       "0                                       O,O,O,O,U-STATE,O,O,O,O,O,O,O,O,O,O,O,O   \n",
       "1                                                         O,O,O,O,O,U-STATE,O,O   \n",
       "2  O,O,O,U-STATE,O,O,O,O,O,O,O,O,O,U-CITY,O,O,O,O,O,O,O,O,O,O,O,O,O,O,U-STATE,O   \n",
       "3                                     O,O,O,B-CITY,L-CITY,U-STATE,O,O,O,O,O,O,O   \n",
       "4                                         O,O,O,O,O,O,O,B-CITY,L-CITY,U-STATE,O   \n",
       "\n",
       "                                                                                                   labels_list  \n",
       "0                                                    [O, O, O, O, U-STATE, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "1                                                                               [O, O, O, O, O, U-STATE, O, O]  \n",
       "2  [O, O, O, U-STATE, O, O, O, O, O, O, O, O, O, U-CITY, O, O, O, O, O, O, O, O, O, O, O, O, O, O, U-STATE, O]  \n",
       "3                                                      [O, O, O, B-CITY, L-CITY, U-STATE, O, O, O, O, O, O, O]  \n",
       "4                                                            [O, O, O, O, O, O, O, B-CITY, L-CITY, U-STATE, O]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Modeling preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique tags from word labels\n",
    "tags = pd.concat([df_tag_train, df_tag_dev])[\"labels\"].unique().tolist()\n",
    "\n",
    "# Create label to ID and ID to label mappings\n",
    "label2id = {k: v for v, k in enumerate(tags)}\n",
    "id2label = {v: k for v, k in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Setup the model and tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/genereux.akotenou/.conda/envs/macbook/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer using a pre-trained BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=48, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a pre-trained BERT model for token classification with the custom label mappings\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-large-uncased\",\n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A custom dataset class is created to handle the input data, applying tokenization and ensuring that sequences are properly padded or truncated to fit the model’s expected input size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min:  1 ; Max:  60\n"
     ]
    }
   ],
   "source": [
    "df_temp = pd.concat([train_dataset, test_dataset])\n",
    "df_temp['labels_list_length'] = df_temp['labels_list'].apply(len)\n",
    "min_length = df_temp['labels_list_length'].min()\n",
    "max_length = df_temp['labels_list_length'].max()\n",
    "\n",
    "print(\"Min: \", min_length, \"; Max: \", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
    "    tokenized_sentence, labels = [], []\n",
    "    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "        labels.extend([label] * n_subwords)\n",
    "    return tokenized_sentence, labels\n",
    "    \n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.data.sentence[index]  \n",
    "        word_labels = self.data.word_labels[index]  \n",
    "        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
    "        \n",
    "        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"]\n",
    "        labels.insert(0, \"O\")\n",
    "        labels.insert(-1, \"O\")\n",
    "\n",
    "        if len(tokenized_sentence) > self.max_len:\n",
    "            tokenized_sentence = tokenized_sentence[:self.max_len]\n",
    "            labels = labels[:self.max_len]\n",
    "        else:\n",
    "            tokenized_sentence += ['[PAD]'] * (self.max_len - len(tokenized_sentence))\n",
    "            labels += [\"O\"] * (self.max_len - len(labels))\n",
    "\n",
    "        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "        label_ids = [label2id[label] for label in labels]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attn_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(label_ids, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 60+20\n",
    "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  2110,  1997,  5057, 13520,  2005,  5374,  9451,  3081,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor([ 0,  0,  0,  0,  0,  0, 15,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 16\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The fine-tuning process involves setting up the Trainer class from the transformers library, which simplifies the training loop, handles model optimization, and tracks metrics like accuracy, precision, recall, and F1-score. We specify training arguments such as the number of epochs, batch size, learning rate, and the device (GPU or CPU). The model is trained to minimize the loss function, adjusting its weights based on the labeled data to improve its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Helper\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels.flatten(), preds.flatten(), average='weighted')\n",
    "    acc = accuracy_score(labels.flatten(), preds.flatten())\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Training arguments for the Trainer API\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=VALID_BATCH_SIZE,\n",
    "    warmup_steps=25,\n",
    "    weight_decay=0.001,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients for larger effective batch size\n",
    "    fp16=True,  # Enable mixed precision training for faster computation\n",
    "    report_to=[\"none\"] #set this to true if you have a WANDB API key\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_set,\n",
    "    eval_dataset=testing_set,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Fine-tuning**\n",
    "\n",
    "For this task, we fine-tune the BertForTokenClassification model, a variant of BERT designed for sequence tagging tasks like Named Entity Recognition (NER). Fine-tuning involves taking a pre-trained BERT model and adapting it to our specific task—location mention recognition—by training it further on our labeled dataset. This step leverages the knowledge BERT has from its initial pre-training on a vast corpus while specializing it for identifying location mentions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='112' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/112 : < :, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Make WER Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_on_sentences(sentences, model, tokenizer, max_len=80, with_extra=False):\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    results = []\n",
    "    extra_results = []\n",
    "    \n",
    "    for sentence in tqdm(sentences):\n",
    "        # Tokenize the sentence and prepare input for the model\n",
    "        tokenized_sentence = tokenizer(\n",
    "            sentence.split(),\n",
    "            is_split_into_words=True,\n",
    "            return_offsets_mapping=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Move tensors to the correct device\n",
    "        input_ids = tokenized_sentence['input_ids'].to(device)\n",
    "        attention_mask = tokenized_sentence['attention_mask'].to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=2)  # Get the index of the highest logit for each token\n",
    "        \n",
    "        # Convert predictions to labels\n",
    "        pred_labels = [id2label[pred.item()] for pred in predictions[0]]\n",
    "        \n",
    "        # Get the original tokens from input_ids\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "        \n",
    "        # Filter out tokens with the 'O' label and concatenate them\n",
    "        filtered_tokens = [\n",
    "            token for token, label in zip(tokens, pred_labels)\n",
    "            if label != 'O' and token not in ['[CLS]', '[SEP]', '[PAD]']\n",
    "        ]\n",
    "        filtered_labels = [\n",
    "            label for token, label in zip(tokens, pred_labels)\n",
    "            if label != 'O' and token not in ['[CLS]', '[SEP]', '[PAD]']\n",
    "        ]\n",
    "        \n",
    "        results.append(\" \".join(filtered_tokens))\n",
    "        extra_results.append(filtered_labels)\n",
    "\n",
    "    if with_extra:\n",
    "        return results, extra_results\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let count NER label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O</td>\n",
       "      <td>314421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U-COUNTRY</td>\n",
       "      <td>4575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U-STATE</td>\n",
       "      <td>4298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U-CITY</td>\n",
       "      <td>2822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B-CITY</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>L-CITY</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B-COUNTRY</td>\n",
       "      <td>653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>L-COUNTRY</td>\n",
       "      <td>653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B-ISLAND</td>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>L-ISLAND</td>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>L-STATE</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>B-STATE</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>U-ISLAND</td>\n",
       "      <td>408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>B-HUMAN-MADE</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>L-HUMAN-MADE</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>B-COUNTY</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>L-COUNTY</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I-CITY</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I-HUMAN-MADE</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>B-NATURAL</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>L-NATURAL</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>U-DISTRICT</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>I-ROAD</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>U-CONTINENT</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I-STATE</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>U-COUNTY</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>U-HUMAN-MADE</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>B-ROAD</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>L-ROAD</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>I-COUNTRY</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>B-NEIGHBORHOOD</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>L-NEIGHBORHOOD</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>U-NATURAL</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>B-DISTRICT</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>L-DISTRICT</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>B-OTHER</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>L-OTHER</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>I-ISLAND</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>I-COUNTY</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>U-NEIGHBORHOOD</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>I-NATURAL</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>U-OTHER</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>U-ROAD</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>I-OTHER</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>I-NEIGHBORHOOD</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>B-CONTINENT</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>L-CONTINENT</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>I-DISTRICT</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Label  Frequency\n",
       "0                O     314421\n",
       "1        U-COUNTRY       4575\n",
       "2          U-STATE       4298\n",
       "3           U-CITY       2822\n",
       "4           B-CITY       1050\n",
       "5           L-CITY       1050\n",
       "6        B-COUNTRY        653\n",
       "7        L-COUNTRY        653\n",
       "8         B-ISLAND        572\n",
       "9         L-ISLAND        572\n",
       "10         L-STATE        446\n",
       "11         B-STATE        446\n",
       "12        U-ISLAND        408\n",
       "13    B-HUMAN-MADE        250\n",
       "14    L-HUMAN-MADE        250\n",
       "15        B-COUNTY        194\n",
       "16        L-COUNTY        194\n",
       "17          I-CITY        187\n",
       "18    I-HUMAN-MADE        153\n",
       "19       B-NATURAL        136\n",
       "20       L-NATURAL        136\n",
       "21      U-DISTRICT        121\n",
       "22          I-ROAD        101\n",
       "23     U-CONTINENT         85\n",
       "24         I-STATE         83\n",
       "25        U-COUNTY         80\n",
       "26    U-HUMAN-MADE         69\n",
       "27          B-ROAD         65\n",
       "28          L-ROAD         65\n",
       "29       I-COUNTRY         55\n",
       "30  B-NEIGHBORHOOD         49\n",
       "31  L-NEIGHBORHOOD         49\n",
       "32       U-NATURAL         49\n",
       "33      B-DISTRICT         46\n",
       "34      L-DISTRICT         46\n",
       "35         B-OTHER         36\n",
       "36         L-OTHER         36\n",
       "37        I-ISLAND         34\n",
       "38        I-COUNTY         33\n",
       "39  U-NEIGHBORHOOD         29\n",
       "40       I-NATURAL         22\n",
       "41         U-OTHER         20\n",
       "42          U-ROAD         18\n",
       "43         I-OTHER         14\n",
       "44  I-NEIGHBORHOOD         11\n",
       "45     B-CONTINENT         10\n",
       "46     L-CONTINENT         10\n",
       "47      I-DISTRICT          4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label = pd.concat([df_tag_train, df_tag_dev])[\"labels\"].unique().tolist()\n",
    "label_counts = pd.concat([df_tag_train, df_tag_dev])[\"labels\"].value_counts().reset_index()\n",
    "label_counts.columns = [\"Label\", \"Frequency\"]\n",
    "display(label_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let define model **Args** and hyperparameters optimisation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "sweep_config = {\n",
    "    \"method\": \"bayes\",  # grid, random\n",
    "    \"metric\": {\"name\": \"wer\", \"goal\": \"minimize\"},\n",
    "    \"parameters\": {\n",
    "        \"num_train_epochs\": {\"values\": [1, 2, 3, 5, 8]},\n",
    "        \"learning_rate\": {\"min\": 5e-5, \"max\": 4e-4},\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize a W&B sweep with the config defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/genereux.akotenou/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: fbynau3c\n",
      "Sweep URL: https://wandb.ai/genereux-akotenou-local/LMR-HPC/sweeps/fbynau3c\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"LMR-HPC\")\n",
    "#%%capture\n",
    "# wandb.init(project=\"LMR\", name=\"Location-Mention-Recognition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = NERArgs()\n",
    "\n",
    "# general\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.train_batch_size = 64\n",
    "model_args.eval_batch_size = 32\n",
    "model_args.labels_list = label\n",
    "model_args.use_multiprocessing = True\n",
    "model_args.wandb_project = \"LMR\"\n",
    "\n",
    "# for eaarly stoping\n",
    "\"\"\"model_args.use_early_stopping = True\n",
    "model_args.early_stopping_delta = 0.01\n",
    "model_args.early_stopping_metric = \"wer\"\n",
    "model_args.early_stopping_metric_minimize = False\n",
    "model_args.early_stopping_patience = 5\"\"\"\n",
    "model_args.evaluate_during_training_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval():\n",
    "    wandb.init(name=\"Location-Mention-Recognition-HPC\")\n",
    "    model = NERModel(\n",
    "        \"bert\", \n",
    "        \"bert-base-cased\", \n",
    "        use_cuda=False,\n",
    "        args=model_args, \n",
    "        sweep_config=wandb.config)\n",
    "\n",
    "    # Train the model\n",
    "    print('### TRAINING')\n",
    "    # train_data1, _ = train_test_split(train_data, test_size=0.99998)\n",
    "    model.train_model(\n",
    "        train_data, \n",
    "        eval_data=test_data, \n",
    "        wer=LMR_Metrics.wer_type\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print('### EVALUATION')\n",
    "    result, model_outputs, wrong_preds = model.eval_model(test_data, wer=LMR_Metrics.wer_type)\n",
    "\n",
    "    # Log metrics to wandb\n",
    "    wandb.log({\"eval_result\": result, \"model_outputs\": model_outputs})\n",
    "\n",
    "    # Sync wandb\n",
    "    wandb.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: t5notlaa with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00033320406223994023\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgenereux-akotenou\u001b[0m (\u001b[33mgenereux-akotenou-local\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/genereux.akotenou/LMR/approach_3/wandb/run-20240825_064852-t5notlaa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/genereux-akotenou-local/LMR-HPC/runs/t5notlaa' target=\"_blank\">Location-Mention-Recognition-HPC</a></strong> to <a href='https://wandb.ai/genereux-akotenou-local/LMR-HPC' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/genereux-akotenou-local/LMR-HPC/sweeps/fbynau3c' target=\"_blank\">https://wandb.ai/genereux-akotenou-local/LMR-HPC/sweeps/fbynau3c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/genereux-akotenou-local/LMR-HPC' target=\"_blank\">https://wandb.ai/genereux-akotenou-local/LMR-HPC</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/genereux-akotenou-local/LMR-HPC/sweeps/fbynau3c' target=\"_blank\">https://wandb.ai/genereux-akotenou-local/LMR-HPC/sweeps/fbynau3c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/genereux-akotenou-local/LMR-HPC/runs/t5notlaa' target=\"_blank\">https://wandb.ai/genereux-akotenou-local/LMR-HPC/runs/t5notlaa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/genereux.akotenou/.conda/envs/macbook/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TRAINING\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94f7d87b57d4131841380847729311e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e287435a564a03ad7b96314bca133e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:t5notlaa) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd13bdd50ee4d41bc161fd89f223ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.022 MB uploaded\\r'), FloatProgress(value=0.23401931227334294, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Location-Mention-Recognition-HPC</strong> at: <a href='https://wandb.ai/genereux-akotenou-local/LMR-HPC/runs/t5notlaa' target=\"_blank\">https://wandb.ai/genereux-akotenou-local/LMR-HPC/runs/t5notlaa</a><br/> View project at: <a href='https://wandb.ai/genereux-akotenou-local/LMR-HPC' target=\"_blank\">https://wandb.ai/genereux-akotenou-local/LMR-HPC</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240825_064852-t5notlaa/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:t5notlaa). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "721e2852fbcb45eca420d79363cc3cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112203945716222, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/genereux.akotenou/LMR/approach_3/wandb/run-20240825_064916-t5notlaa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/genereux-akotenou-local/LMR-HPC/runs/t5notlaa' target=\"_blank\">Location-Mention-Recognition-HPC</a></strong> to <a href='https://wandb.ai/genereux-akotenou-local/LMR-HPC' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/genereux-akotenou-local/LMR-HPC/sweeps/fbynau3c' target=\"_blank\">https://wandb.ai/genereux-akotenou-local/LMR-HPC/sweeps/fbynau3c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/genereux-akotenou-local/LMR-HPC' target=\"_blank\">https://wandb.ai/genereux-akotenou-local/LMR-HPC</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/genereux-akotenou-local/LMR-HPC/sweeps/fbynau3c' target=\"_blank\">https://wandb.ai/genereux-akotenou-local/LMR-HPC/sweeps/fbynau3c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/genereux-akotenou-local/LMR-HPC/runs/t5notlaa' target=\"_blank\">https://wandb.ai/genereux-akotenou-local/LMR-HPC/runs/t5notlaa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f9a3fb66d645d1a0294d908ae2437b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 2:   0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%%capture\n",
    "wandb.agent(sweep_id, train_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.08742212575106394,\n",
       " 'precision': 0.7850278199291857,\n",
       " 'recall': 0.7846309403437816,\n",
       " 'f1_score': 0.7848293299620733}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quick prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e89b32805a62473bb036ae7057a79d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff62f6660ffd4f27ace0f83583899deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions, raw_outputs = model.predict([\n",
    "    \"Elicott City, Maryland, struck by catastrophic flooding; 1 missing.\",\n",
    "    \"Memorial Day weekend floods ravage Maryland town\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'Elicott': 'B-CITY'},\n",
       "  {'City,': 'L-CITY'},\n",
       "  {'Maryland,': 'O'},\n",
       "  {'struck': 'O'},\n",
       "  {'by': 'O'},\n",
       "  {'catastrophic': 'O'},\n",
       "  {'flooding;': 'O'},\n",
       "  {'1': 'O'},\n",
       "  {'missing.': 'O'}],\n",
       " [{'Memorial': 'O'},\n",
       "  {'Day': 'O'},\n",
       "  {'weekend': 'O'},\n",
       "  {'floods': 'O'},\n",
       "  {'ravage': 'O'},\n",
       "  {'Maryland': 'O'},\n",
       "  {'town': 'O'}]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Make prediction for Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2942/2942 [05:19<00:00,  9.21it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b19176073544fc088c22a8d54a9858a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3a6ee5362b4e1e9af0197df0e51c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Prediction:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Data and Preprocess\n",
    "# df_context = pd.read_csv('../data/provided/Test.csv')\n",
    "# df_context = Preprocess.remove_special_characters(df_context, column_name='text')\n",
    "# df_context['text'] = df_context['text'].apply(lambda x: Preprocess.treat_hashtags(x))\n",
    "# df_context['text'] = df_context['text'].apply(lambda x: Preprocess.correct_spelling(x))\n",
    "# #df_context['text'] = df_context['text'].apply(lambda x: Preprocess.remove_stop_words(x))\n",
    "# df_context.to_csv(\"../data/provided/Test-processed.csv\")\n",
    "\n",
    "df_context = pd.read_csv('../data/provided/Test.csv')\n",
    "df_context = Preprocess.remove_non_ascii(df_context, column_name='text')\n",
    "df_context = Preprocess.remove_usertag(df_context, column_name='text')\n",
    "df_context = Preprocess.reformat_hashtag(df_context, column_name='text')\n",
    "df_context = Preprocess.remove_stop_words(df_context, column_name='text', new_col=\"text_transformed\", transformation=[\n",
    "    \"tokenize\", \"lemma\", \"lower\"], save_in=\"../data/provided/Test-processed.csv\")\n",
    "\n",
    "#df_context = pd.read_csv('../data/provided/Test-processed.csv')\n",
    "\n",
    "ids = df_context[\"tweet_id\"].values\n",
    "tweets = df_context[\"text_transformed\"].values\n",
    "\n",
    "# Make prediction\n",
    "predictions, raw_outputs = model.predict(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to ../submissions/submission_8.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract Location Mention based on model output\n",
    "results = []\n",
    "for sentence in predictions:\n",
    "    result = \" \".join([word for d in sentence for word, tag in d.items() if tag != 'O'])\n",
    "    if result == \"\":\n",
    "        result = \" \"\n",
    "    results.append(result)\n",
    "\n",
    "Predictions.to_csv(ids, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
