{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: 0.5em; background-color: #1876d1; color: #fff; font-weight: bold; font-size: 1.4em;\">\n",
    "    [Approach 3]  Location Mention Recognition - NER BERT Transformer\n",
    "</div>\n",
    "\n",
    "In this Jupyter notebook, we will use Name Entity Recognition to extract from X (Twitter formely) tweets Location Mention from Emergency Situation.\n",
    "\n",
    "Note :\n",
    "* Do NER\n",
    "* Try BERT Model\n",
    "* Extract Location Mention\n",
    "\n",
    "---\n",
    "<b>#Microsoft Learn Challenge, #Zindi, #Hamad Bin Khalifa University </b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install simpletransformers\n",
    "#!pip install pyspellchecker\n",
    "#!pip install stanza\n",
    "#!pip install nltk\n",
    "#!pip install python-dotenv\n",
    "#!pip install werpy\n",
    "#!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general utils\n",
    "import werpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import stanza, os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from simpletransformers.ner import NERModel, NERArgs\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "\n",
    "# utils setup\n",
    "current_directory = os.getcwd()\n",
    "root_directory = os.path.abspath(os.path.join(current_directory, os.pardir))\n",
    "sys.path.append(root_directory)\n",
    "\n",
    "# logging\n",
    "import wandb\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"transformers_3.ipynb\"\n",
    "\n",
    "# custom utils\n",
    "from utils.io import Predictions\n",
    "from utils.metrics import LMR_Metrics\n",
    "from utils.io import LMR_BILOU_Scrapper, LMR_JSON_Scrapper\n",
    "from utils.preprocessing import Preprocess"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exploring Data**\n",
    "\n",
    "The provided Train.csv contain many missing value so we have to get data from initial source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: california_wildfires_2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.17file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: canada_wildfires_2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.35file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: cyclone_idai_2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.58file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: ecuador_earthquake_2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.51file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: greece_wildfires_2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.39file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_dorian_2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.74file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_florence_2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.14file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_harvey_2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.34file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_irma_2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.28file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_maria_2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.42file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hurricane_matthew_2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.99file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: italy_earthquake_aug_2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.52file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: kaikoura_earthquake_2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.46file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: kerala_floods_2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.35file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: maryland_floods_2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.74file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: midwestern_us_floods_2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.48file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: pakistan_earthquake_2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.54file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: puebla_mexico_earthquake_2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.38file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: srilanka_floods_2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.27file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "LMR_JSON_Scrapper(output_dir=\"../data/self_scrapped/raw\").run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let concatenate out dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SHAPE:  (14392, 3)\n",
      "TEST  SHAPE:  (4066, 3)\n",
      "DEV   SHAPE:  (2056, 3)\n"
     ]
    }
   ],
   "source": [
    "train_dfs = []\n",
    "dev_dfs   = []\n",
    "test_dfs  = []\n",
    "path_dfs  = \"../data/self_scrapped/raw\"\n",
    "for filename in os.listdir(path_dfs):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(path_dfs, filename)\n",
    "        if filename.startswith(\"train\"):\n",
    "            df = pd.read_csv(file_path)\n",
    "            train_dfs.append(df)\n",
    "        elif filename.startswith(\"dev\"):\n",
    "            df = pd.read_csv(file_path)\n",
    "            dev_dfs.append(df)\n",
    "        elif filename.startswith(\"test_unlabeled\"):\n",
    "            df = pd.read_csv(file_path)\n",
    "            test_dfs.append(df)\n",
    "\n",
    "df_train = pd.concat(train_dfs, ignore_index=True) if train_dfs else pd.DataFrame()\n",
    "df_test  = pd.concat(test_dfs, ignore_index=True) if test_dfs else pd.DataFrame()\n",
    "df_dev   = pd.concat(dev_dfs, ignore_index=True) if dev_dfs else pd.DataFrame()\n",
    "\n",
    "print(\"TRAIN SHAPE: \", df_train.shape)\n",
    "print(\"TEST  SHAPE: \", df_test.shape)\n",
    "print(\"DEV   SHAPE: \", df_dev.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We observe in sentencess that we have hashtag, no-ascii character, stopword , ... we have to clean data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>location_mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_1022420413882744832</td>\n",
       "      <td>Nearly half of #houses checked in #fire-stricken areas deemed #uninhabitable #GO #PrayForGreece #PrayForAthens #AthensFires ·ºûC·ºü7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_1021778661895294976</td>\n",
       "      <td>RT @anadoluagency: #Greece: Death toll from wildfires hits 74</td>\n",
       "      <td>Greece=&gt;COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_1022015997740503042</td>\n",
       "      <td>When the essence of cooperation meets the sad reality of lifeThe IPA partner country offers financial aid to Greece to handle disaster @InterregIPACBC #Greecefires</td>\n",
       "      <td>Greece=&gt;COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_1022557424585240576</td>\n",
       "      <td>We are live from the Lureio Idrima the orphanage and nursing home operared by the nuns of the Holy Trinity Monastery that was destroyed by the fire in Neos Voutzas. Here too the scene is apocalyptic.</td>\n",
       "      <td>Holy Trinity Monastery=&gt;HUMAN-MADE POINT-OF-INTEREST * Neos Voutzas=&gt;NEIGHBORHOOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_1021749412639457280</td>\n",
       "      <td>RT @AP: Greek prime minister declares 3-day national mourning period for dozens killed by wildfires near Athens.</td>\n",
       "      <td>Athens=&gt;CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ID_1024662121391579136</td>\n",
       "      <td>#Greece vows to speed up destruction of illegal property after #wildfires</td>\n",
       "      <td>Greece=&gt;COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ID_1022441664697298944</td>\n",
       "      <td>In Mati, hundreds of humanitarian volunteers have joined relief efforts following Greece‚Äôs most deadly wildfires in a decade</td>\n",
       "      <td>Mati=&gt;CITY * Greece=&gt;COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ID_1024974395759108097</td>\n",
       "      <td>State Minister #Flambouraris offers thanks for assistance to #Greecefires victims</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ID_1022473412034416640</td>\n",
       "      <td>Unfortunately theres the first confirmed fatality, hope to be the last, among tourists at the #AthensFires: an Irishman in honeymoon. So sorry, R.I.P ·Ω¢2  #Mati #Attica #wildfires #PrayForGreece #PrayForAthens #Œ†œÖœÅŒ∫Œ±Œ≥ŒπŒ± #ŒëœÑœÑŒπŒ∫Œ∑ #ŒºŒ±œÑŒπ</td>\n",
       "      <td>Attica=&gt;CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ID_1022371614091096064</td>\n",
       "      <td>A special account has been opened at the Bank of Greece for donations in support of fire victims. Account number 23/2341195169, IBAN GR4601000230000002341195169) available for foreign states, businesses and individuals from Greece and abroad to provide their financial support</td>\n",
       "      <td>Greece=&gt;COUNTRY * Greece=&gt;COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ID_1021723092593401857</td>\n",
       "      <td>Greek Prime Minister declares three-day national mourning as 60 people are reported dead in Greece wildfires - Irish Department of Foreign Affairs say they are providing consular assistance to a number of people</td>\n",
       "      <td>Greece=&gt;COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ID_1023305338672611328</td>\n",
       "      <td>We are deeply saddened by the loss of more than 50 people in the forest fires in the Eastern #Attica region of #Greece. We extend our deepest condolences to the Greek people and Greek government. Our prayers and thoughts are with the families affected from the fire.</td>\n",
       "      <td>Greece=&gt;COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ID_1022047647010631680</td>\n",
       "      <td>#Greek Orthodox Church could think of other people for a change and donate some money to the victims of the #Athensfires #œÄœÖœÅŒ∫Œ±Œ≥ŒπŒ±</td>\n",
       "      <td>Greek Orthodox Church=&gt;HUMAN-MADE POINT-OF-INTEREST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ID_1023484563799126016</td>\n",
       "      <td>Greek firefighters join public outcry at ‚Äòwoeful‚Äô response to lethal wildfires and support the argument of @YanisVaroufakis that austerity enlarged the scale of this disaster:  #GreekFires @atsipras @SYRIZAEP #Greece #AthensFires</td>\n",
       "      <td>Greece=&gt;COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ID_1022586744837943296</td>\n",
       "      <td>2nd DAY crypto donations results for the Victims of Greek Wildfires (paypal and cryptocurrencies). Keep donating!  $BTC $ETH #Greece #Œ†œÖœÅŒ∫Œ±Œ≥ŒπŒ± #wildfires #donation #victims</td>\n",
       "      <td>Greece=&gt;COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ID_1024253442049683456</td>\n",
       "      <td>Send Aid to Wildfire-Ravaged Greece</td>\n",
       "      <td>Greece=&gt;COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ID_1022451562080333824</td>\n",
       "      <td>RT @amna_newseng: Death toll from #Greecefires rises to 83 following death of burns patient in hospital</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ID_1022882396549472256</td>\n",
       "      <td>Financial Mirror dot com ‚Äì CYPRUS: Over 1,000 aid boxes for Greece fire victims in Tsunami of love</td>\n",
       "      <td>Greece=&gt;COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ID_1021772564706811904</td>\n",
       "      <td>plse #PrayForGreece #PrayForAthens 74 people lost their lives and 150 got injured</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ID_1021770980576952321</td>\n",
       "      <td>#Greece wildfires: 60 dead in holiday area - #BBC News #rescueservices #Athens #localmedia #Mati</td>\n",
       "      <td>Greece=&gt;COUNTRY * Athens=&gt;CITY * Mati=&gt;CITY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tweet_id  \\\n",
       "0   ID_1022420413882744832   \n",
       "1   ID_1021778661895294976   \n",
       "2   ID_1022015997740503042   \n",
       "3   ID_1022557424585240576   \n",
       "4   ID_1021749412639457280   \n",
       "5   ID_1024662121391579136   \n",
       "6   ID_1022441664697298944   \n",
       "7   ID_1024974395759108097   \n",
       "8   ID_1022473412034416640   \n",
       "9   ID_1022371614091096064   \n",
       "10  ID_1021723092593401857   \n",
       "11  ID_1023305338672611328   \n",
       "12  ID_1022047647010631680   \n",
       "13  ID_1023484563799126016   \n",
       "14  ID_1022586744837943296   \n",
       "15  ID_1024253442049683456   \n",
       "16  ID_1022451562080333824   \n",
       "17  ID_1022882396549472256   \n",
       "18  ID_1021772564706811904   \n",
       "19  ID_1021770980576952321   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                    text  \\\n",
       "0                                                                                                                                                       Nearly half of #houses checked in #fire-stricken areas deemed #uninhabitable #GO #PrayForGreece #PrayForAthens #AthensFires ·ºûC·ºü7   \n",
       "1                                                                                                                                                                                                                          RT @anadoluagency: #Greece: Death toll from wildfires hits 74   \n",
       "2                                                                                                                    When the essence of cooperation meets the sad reality of lifeThe IPA partner country offers financial aid to Greece to handle disaster @InterregIPACBC #Greecefires   \n",
       "3                                                                                We are live from the Lureio Idrima the orphanage and nursing home operared by the nuns of the Holy Trinity Monastery that was destroyed by the fire in Neos Voutzas. Here too the scene is apocalyptic.   \n",
       "4                                                                                                                                                                       RT @AP: Greek prime minister declares 3-day national mourning period for dozens killed by wildfires near Athens.   \n",
       "5                                                                                                                                                                                                              #Greece vows to speed up destruction of illegal property after #wildfires   \n",
       "6                                                                                                                                                           In Mati, hundreds of humanitarian volunteers have joined relief efforts following Greece‚Äôs most deadly wildfires in a decade   \n",
       "7                                                                                                                                                                                                      State Minister #Flambouraris offers thanks for assistance to #Greecefires victims   \n",
       "8                                               Unfortunately theres the first confirmed fatality, hope to be the last, among tourists at the #AthensFires: an Irishman in honeymoon. So sorry, R.I.P ·Ω¢2  #Mati #Attica #wildfires #PrayForGreece #PrayForAthens #Œ†œÖœÅŒ∫Œ±Œ≥ŒπŒ± #ŒëœÑœÑŒπŒ∫Œ∑ #ŒºŒ±œÑŒπ   \n",
       "9   A special account has been opened at the Bank of Greece for donations in support of fire victims. Account number 23/2341195169, IBAN GR4601000230000002341195169) available for foreign states, businesses and individuals from Greece and abroad to provide their financial support   \n",
       "10                                                                   Greek Prime Minister declares three-day national mourning as 60 people are reported dead in Greece wildfires - Irish Department of Foreign Affairs say they are providing consular assistance to a number of people   \n",
       "11            We are deeply saddened by the loss of more than 50 people in the forest fires in the Eastern #Attica region of #Greece. We extend our deepest condolences to the Greek people and Greek government. Our prayers and thoughts are with the families affected from the fire.   \n",
       "12                                                                                                                                                    #Greek Orthodox Church could think of other people for a change and donate some money to the victims of the #Athensfires #œÄœÖœÅŒ∫Œ±Œ≥ŒπŒ±   \n",
       "13                                                 Greek firefighters join public outcry at ‚Äòwoeful‚Äô response to lethal wildfires and support the argument of @YanisVaroufakis that austerity enlarged the scale of this disaster:  #GreekFires @atsipras @SYRIZAEP #Greece #AthensFires   \n",
       "14                                                                                                          2nd DAY crypto donations results for the Victims of Greek Wildfires (paypal and cryptocurrencies). Keep donating!  $BTC $ETH #Greece #Œ†œÖœÅŒ∫Œ±Œ≥ŒπŒ± #wildfires #donation #victims   \n",
       "15                                                                                                                                                                                                                                                   Send Aid to Wildfire-Ravaged Greece   \n",
       "16                                                                                                                                                                               RT @amna_newseng: Death toll from #Greecefires rises to 83 following death of burns patient in hospital   \n",
       "17                                                                                                                                                                                    Financial Mirror dot com ‚Äì CYPRUS: Over 1,000 aid boxes for Greece fire victims in Tsunami of love   \n",
       "18                                                                                                                                                                                                     plse #PrayForGreece #PrayForAthens 74 people lost their lives and 150 got injured   \n",
       "19                                                                                                                                                                                      #Greece wildfires: 60 dead in holiday area - #BBC News #rescueservices #Athens #localmedia #Mati   \n",
       "\n",
       "                                                                    location_mentions  \n",
       "0                                                                                 NaN  \n",
       "1                                                                     Greece=>COUNTRY  \n",
       "2                                                                     Greece=>COUNTRY  \n",
       "3   Holy Trinity Monastery=>HUMAN-MADE POINT-OF-INTEREST * Neos Voutzas=>NEIGHBORHOOD  \n",
       "4                                                                        Athens=>CITY  \n",
       "5                                                                     Greece=>COUNTRY  \n",
       "6                                                        Mati=>CITY * Greece=>COUNTRY  \n",
       "7                                                                                 NaN  \n",
       "8                                                                        Attica=>CITY  \n",
       "9                                                   Greece=>COUNTRY * Greece=>COUNTRY  \n",
       "10                                                                    Greece=>COUNTRY  \n",
       "11                                                                    Greece=>COUNTRY  \n",
       "12                                Greek Orthodox Church=>HUMAN-MADE POINT-OF-INTEREST  \n",
       "13                                                                    Greece=>COUNTRY  \n",
       "14                                                                    Greece=>COUNTRY  \n",
       "15                                                                    Greece=>COUNTRY  \n",
       "16                                                                                NaN  \n",
       "17                                                                    Greece=>COUNTRY  \n",
       "18                                                                                NaN  \n",
       "19                                        Greece=>COUNTRY * Athens=>CITY * Mati=>CITY  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are some sentences without a location mention. We need to look closer. It could be normal if there is no corresponding location found in the tweet, or it might be an error from the labeling task. Note that for the test set, it is normal for all location_mentions to be NaN. (üòé Yeah, we have to predict this value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_id                0\n",
      "text                    0\n",
      "location_mentions    4026\n",
      "dtype: int64\n",
      "tweet_id                0\n",
      "text                    0\n",
      "location_mentions    4066\n",
      "dtype: int64\n",
      "tweet_id               0\n",
      "text                   0\n",
      "location_mentions    573\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train.isnull().sum())\n",
    "print(df_test.isnull().sum())\n",
    "print(df_dev.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preprocessing Data**\n",
    "\n",
    "- Remove special character\n",
    "- Treat HASHTAG, USERTAG\n",
    "- Remove stop word\n",
    "- Tokenization\n",
    "- Stemming\n",
    "- BIO Tagging\n",
    "\n",
    "##### **<> BIO Tagging**\n",
    "\n",
    "BIO stands for Begin, Inside, and Outside. It‚Äôs a method for tagging tokens (words or subwords) in a sequence to identify entities within the text. Each token in the text is assigned a tag that indicates whether it is at the beginning of an entity, inside an entity, or outside of any entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "train_path = \"../data/transformed/train.tag.csv\"\n",
    "if not os.path.exists(train_path):\n",
    "    df_train = Preprocess.remove_non_ascii(df_train, column_name='text')\n",
    "    df_train = Preprocess.remove_usertag(df_train, column_name='text')\n",
    "    df_train = Preprocess.reformat_hashtag(df_train, column_name='text')\n",
    "    df_train = Preprocess.remove_stop_words(df_train, column_name='text', new_col=\"text_transformed\", transformation=[\n",
    "        \"tokenize\", \"lemma\", \"lower\"], save_in=\"../data/transformed/train.lemma.csv\")\n",
    "    df_tag_train = Preprocess.build_bilou_encoding(df_train, text_col=\"text_transformed\", save_in=train_path)\n",
    "else:\n",
    "    df_tag_train = pd.read_csv(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEV\n",
    "dev_path = \"../data/transformed/dev.tag.csv\"\n",
    "if not os.path.exists(dev_path):\n",
    "    df_dev = Preprocess.remove_non_ascii(df_dev, column_name='text')\n",
    "    df_dev = Preprocess.remove_usertag(df_dev, column_name='text')\n",
    "    df_dev = Preprocess.reformat_hashtag(df_dev, column_name='text')\n",
    "    df_dev = Preprocess.remove_stop_words(df_dev, column_name='text', new_col=\"text_transformed\", transformation=[\n",
    "        \"tokenize\", \"lemma\", \"lower\"], save_in=\"../data/transformed/dev.lemma.csv\")\n",
    "    df_tag_dev = Preprocess.build_bilou_encoding(df_dev, text_col=\"text_transformed\", save_in=dev_path)\n",
    "else:\n",
    "    df_tag_dev = pd.read_csv(dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "test_path = \"../data/transformed/test.lemma.csv\"\n",
    "if not os.path.exists(test_path):\n",
    "    df_test = Preprocess.remove_non_ascii(df_test, column_name='text')\n",
    "    df_test = Preprocess.remove_usertag(df_test, column_name='text')\n",
    "    df_test = Preprocess.reformat_hashtag(df_test, column_name='text')\n",
    "    df_test = Preprocess.remove_stop_words(df_test, column_name='text', new_col=\"text_transformed\", transformation=[\n",
    "        \"tokenize\", \"lemma\", \"lower\"], save_in=test_path)\n",
    "else:\n",
    "    df_test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>684</td>\n",
       "      <td>nearly</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>684</td>\n",
       "      <td>half</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>684</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>684</td>\n",
       "      <td>house</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>684</td>\n",
       "      <td>check</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>684</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>684</td>\n",
       "      <td>fire</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>684</td>\n",
       "      <td>stricken</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>684</td>\n",
       "      <td>area</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>684</td>\n",
       "      <td>deem</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>684</td>\n",
       "      <td>uninhabitable</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>684</td>\n",
       "      <td>go</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>684</td>\n",
       "      <td>c7</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>401</td>\n",
       "      <td>rt</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>401</td>\n",
       "      <td>greece</td>\n",
       "      <td>U-COUNTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>401</td>\n",
       "      <td>death</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>401</td>\n",
       "      <td>toll</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>401</td>\n",
       "      <td>from</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>401</td>\n",
       "      <td>wildfire</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>401</td>\n",
       "      <td>hit</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>401</td>\n",
       "      <td>74</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>536</td>\n",
       "      <td>when</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>536</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>536</td>\n",
       "      <td>essence</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>536</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>536</td>\n",
       "      <td>cooperation</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>536</td>\n",
       "      <td>meet</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>536</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>536</td>\n",
       "      <td>sad</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>536</td>\n",
       "      <td>reality</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  sentence_id          words     labels\n",
       "0            0          684         nearly          O\n",
       "1            1          684           half          O\n",
       "2            2          684             of          O\n",
       "3            3          684          house          O\n",
       "4            4          684          check          O\n",
       "5            5          684             in          O\n",
       "6            6          684           fire          O\n",
       "7            7          684       stricken          O\n",
       "8            8          684           area          O\n",
       "9            9          684           deem          O\n",
       "10          10          684  uninhabitable          O\n",
       "11          11          684             go          O\n",
       "12          12          684             c7          O\n",
       "13          13          401             rt          O\n",
       "14          14          401         greece  U-COUNTRY\n",
       "15          15          401          death          O\n",
       "16          16          401           toll          O\n",
       "17          17          401           from          O\n",
       "18          18          401       wildfire          O\n",
       "19          19          401            hit          O\n",
       "20          20          401             74          O\n",
       "21          21          536           when          O\n",
       "22          22          536            the          O\n",
       "23          23          536        essence          O\n",
       "24          24          536             of          O\n",
       "25          25          536    cooperation          O\n",
       "26          26          536           meet          O\n",
       "27          27          536            the          O\n",
       "28          28          536            sad          O\n",
       "29          29          536        reality          O"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tag_train.head(30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prepare training, dev and test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tag_train[\"sentence_id\"] = LabelEncoder().fit_transform(df_tag_train[\"sentence_id\"])\n",
    "df_tag_dev[\"sentence_id\"]   = LabelEncoder().fit_transform(df_tag_dev[\"sentence_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>684</td>\n",
       "      <td>nearly</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>684</td>\n",
       "      <td>half</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>684</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>684</td>\n",
       "      <td>house</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>684</td>\n",
       "      <td>check</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  sentence_id   words labels\n",
       "0           0          684  nearly      O\n",
       "1           1          684    half      O\n",
       "2           2          684      of      O\n",
       "3           3          684   house      O\n",
       "4           4          684   check      O"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tag_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>684</td>\n",
       "      <td>nearly</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>684</td>\n",
       "      <td>half</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>684</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>684</td>\n",
       "      <td>house</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>684</td>\n",
       "      <td>check</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292721</th>\n",
       "      <td>5841</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292722</th>\n",
       "      <td>5841</td>\n",
       "      <td>ddrc</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292723</th>\n",
       "      <td>5841</td>\n",
       "      <td>patient</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292724</th>\n",
       "      <td>5841</td>\n",
       "      <td>preparedness</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292725</th>\n",
       "      <td>5841</td>\n",
       "      <td>plan</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>292726 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id         words labels\n",
       "0               684        nearly      O\n",
       "1               684          half      O\n",
       "2               684            of      O\n",
       "3               684         house      O\n",
       "4               684         check      O\n",
       "...             ...           ...    ...\n",
       "292721         5841           the      O\n",
       "292722         5841          ddrc      O\n",
       "292723         5841       patient      O\n",
       "292724         5841  preparedness      O\n",
       "292725         5841          plan      O\n",
       "\n",
       "[292726 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train  = df_tag_train[[\"sentence_id\", \"words\"]]\n",
    "X_test   = df_tag_dev[[\"sentence_id\", \"words\"]]\n",
    "y_train  = df_tag_train[\"labels\"]\n",
    "y_test   = df_tag_dev[\"labels\"]\n",
    "\n",
    "train_data = pd.DataFrame({\"sentence_id\": X_train[\"sentence_id\"], \"words\": X_train[\"words\"], \"labels\": y_train})\n",
    "test_data = pd.DataFrame({\"sentence_id\": X_test[\"sentence_id\"], \"words\": X_test[\"words\"], \"labels\": y_test})\n",
    "\n",
    "train_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let count NER label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O</td>\n",
       "      <td>314421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U-COUNTRY</td>\n",
       "      <td>4575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U-STATE</td>\n",
       "      <td>4298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U-CITY</td>\n",
       "      <td>2822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B-CITY</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>L-CITY</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B-COUNTRY</td>\n",
       "      <td>653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>L-COUNTRY</td>\n",
       "      <td>653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>L-ISLAND</td>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B-ISLAND</td>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>B-STATE</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>L-STATE</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>U-ISLAND</td>\n",
       "      <td>408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>B-HUMAN-MADE</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>L-HUMAN-MADE</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>L-COUNTY</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>B-COUNTY</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I-CITY</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I-HUMAN-MADE</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>B-NATURAL</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>L-NATURAL</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>U-DISTRICT</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>I-ROAD</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>U-CONTINENT</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I-STATE</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>U-COUNTY</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>U-HUMAN-MADE</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>B-ROAD</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>L-ROAD</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>I-COUNTRY</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>U-NATURAL</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>L-NEIGHBORHOOD</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>B-NEIGHBORHOOD</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>B-DISTRICT</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>L-DISTRICT</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>B-OTHER</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>L-OTHER</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>I-ISLAND</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>I-COUNTY</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>U-NEIGHBORHOOD</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>I-NATURAL</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>U-OTHER</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>U-ROAD</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>I-OTHER</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>I-NEIGHBORHOOD</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>B-CONTINENT</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>L-CONTINENT</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>I-DISTRICT</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Label  Frequency\n",
       "0                O     314421\n",
       "1        U-COUNTRY       4575\n",
       "2          U-STATE       4298\n",
       "3           U-CITY       2822\n",
       "4           B-CITY       1050\n",
       "5           L-CITY       1050\n",
       "6        B-COUNTRY        653\n",
       "7        L-COUNTRY        653\n",
       "8         L-ISLAND        572\n",
       "9         B-ISLAND        572\n",
       "10         B-STATE        446\n",
       "11         L-STATE        446\n",
       "12        U-ISLAND        408\n",
       "13    B-HUMAN-MADE        250\n",
       "14    L-HUMAN-MADE        250\n",
       "15        L-COUNTY        194\n",
       "16        B-COUNTY        194\n",
       "17          I-CITY        187\n",
       "18    I-HUMAN-MADE        153\n",
       "19       B-NATURAL        136\n",
       "20       L-NATURAL        136\n",
       "21      U-DISTRICT        121\n",
       "22          I-ROAD        101\n",
       "23     U-CONTINENT         85\n",
       "24         I-STATE         83\n",
       "25        U-COUNTY         80\n",
       "26    U-HUMAN-MADE         69\n",
       "27          B-ROAD         65\n",
       "28          L-ROAD         65\n",
       "29       I-COUNTRY         55\n",
       "30       U-NATURAL         49\n",
       "31  L-NEIGHBORHOOD         49\n",
       "32  B-NEIGHBORHOOD         49\n",
       "33      B-DISTRICT         46\n",
       "34      L-DISTRICT         46\n",
       "35         B-OTHER         36\n",
       "36         L-OTHER         36\n",
       "37        I-ISLAND         34\n",
       "38        I-COUNTY         33\n",
       "39  U-NEIGHBORHOOD         29\n",
       "40       I-NATURAL         22\n",
       "41         U-OTHER         20\n",
       "42          U-ROAD         18\n",
       "43         I-OTHER         14\n",
       "44  I-NEIGHBORHOOD         11\n",
       "45     B-CONTINENT         10\n",
       "46     L-CONTINENT         10\n",
       "47      I-DISTRICT          4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label = pd.concat([df_tag_train, df_tag_dev])[\"labels\"].unique().tolist()\n",
    "label_counts = pd.concat([df_tag_train, df_tag_dev])[\"labels\"].value_counts().reset_index()\n",
    "label_counts.columns = [\"Label\", \"Frequency\"]\n",
    "display(label_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let define model **Args** and hyperparameters optimisation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "sweep_config = {\n",
    "    \"method\": \"bayes\",  # grid, random\n",
    "    \"metric\": {\"name\": \"wer\", \"goal\": \"minimize\"},\n",
    "    \"parameters\": {\n",
    "        \"num_train_epochs\": {\"values\": [1, 2, 3, 5, 8]},\n",
    "        \"learning_rate\": {\"min\": 5e-5, \"max\": 4e-4},\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize a W&B sweep with the config defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep_id = wandb.sweep(sweep_config, project=\"LMR\")\n",
    "#%%capture\n",
    "# wandb.init(project=\"LMR\", name=\"Location-Mention-Recognition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = NERArgs()\n",
    "\n",
    "# general\n",
    "model_args.num_train_epochs=4\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.train_batch_size = 64\n",
    "model_args.eval_batch_size = 32\n",
    "model_args.labels_list = label\n",
    "model_args.use_multiprocessing = True\n",
    "# model_args.wandb_project = \"LMR\"\n",
    "\n",
    "# for eaarly stoping\n",
    "# model_args.use_early_stopping = True\n",
    "# model_args.early_stopping_delta = 0.01\n",
    "# model_args.early_stopping_metric = \"wer\"\n",
    "# model_args.early_stopping_metric_minimize = False\n",
    "# model_args.early_stopping_patience = 5\n",
    "model_args.evaluate_during_training_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TRAINING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(34373) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(34374) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(34375) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(34376) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(34377) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(34378) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(34379) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(34380) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1f13141b6444f98164df0f10ee4ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b728171258fb4dbd97e49b231b9984f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a537bec8a24c6e8a9600c7737f82e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 4:   0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(40715) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(40716) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(40717) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(40718) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(40719) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(40720) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(40721) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(40722) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0da98966b2463a958a6d74e39fc412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1367eb412b42c1aadc9862d3968683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-STATE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-COUNTY seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-COUNTY seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-CITY seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-CITY seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-COUNTRY seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-COUNTRY seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-NATURAL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-OTHER seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-OTHER seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-ROAD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-CONTINENT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-ISLAND seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-CONTINENT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-DISTRICT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-STATE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-DISTRICT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-NEIGHBORHOOD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-HUMAN-MADE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-NATURAL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-HUMAN-MADE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-ISLAND seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-NEIGHBORHOOD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a394aad57bd94353b895a3dfa29a0e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 4:   0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = NERModel(\n",
    "    \"bert\", \n",
    "    \"bert-large-uncased\", #bert-large-uncased\n",
    "    use_cuda=False,\n",
    "    args=model_args, \n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print('### TRAINING')\n",
    "model.train_model(\n",
    "    train_data, \n",
    "    eval_data=test_data, \n",
    "    wer=LMR_Metrics.wer_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### EVALUATION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(29932) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(29938) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(29939) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(29940) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(29941) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(29942) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(29943) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(29944) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94df0178866a4645924e999582213ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ef3928704f450ea822002e8823b9f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "print('### EVALUATION')\n",
    "result, model_outputs, wrong_preds = model.eval_model(test_data, wer=LMR_Metrics.wer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.08372374755831866,\n",
       " 'precision': 0.7946026986506747,\n",
       " 'recall': 0.8038422649140546,\n",
       " 'f1_score': 0.7991957778336265,\n",
       " 'wer': 0.35615196468537486}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quick prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(34283) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(34284) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(34285) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(34286) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(34287) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(34288) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(34289) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(34290) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b527e5272a417e92b1e2b3c2f75e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f530ee4901fd4230bd6a503ae1c22f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions, raw_outputs = model.predict([\n",
    "    \"elicott city, maryland, struck by catastrophic flooding; 1 missing.\",\n",
    "    \"memorial day weekend floods ravage maryland town\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'elicott': 'O'},\n",
       "  {'city,': 'L-CITY'},\n",
       "  {'maryland,': 'U-STATE'},\n",
       "  {'struck': 'O'},\n",
       "  {'by': 'O'},\n",
       "  {'catastrophic': 'O'},\n",
       "  {'flooding;': 'O'},\n",
       "  {'1': 'O'},\n",
       "  {'missing.': 'O'}],\n",
       " [{'memorial': 'O'},\n",
       "  {'day': 'O'},\n",
       "  {'weekend': 'O'},\n",
       "  {'floods': 'O'},\n",
       "  {'ravage': 'O'},\n",
       "  {'maryland': 'U-STATE'},\n",
       "  {'town': 'O'}]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Make prediction for Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2942/2942 [03:56<00:00, 12.44it/s]\n",
      "python(30065) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(30066) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(30067) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(30068) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(30069) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(30070) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(30071) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(30072) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37cbfad7046b443fa18a42c6bc392348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473a377b826b4ba1b3bdaf349668556a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Prediction:   0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Data and Preprocess\n",
    "# df_context = pd.read_csv('../data/provided/Test.csv')\n",
    "# df_context = Preprocess.remove_special_characters(df_context, column_name='text')\n",
    "# df_context['text'] = df_context['text'].apply(lambda x: Preprocess.treat_hashtags(x))\n",
    "# df_context['text'] = df_context['text'].apply(lambda x: Preprocess.correct_spelling(x))\n",
    "# #df_context['text'] = df_context['text'].apply(lambda x: Preprocess.remove_stop_words(x))\n",
    "# df_context.to_csv(\"../data/provided/Test-processed.csv\")\n",
    "\n",
    "df_context = pd.read_csv('../data/provided/Test.csv')\n",
    "df_context = Preprocess.remove_non_ascii(df_context, column_name='text')\n",
    "df_context = Preprocess.remove_usertag(df_context, column_name='text')\n",
    "df_context = Preprocess.reformat_hashtag(df_context, column_name='text')\n",
    "df_context = Preprocess.remove_stop_words(df_context, column_name='text', new_col=\"text_transformed\", transformation=[\n",
    "    \"tokenize\", \"lemma\", \"lower\"], save_in=\"../data/provided/Test-processed.csv\")\n",
    "\n",
    "#df_context = pd.read_csv('../data/provided/Test-processed.csv')\n",
    "\n",
    "ids = df_context[\"tweet_id\"].values\n",
    "tweets = df_context[\"text_transformed\"].values\n",
    "\n",
    "# Make prediction\n",
    "predictions, raw_outputs = model.predict(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to ../submissions/submission_9.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract Location Mention based on model output\n",
    "results = []\n",
    "for sentence in predictions:\n",
    "    result = \" \".join([word for d in sentence for word, tag in d.items() if tag != 'O'])\n",
    "    if result == \"\":\n",
    "        result = \" \"\n",
    "    results.append(result)\n",
    "\n",
    "Predictions.to_csv(ids, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
